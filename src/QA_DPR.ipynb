{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOJx6TXNCKAr"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
        "    # Google cloud credentials\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/MyDrive/Uni/Magistrale/NLP/Project/nlp-project-338723-0510aa0a4912.json\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm45AN-fDEiv"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvk3m-FdCKA0",
        "outputId": "f323fc65-0287-4963-a8e9-e78a1bbcfe0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPUs are not available, setting flag 'using_TPU' to False.\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import List, Union, Dict, Tuple\n",
        "from transformers import BertTokenizerFast, DistilBertTokenizerFast, \\\n",
        "                         TFBertModel, TFDistilBertModel\n",
        "import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import utils\n",
        "from collections import deque\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_SEQ_LEN = 512\n",
        "BERT_DIMENSIONALITY = 768\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')\n",
        "\n",
        "if using_TPU:\n",
        "    try: \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
        "        using_TPU = False\n",
        "        print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "BATCH_SIZE = 4 if not using_TPU else 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox3qwrcACKA4"
      },
      "source": [
        "# Question Answering with the DPR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DtqzKMW6scF"
      },
      "source": [
        "Now that we have trained the Dense Passage Retriever, we have an efficient way to collect the best scoring paragraphs given a question (a dot product between the question's encoding representation and the matrix of paragraph representations).\n",
        "\n",
        "On top of this paragraph selection method, we can define our final Question Answering model in this way:\n",
        "- As input, the model should take the tokenized question (using the standard DistilBert tokenizer)\n",
        "- The model should then use `bert_q` (non-trainable) to create a representation of the question in the learnt 768-d space, that is in common with the paragraph representations.\n",
        "- We compute similarity scores between the representation of the question and all pre-computed (non-trainable) representations of paragraphs. Based on these scores, we select the top-k ($k=100$) paragraphs. \n",
        "    - Note that at training, validation and testing time we have three different paragraph representation matrices to use, so we should build a mechanism to easily switch between these representations.\n",
        "    - It's also advisable to maintain an array of pre-tokenised paragraphs since they will be used in the second part of the model.\n",
        "\n",
        "Once we have top-scoring paragraph indices available, we should decide which specific paragraph contains the question and where. \n",
        "- To do that, we encode both the question and the selected paragraphs using cross-attention through another Bert model (`reader`) (trainable model). This model will output k $512 \\times 768$ encodings for each question in the batch. Each encoding will be denoted as $P_i$ in contrast to $\\hat{P}_i$ which is the 768-d encoding at the `[CLS]` token. \n",
        "- For question answering, for each of the $k$ selected paragraphs, we must compute the probability of the paragraph being selected $P_{selected}(i)$, as well as the usual $P_{start, i}(s), P_{end, i}(t)$ for each of the $s$-th and $t$-th words of the $i$-th paragraph.\n",
        "- All probabilities are computed through dense layers:\n",
        "\\begin{gather}\n",
        "P_{start,i}(s) = softmax(P_i w_{start})_s\n",
        "\\\\\n",
        "P_{end,i}(t) = softmax(P_i w_{end})_t\n",
        "\\\\\n",
        "P_{selected}(i) = softmax(\\hat{P}_i^\\intercal w_{selected})_i\n",
        "\\\\\n",
        "\\end{gather}\n",
        "where $w_{start}$, $w_{end}$ and $w_{selected}$ are learnt vectors, while $\\hat{P}_i = [P_{1}^{[CLS]}, \\dots, P_k^{[CLS]}]$.\n",
        "- As final answer, we select the highest scoring start-end legal span from the highest-scoring paragraph.\n",
        "\n",
        "During training: For each question, we create a batch by sampling $m$ ($m=24$ in the paper) from the top-100 passages returned by the retrieval system (DPR, so by computing similarities with the pre-computed representations). The training objective is to maximize the marginal log-likelihood of all the correct answer spans in the positive passage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. In the paper, a batch size of 16 was used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z63i9X7CKA7"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GU11Vd7CKA8"
      },
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXvBSQypCKA9"
      },
      "source": [
        "We load all data that was prepared into the `dense_passage_retriever` notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cp4MBwvCKA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e3c082-185a-43cb-e0cc-214762870774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "    datasets_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
        "else:\n",
        "    # Create the folder where we'll save the weights of the model\n",
        "    checkpoint_dir = os.path.join(\"checkpoints\", \"training_dpr\")\n",
        "    datasets_dir = os.path.join(\"checkpoints\", \"training_dpr\", \"dataset\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(datasets_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNvPry74CKBB"
      },
      "outputs": [],
      "source": [
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]\n",
        "\n",
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA_XJ_1FCKBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593b917f-c2b8-44d3-8ef8-5cfc7ef251fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/train_768.proto).\n",
            "Loading val_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/val_768.proto).\n",
            "Loading test_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/test_768.proto).\n"
          ]
        }
      ],
      "source": [
        "def decode_fn(record_bytes):\n",
        "    # Reads one element from the dataset (as bytes) and decodes it in a tf.data Dataset element.\n",
        "    example = tf.io.parse_single_example(\n",
        "      # Data\n",
        "      record_bytes,\n",
        "      # Schema\n",
        "      {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"context__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64)})\n",
        "    return {\n",
        "      \"questions\": {'input_ids': example['question__input_ids'],\n",
        "                    'attention_mask': example['question__attention_mask'],\n",
        "                    'index': example['question__index']},\n",
        "      \"answers\":   {'out_s': example['answer__out_s'],\n",
        "                    'out_e': example['answer__out_e']},\n",
        "      \"paragraphs\":{'input_ids': example['paragraph__input_ids'],\n",
        "                    'attention_mask': example['paragraph__attention_mask'],\n",
        "                    'tokens_s': example['paragraph__tokens_s'],\n",
        "                    'tokens_e': example['paragraph__tokens_e']},\n",
        "      \"hard_paragraphs\": {'input_ids': example['hard_paragraph__input_ids'],\n",
        "                          'attention_mask': example['hard_paragraph__attention_mask']},\n",
        "      \"context_ids\": (example['context__index'], example['paragraph__index'])\n",
        "    }\n",
        "\n",
        "def load_tf_dataset_from_cloud(questions, fn, batch_size=BATCH_SIZE):\n",
        "    # Prepare strings\n",
        "    filename = f'{fn}_{BERT_DIMENSIONALITY}.proto'\n",
        "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
        "    dst_name = fn_type + '.proto'\n",
        "    bucket_name = 'volpepe-nlp-project-squad-datasets'\n",
        "    gcs_filename = f'gs://{bucket_name}/{dst_name}'\n",
        "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
        "    # Return it as processed dataset\n",
        "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_fn)\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "dataset_train = load_tf_dataset_from_cloud(train_questions, os.path.join(datasets_dir, 'train'))\n",
        "dataset_val = load_tf_dataset_from_cloud(val_questions, os.path.join(datasets_dir, 'val'))\n",
        "dataset_test = load_tf_dataset_from_cloud(val_questions, os.path.join(datasets_dir, 'test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClATw7cey1TI"
      },
      "source": [
        "We also load the paragraphs' `model_p` encodings and the questions' `model_q` encodings for the selection part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHrhGhExy_H6"
      },
      "outputs": [],
      "source": [
        "representations_dir = os.path.join(datasets_dir, 'representations')\n",
        "\n",
        "train_paragraphs_encodings = np.load(os.path.join(representations_dir, 'train_paragraphs_encodings.npy'))\n",
        "val_paragraphs_encodings   = np.load(os.path.join(representations_dir, 'val_paragraphs_encodings.npy'))\n",
        "test_paragraphs_encodings  = np.load(os.path.join(representations_dir, 'test_paragraphs_encodings.npy'))\n",
        "\n",
        "train_questions_encodings  = np.load(os.path.join(representations_dir, 'train_questions_encodings.npy'))\n",
        "val_questions_encodings    = np.load(os.path.join(representations_dir, 'val_questions_encodings.npy'))\n",
        "test_questions_encodings   = np.load(os.path.join(representations_dir, 'test_questions_encodings.npy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we load the tokenizer."
      ],
      "metadata": {
        "id": "MBDEUyMM13R8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "498HGSpRCKBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "32fab545874a4899a77bc60c1fe48fd5",
            "1c9112e1459a4ee79c8567fb98287f58",
            "43a5c84a3c7c44b2b216b18c9949516b",
            "40fd2b78300e4e3c8b1058d336426a86",
            "a2f0d182e15148dd8a30e3d36e547ef6",
            "01e5e40e05d14cfca245c7a626f88fa9",
            "7f5a5db32b6e41139238fe1998c023a3",
            "5d46bb2509d140e1b81b49b45371d83c",
            "b1f46072e1b34001a5fd2ef3a78a4c16",
            "cbcfd04cdd234503bc63c593cc903e79",
            "73c87b0a961349c9a3ff4730259d612b",
            "9b0dd8ad11f240b6a09588ec480f318f",
            "47d173092b564043bad47f9c8a150fb3",
            "14e1427d2de44a638df0ca3955bf42c5",
            "05c33627cbe84b8f860187314ffbbe7b",
            "e73d8a04d67e4e0989fdce1f37edadca",
            "bde53eb3e4d9407eb49f56c878c107fe",
            "465db849ce304f23beb85644fd7f71ee",
            "03cb13ebcd864c469c0013d858040ca9",
            "f6b81abf31c3443aa2e44402355560f1",
            "1aa71f1c43ab4de2ab1d20d4f3d91b0f",
            "19c2f53cfbb34d74975c9d76e5d54bf2",
            "ecdd6deeda8242c69f7ef9a395e17b77",
            "c28f7e123c33422bb4b6bbfa8d56c4a4",
            "14dd8ebc961d4b229ec5c8873f63dbf4",
            "514711368e02481e8427889bf9552145",
            "9ff597a06b6943068ae8361d353d8145",
            "e78d741cb8b14472a000413f91ed72ec",
            "2e8a8b2d08a1459084524a3ae4cdb4e7",
            "33105da64f614efc964546e5cd34f0c0",
            "18255275cb6b4550a45658257a040b4a",
            "0984a727b2754fa0bb03ba731d42980e",
            "504a6b0dd2174000966a2341608709a2",
            "16ea6f9e732c43b6bdafa1c663fcf32c",
            "78061091bbf745cb8c644756254e1208",
            "abfcc4c5fdda433084fdc7f7330a6c0b",
            "39beab7e013c4d238bc6296fcbde1705",
            "831af2b12ad8434e84d1c336d6e366bc",
            "5f8bdc87f1244f108bbdc9578f79dc14",
            "771d7b4b63e141ccad93ab6676f446bc",
            "606326a7455349f6861930451fd958e3",
            "94ac7940276342628e37f7699342906e",
            "b2337551c0da4ec3945f12adf15e06b5",
            "7bb684657b04422da4f03552f45fc3a7"
          ]
        },
        "outputId": "b1ecac54-9593-443a-d728-7b6e1d3c6d56"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32fab545874a4899a77bc60c1fe48fd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b0dd8ad11f240b6a09588ec480f318f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecdd6deeda8242c69f7ef9a395e17b77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16ea6f9e732c43b6bdafa1c663fcf32c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTiQu6Enq8xD"
      },
      "source": [
        "## Paragraphs preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKiscflLrEmV"
      },
      "source": [
        "Firstly, we pre-tokenize the paragraphs to directly use them inside the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDK42osrAlsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dfd2e1-a9fe-4ccb-ef99-826625e8c4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13975/13975 [00:15<00:00, 923.00it/s] \n",
            "100%|██████████| 4921/4921 [00:03<00:00, 1249.20it/s]\n",
            "100%|██████████| 2067/2067 [00:01<00:00, 1162.22it/s]\n"
          ]
        }
      ],
      "source": [
        "pretokenized_paragraphs = {\n",
        "    'train': {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'offset_mapping': []\n",
        "    },\n",
        "    'val': {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'offset_mapping': []\n",
        "    }, \n",
        "    'test': {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'offset_mapping': []\n",
        "    }\n",
        "}\n",
        "\n",
        "for i in tqdm(range(len(train_paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        train_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['train']['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['train']['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['train']['offset_mapping'].append(token_p['offset_mapping'])\n",
        "\n",
        "for i in tqdm(range(len(val_paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        val_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['val']['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['val']['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['val']['offset_mapping'].append(token_p['offset_mapping'])\n",
        "\n",
        "for i in tqdm(range(len(test_paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        test_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['test']['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['test']['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['test']['offset_mapping'].append(token_p['offset_mapping'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPG2DHje99it"
      },
      "source": [
        "# Question Answering model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAEYdrfC4mZL"
      },
      "source": [
        "First we create some utility layers for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMjdHTkquGnN"
      },
      "outputs": [],
      "source": [
        "class ParagraphSelector(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer that is used to select the top-100 paragraphs given the question encoding as input.\n",
        "    It requires the question encoding and a string as input that tells which matrix\n",
        "    shall be used for the pre-encoded paragraph representations comparison.\n",
        "    Returns the pre-tokenised best paragraphs and their indices.\n",
        "    '''\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        super(ParagraphSelector, self).__init__(trainable=False, **kwargs)\n",
        "        self.pretokenized_paragraphs = pretokenized_paragraphs\n",
        "        self.representations = {\n",
        "            'train': train_paragraphs_encodings,\n",
        "            'val'  : val_paragraphs_encodings,\n",
        "            'test' : test_paragraphs_encodings\n",
        "        }\n",
        "        self.input_ids = {\n",
        "            dset: tf.constant(np.array(\n",
        "                self.pretokenized_paragraphs[dset]['input_ids']).squeeze())\n",
        "            for dset in ['train', 'val', 'test']\n",
        "        }\n",
        "        self.attention_masks = {\n",
        "            dset: tf.constant(np.array(\n",
        "                self.pretokenized_paragraphs[dset]['attention_mask']).squeeze())\n",
        "            for dset in ['train', 'val', 'test']\n",
        "        }\n",
        "\n",
        "    def call(self, q_repr:tf.Tensor, representation_type:str):\n",
        "        scores = tf.tensordot(q_repr, self.representations[representation_type].T, axes=1)\n",
        "        # Collect the best 100 scores\n",
        "        top100_indices = tf.argsort(scores, direction='DESCENDING')[:,:100]\n",
        "        # Now gather the pretokenized paragraphs using these indices\n",
        "        best_input_ids = tf.gather(\n",
        "            self.input_ids[representation_type], \n",
        "            top100_indices, batch_dims=1, axis=0)   # First index dimension is batch dimension, but gather row elements in target\n",
        "        best_attention_masks = tf.gather(\n",
        "            self.attention_masks[representation_type], \n",
        "            top100_indices, batch_dims=1, axis=0)\n",
        "        # Return the pre-tokenised paragraph and indices\n",
        "        return top100_indices, best_input_ids, best_attention_masks\n",
        "\n",
        "\n",
        "class BestScoringCollector(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer to collect the start and end probabilities from the best scoring paragraph\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BestScoringCollector, self).__init__(trainable=False, **kwargs)\n",
        "\n",
        "    def call(self, probs_s, probs_e, probs_sel):\n",
        "        # Selection of best scoring paragraphs\n",
        "        best_scoring_paragraphs = tf.squeeze(tf.argmax(probs_sel, axis=1, output_type=tf.int32))\n",
        "        # Selection of related start-end probabilities\n",
        "        probs_s = tf.squeeze(tf.gather(probs_s, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        probs_e = tf.squeeze(tf.gather(probs_e, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        return probs_s, probs_e\n",
        "\n",
        "\n",
        "class ReaderEvaluator(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer to compute the start, end and selection probabilities given the paragraphs' \n",
        "    full and search encodings.\n",
        "    '''\n",
        "    def __init__(self, **kwargs) -> None:\n",
        "        super(ReaderEvaluator, self).__init__(trainable=True, **kwargs)\n",
        "        self.start_token_logits = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"start_token_logits\")\n",
        "        self.start_token_probabilities = keras.layers.Softmax(name=\"start_probs\", axis=1, dtype='float32')\n",
        "        self.end_token_logits = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"end_token_logits\")\n",
        "        self.end_token_probabilities = keras.layers.Softmax(name=\"end_probs\", axis=1, dtype='float32')\n",
        "        self.paragraph_selection_logits = keras.layers.Dense(1, name=\"selection_logits\")\n",
        "        self.paragraph_selection_probabilities = keras.layers.Softmax(name=\"selection_probs\", dtype='float32')\n",
        "        self.flatten = keras.layers.Flatten()\n",
        "        self.best_scoring_collector = BestScoringCollector(name='best_scoring_collector')\n",
        "\n",
        "    def call(self, paragraphs_full_encodings, paragraphs_search_encodings):\n",
        "        # Compute probabilities for the start token\n",
        "        out_S = self.start_token_logits(paragraphs_full_encodings)\n",
        "        out_S = tf.squeeze(out_S)\n",
        "        out_S = self.start_token_probabilities(out_S)\n",
        "\n",
        "        # The same is done for the end tokens.\n",
        "        out_E = self.end_token_logits(paragraphs_full_encodings)\n",
        "        out_E = tf.squeeze(out_E)\n",
        "        out_E = self.end_token_probabilities(out_E)\n",
        "\n",
        "        # Also, we compute paragraph selection probabilities\n",
        "        out_SEL = self.paragraph_selection_logits(paragraphs_search_encodings)\n",
        "        out_SEL = self.flatten(out_SEL)\n",
        "        out_SEL = self.paragraph_selection_probabilities(out_SEL)\n",
        "\n",
        "        out_S, out_E = self.best_scoring_collector(out_S, out_E, out_SEL)\n",
        "        return out_S, out_E, out_SEL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv36gxUkZDgJ"
      },
      "source": [
        "We need a function to map the dataset's context ids to ground truth indices in the representation matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxIATaeHafFe"
      },
      "outputs": [],
      "source": [
        "# Step 1: pre-compute how many paragraphs each context has in each dataset\n",
        "paragraphs_per_context = {\n",
        "    'train': np.cumsum([len(p['paragraphs']) for p in train_paragraphs_and_questions]),\n",
        "    'val'  : np.cumsum([len(p['paragraphs']) for p in val_paragraphs_and_questions]),\n",
        "    'test' : np.cumsum([len(p['paragraphs']) for p in test_paragraphs_and_questions])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCJ0CT1cy8BH"
      },
      "outputs": [],
      "source": [
        "# Step 2: actual computation with tf.function for efficiency\n",
        "@tf.function\n",
        "def tf_get_paragraph_encoding_index(batch, dataset):\n",
        "    art_ids, par_ids = batch['context_ids']\n",
        "    idxs = tf.TensorArray(tf.int64, size=len(art_ids))\n",
        "    for i in tf.range(len(art_ids), dtype=tf.int32):\n",
        "        idx = par_ids[i]\n",
        "        if art_ids[i] > 0:\n",
        "           idx += tf.gather(paragraphs_per_context[dataset], art_ids[i]-1)\n",
        "        idxs = idxs.write(i, idx)\n",
        "    return idxs.stack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SvQvp76GMUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023e5fbe-ab90-4071-f26d-2204df99de46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 3, 14,  0,  0]), array([ 4,  2, 49,  2]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([151, 811,  49,   2])>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "test_elements = next(dataset_train.as_numpy_iterator())\n",
        "print(test_elements['context_ids'])\n",
        "tf_get_paragraph_encoding_index(test_elements, 'train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8WqHcizdni"
      },
      "source": [
        "Finally we can create the whole Question Answering model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWgn3biUgtf-",
        "outputId": "86663e5f-b287-4ba5-9e41-1aa67f651787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = next(dataset_train.as_numpy_iterator())"
      ],
      "metadata": {
        "id": "zIGMbPxegxKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the best 100 paragraphs given the questions' representations\n",
        "q_repr = tf.gather(model_QA.questions_representations['train'], \n",
        "                                   input_data['questions']['index'])\n",
        "top100_indices, best_input_ids, best_attention_mask = model_QA.paragraph_selector(\n",
        "    q_repr, 'train')    # We are training, so we index in the training paragraphs matrix\n",
        "# Obtain the ground truth indices\n",
        "gt_indices = model_QA.tf_get_paragraph_encoding_index(input_data, 'train')\n",
        "# Obtain the training batch and the ground truth mask\n",
        "data_batch, gt_mask = model_QA.tf_obtain_training_info(gt_indices, top100_indices)\n",
        "# Put together question and paragraph encodings to obtain a full tokenization\n",
        "input_ids_for_reader, attention_mask_for_reader, gt_start, gt_end = \\\n",
        "    model_QA.tf_put_questions_and_paragraphs_together(\n",
        "        input_data['questions']['input_ids'], \n",
        "        tf.squeeze(\n",
        "            tf.gather(\n",
        "                pretokenized_paragraphs['train']['input_ids'], \n",
        "                data_batch, axis=0, batch_dims=1)\n",
        "        ), \n",
        "        input_data['answers'])"
      ],
      "metadata": {
        "id": "oWJoLAk7g9Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test():\n",
        "    a = tf.TensorArray(tf.float32, size=4)\n",
        "    for i in tf.range(4):\n",
        "        q_repr = reader({\n",
        "            'input_ids': input_ids_for_reader[i,:,:],\n",
        "            'attention_mask': attention_mask_for_reader[i,:,:],\n",
        "            'training': False\n",
        "        }).last_hidden_state\n",
        "        a = a.write(i, q_repr)\n",
        "    return a.stack()"
      ],
      "metadata": {
        "id": "1xr6k9VWitWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoy9jR1dnjxu",
        "outputId": "2cd5ce21-2640-45be-e961-b56eb5164361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([4, 4, 512, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.rank(input_ids_for_reader) > 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs0mkv7OoibF",
        "outputId": "fc4e0940-623f-4158-d21d-e1427b5c3b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "pc3kqietgxwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vaO3xVT5eiE"
      },
      "outputs": [],
      "source": [
        "class QuestionAnsweringModel(keras.Model):\n",
        "    def __init__(self, reader, questions_representations, m=24):\n",
        "        '''\n",
        "        Complete Question Answering model.\n",
        "        - We can call \"predict\" on it passing a pre-tokenised question (in a dict with\n",
        "            keys 'input_ids' and 'attention_mask') and a string ('train', 'val', 'test') \n",
        "            defining the appropriate paragraph set to consider for reading paragraphs. \n",
        "            Default is 'test'.\n",
        "        - We can otherwise call it directly passing a batch of pre-tokenised \n",
        "            questions + paragraphs.\n",
        "\n",
        "        Requires a reader (usually a virgin DistilBert model) and a dictionary {\n",
        "            'train': ..., \n",
        "            'val': ..., \n",
        "            'test': ... \n",
        "        } containing the matrices of `model_q` question encodings, to be used at \n",
        "        training and evaluation time\n",
        "        '''\n",
        "        super(QuestionAnsweringModel, self).__init__()\n",
        "        # Layers\n",
        "        self.paragraph_selector = ParagraphSelector(name='paragraph_selector')\n",
        "        self.reader = reader    # DistilBert model\n",
        "        self.reader_evaluator   = ReaderEvaluator(name='reader_evaluator')\n",
        "        # model_q question encodings (pre-computed to run a faster training/evaluation)\n",
        "        self.questions_representations = questions_representations\n",
        "        # Hyperparams\n",
        "        self.m = m\n",
        "        # Metrics\n",
        "        self.loss_tracker     = keras.metrics.Mean(name=\"loss\")\n",
        "        self.start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
        "        self.end_acc_metric   = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
        "        self.sel_acc_metric   = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        if tf.rank(inputs['input_ids']) > 2:\n",
        "            full_encodings = tf.TensorArray(tf.float32, size=len(inputs['input_ids']))\n",
        "            for i in tf.range(tf.shape(inputs['input_ids'])[0]):\n",
        "                encs = self.reader({\n",
        "                    'input_ids': inputs['input_ids'][i,:,:], \n",
        "                    'attention_mask': inputs['attention_mask'][i,:,:],\n",
        "                    'training': inputs['training']\n",
        "                }).last_hidden_state\n",
        "                full_encodings = full_encodings.write(i, encs)\n",
        "            full_encodings = full_encodings.stack()\n",
        "            search_encodings = full_encodings[:,:,0,:]\n",
        "        else:\n",
        "            full_encodings = self.reader({\n",
        "                'input_ids': inputs['input_ids'], \n",
        "                'attention_mask': inputs['attention_mask'],\n",
        "                'training': inputs['training']\n",
        "            }).last_hidden_state\n",
        "            search_encodings = full_encodings[:,0,:]\n",
        "        out_S, out_E, out_SEL = self.reader_evaluator(full_encodings, search_encodings)\n",
        "        return out_S, out_E, out_SEL\n",
        "\n",
        "    @tf.function\n",
        "    def tf_put_questions_and_paragraphs_together(self, questions_input_ids, \n",
        "                                                 data_batch_input_ids, gt_answers=None):\n",
        "        '''\n",
        "        Utility function that merges the question and paragraphs representations in order\n",
        "        to allow cross-attention in the reader. Optionally pass gt_answers to also \n",
        "        correct the one hot encodings for the GT positions.\n",
        "        '''\n",
        "        # Define the arrays that will hold the data\n",
        "        final_input_ids_array = tf.TensorArray(tf.int32, size=len(questions_input_ids))\n",
        "        final_attention_masks_array = tf.TensorArray(tf.int32, size=len(questions_input_ids))\n",
        "        # Empty arrays if gt_answers is None\n",
        "        final_gt_s_array = tf.TensorArray(tf.int32, size=len(questions_input_ids))\n",
        "        final_gt_e_array = tf.TensorArray(tf.int32, size=len(questions_input_ids))\n",
        "        # Iterate over batch dimension\n",
        "        for i in tf.range(len(questions_input_ids)):\n",
        "            q = questions_input_ids[i]\n",
        "            # Obtain the part of the question ids that are before the separator\n",
        "            pos_sep_q = tf.where(q == 102)[0][0]\n",
        "            appendix = tf.cast(q[:pos_sep_q + 1], tf.int32)\n",
        "            input_ids = tf.TensorArray(tf.int32, size=len(data_batch_input_ids[i]))\n",
        "            attention_masks = tf.TensorArray(tf.int32, size=len(data_batch_input_ids[i]))\n",
        "            # Iterate over the paragraphs for each question\n",
        "            for j in tf.range(len(data_batch_input_ids[i])):\n",
        "                p = data_batch_input_ids[i,j]\n",
        "                # Obtain the part of the paragraph ids that are before the separator\n",
        "                # but after the initial token\n",
        "                postfix = p[1:]\n",
        "                # Concatenate the two parts\n",
        "                final_id_seq = tf.concat([appendix, postfix], axis=0)\n",
        "                # Cut the padding at 512, but if the tensor is longer put a 102 token\n",
        "                # at the end.\n",
        "                if tf.where(final_id_seq == 102)[1][0] >= MAX_SEQ_LEN:\n",
        "                    final_id_seq = tf.concat([final_id_seq[:MAX_SEQ_LEN-1], \n",
        "                                            tf.constant([102])], axis=0)\n",
        "                else:\n",
        "                    final_id_seq = final_id_seq[:MAX_SEQ_LEN]\n",
        "                # Create the attention mask\n",
        "                final_attention_mask = tf.cast(final_id_seq != 0, tf.int32)\n",
        "                # Write results on arrays\n",
        "                input_ids = input_ids.write(j, final_id_seq)\n",
        "                attention_masks = attention_masks.write(j, final_attention_mask)\n",
        "            input_ids = input_ids.stack()\n",
        "            attention_masks = attention_masks.stack()\n",
        "            final_input_ids_array = final_input_ids_array.write(i, input_ids)\n",
        "            final_attention_masks_array = final_attention_masks_array.write(i, attention_masks)\n",
        "            if gt_answers is not None:\n",
        "                # This part really only makes sense for the positive paragraph.\n",
        "                # We are shifting the paragraph representation by pos_sep_q:\n",
        "                # so we add the same number to the index where out_s and out_e are 1.\n",
        "                # Both can go out of bounds, so we cap the positions to the maximum sequence\n",
        "                # length\n",
        "                pos_gt_s = tf.math.minimum(tf.where(gt_answers['out_s'][i])[0][0] + pos_sep_q, \n",
        "                                        tf.constant(MAX_SEQ_LEN-1, dtype=tf.int64))\n",
        "                pos_gt_e = tf.math.minimum(tf.where(gt_answers['out_e'][i])[0][0] + pos_sep_q, \n",
        "                                        tf.constant(MAX_SEQ_LEN-1, dtype=tf.int64))\n",
        "                # Create the final GT tensors\n",
        "                final_gt_s = tf.one_hot(pos_gt_s, MAX_SEQ_LEN, dtype=tf.int32)\n",
        "                final_gt_e = tf.one_hot(pos_gt_e, MAX_SEQ_LEN, dtype=tf.int32)\n",
        "                # Write the partial tensors into the final array\n",
        "                final_gt_s_array = final_gt_s_array.write(i, final_gt_s)\n",
        "                final_gt_e_array = final_gt_e_array.write(i, final_gt_e)\n",
        "            else:\n",
        "                # If None, fill these arrays with 0s.\n",
        "                final_gt_s_array.write(i, tf.constant(0, dtype=tf.int32))\n",
        "                final_gt_e_array.write(i, tf.constant(0, dtype=tf.int32))\n",
        "        # Return the stacked arrays\n",
        "        return final_input_ids_array.stack(), final_attention_masks_array.stack(), \\\n",
        "            final_gt_s_array.stack(), final_gt_e_array.stack()\n",
        "\n",
        "    @tf.function\n",
        "    def tf_get_paragraph_encoding_index(self, batch, dataset):\n",
        "        '''\n",
        "        Utility function to obtain the ground truth index of a batch of paragraphs given their contexts\n",
        "        '''\n",
        "        art_ids, par_ids = batch['context_ids']\n",
        "        # Create an array that will hold the indexes\n",
        "        idxs = tf.TensorArray(tf.int64, size=len(art_ids))\n",
        "        # Loop through the elements of the batch\n",
        "        for i in tf.range(len(art_ids), dtype=tf.int32):\n",
        "            # The index is the id of the paragraph to which we add the paragraphs\n",
        "            # present in the previous articles\n",
        "            idx = par_ids[i]\n",
        "            if art_ids[i] > 0:\n",
        "                # Since we used a cumulative sum we can simply index \n",
        "                # the number of paragraphs at art_ids[i]-1\n",
        "                idx += tf.gather(paragraphs_per_context[dataset], art_ids[i]-1)\n",
        "            # Write to the array\n",
        "            idxs = idxs.write(i, idx)\n",
        "        # Return the final tensor\n",
        "        return idxs.stack()\n",
        "\n",
        "    @tf.function\n",
        "    def tf_shuffle_on_columns(self, value):\n",
        "        '''\n",
        "        Utility function that shuffles a tensor randomly on each of its rows.\n",
        "        '''\n",
        "        # Create a tensor of random numbers, argsort it and use them as indices to gather\n",
        "        # values from the original tensor\n",
        "        return tf.gather(value, tf.argsort(tf.random.uniform(tf.shape(value))), batch_dims=1)\n",
        "\n",
        "    @tf.function\n",
        "    def tf_obtain_training_info(self, gt_indexes, top100_indexes):\n",
        "        '''\n",
        "        Obtains a batch of data and the ground truth mask to be used while training the model\n",
        "        '''\n",
        "        # Collect ground truth indexes\n",
        "        gt_paragraphs = tf.expand_dims(tf.cast(gt_indexes, tf.int32), -1)\n",
        "        # A training sample is formed by the positive and m-1 negative examples\n",
        "        # obtained from the top-100 for each of the questions in the batch.\n",
        "        # We create a data batch by sampling m-1 examples from the masked 100 paragraphs\n",
        "        negative_masks = tf.math.not_equal(top100_indexes, gt_paragraphs)\n",
        "        # To keep the graph working with the correct sizes, we create a tensor of negatives \n",
        "        # by random shuffling the large tensor of top100 indices and taking the first m elements.\n",
        "        # Positive examples that could end up in this tensor are replaced by randomly sampling \n",
        "        # from the tensor of top100 indices again.\n",
        "        # It could happen that the positive example is replaced by itself, or that a \n",
        "        # negative sample appears twice in the batch, but it's a non-deterministic process.\n",
        "        negatives = self.tf_shuffle_on_columns(tf.where(\n",
        "            negative_masks, top100_indexes, self.tf_shuffle_on_columns(top100_indexes))\n",
        "        )[:,:self.m-1]\n",
        "        # We concatenate the positive paragraph index to the selected negatives and shuffle\n",
        "        # so that the positive is not always the last element\n",
        "        data_batch = self.tf_shuffle_on_columns(\n",
        "            tf.concat([negatives, gt_paragraphs], axis=1))\n",
        "        # When we have a data batch, we create the ground truth mask, which represents the position\n",
        "        # of the positive sample in the data batch in a one-hot encoded fashion.\n",
        "        gt_mask = tf.cast(data_batch == gt_paragraphs, tf.int32)\n",
        "        return data_batch, gt_mask\n",
        "\n",
        "    def train_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        # Obtain the best 100 paragraphs given the questions' representations\n",
        "        q_repr = tf.gather(self.questions_representations['train'], \n",
        "                                        data['questions']['index'])\n",
        "        top100_indices, best_input_ids, best_attention_mask = self.paragraph_selector(\n",
        "            q_repr, 'train')    # We are training, so we index in the training paragraphs matrix\n",
        "        # Obtain the ground truth indices\n",
        "        gt_indices = self.tf_get_paragraph_encoding_index(data, 'train')\n",
        "        # Obtain the training batch and the ground truth mask\n",
        "        data_batch, gt_mask = self.tf_obtain_training_info(gt_indices, top100_indices)\n",
        "        # Put together question and paragraph encodings to obtain a full tokenization\n",
        "        input_ids_for_reader, attention_mask_for_reader, gt_start, gt_end = \\\n",
        "            self.tf_put_questions_and_paragraphs_together(\n",
        "                data['questions']['input_ids'], \n",
        "                tf.squeeze(\n",
        "                    tf.gather(\n",
        "                        pretokenized_paragraphs['train']['input_ids'], \n",
        "                        data_batch, axis=0, batch_dims=1)\n",
        "                ), \n",
        "                data['answers'])\n",
        "        # Open the gradient tape, obtain predictions and compute the loss\n",
        "        with tf.GradientTape() as tape:\n",
        "            out_S, out_E, out_SEL = self({\n",
        "                'input_ids': input_ids_for_reader,\n",
        "                'attention_mask': attention_mask_for_reader,\n",
        "                'training': True\n",
        "            })\n",
        "            loss_start = self.compiled_loss(gt_start, out_S)\n",
        "            loss_end = self.compiled_loss(gt_end, out_E)\n",
        "            loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "            loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        # Compute the gradients and apply them on the variables\n",
        "        grads = tape.gradient(loss_value, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        # Update the metrics\n",
        "        self.loss_tracker.update_state(loss_value)\n",
        "        self.start_acc_metric.update_state(gt_start, out_S)\n",
        "        self.end_acc_metric.update_state(gt_end, out_E)\n",
        "        self.sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        return {\"loss\": self.loss_tracker.result(), \n",
        "                \"start_accuracy\": self.start_acc_metric.result(),\n",
        "                \"end_accuracy\": self.end_acc_metric.result(),\n",
        "                \"sel_accuracy\": self.sel_acc_metric.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        # Obtain the best 100 paragraphs given the questions' representations\n",
        "        q_repr = tf.gather(self.questions_representations['val'], \n",
        "                                      data['questions']['index'])\n",
        "        top100_indices, best_input_ids, best_attention_mask = self.paragraph_selector(\n",
        "            q_repr, 'val')    # We are validating, so we index in the validation paragraphs matrix\n",
        "        # Obtain the ground truth indices\n",
        "        gt_indices = self.tf_get_paragraph_encoding_index(data, 'val')\n",
        "        # Obtain the training batch and the ground truth mask\n",
        "        data_batch, gt_mask = self.tf_obtain_training_info(\n",
        "            gt_indices, top100_indices)\n",
        "        # Put together question and paragraph encodings to obtain a full tokenization\n",
        "        input_ids_for_reader, attention_mask_for_reader, gt_start, gt_end = \\\n",
        "            self.tf_put_questions_and_paragraphs_together(\n",
        "                data['questions']['input_ids'], \n",
        "                tf.squeeze(\n",
        "                    tf.gather(\n",
        "                        pretokenized_paragraphs['val']['input_ids'], \n",
        "                        data_batch, axis=0, batch_dims=1)\n",
        "                ), \n",
        "                data['answers'])\n",
        "        # Obtain predictions and compute the loss\n",
        "        out_S, out_E, out_SEL = self({\n",
        "            'input_ids': input_ids_for_reader,\n",
        "            'attention_mask': attention_mask_for_reader,\n",
        "            'training': False\n",
        "        })\n",
        "        loss_start = self.compiled_loss(gt_start, out_S)\n",
        "        loss_end = self.compiled_loss(gt_end, out_E)\n",
        "        loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "        loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        # Updates the metrics\n",
        "        self.start_acc_metric.update_state(data['gt_start'], out_S)\n",
        "        self.end_acc_metric.update_state(data['gt_end'], out_E)\n",
        "        self.sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        return {\"loss\": self.loss_tracker.result(), \n",
        "                \"start_accuracy\": self.start_acc_metric.result(),\n",
        "                \"end_accuracy\": self.end_acc_metric.result(),\n",
        "                \"sel_accuracy\": self.sel_acc_metric.result()}\n",
        "\n",
        "    def predict_step(self, data):\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0]\n",
        "        # At prediction time we receive the pre-tokenised questions\n",
        "        # We obtain the best 100 paragraphs for the question\n",
        "        q_repr = tf.gather(self.questions_representations['test'], \n",
        "                                       data['questions']['index'])\n",
        "        top100_indices, best_input_ids, best_attention_masks = self.paragraph_selector(q_repr, 'test')\n",
        "        # Then we produce the reader's representations\n",
        "        input_ids_for_reader, attention_mask_for_reader, _, _ = self.tf_put_questions_and_paragraphs_together(\n",
        "            data['questions']['input_ids'], best_input_ids, gt_answers=None\n",
        "        )\n",
        "        # Let the reader encode the question and paragraphs with cross-attention\n",
        "        full_encodings = self.reader({\n",
        "            'input_ids': input_ids_for_reader,\n",
        "            'attention_mask': attention_mask_for_reader,\n",
        "            'training': False\n",
        "        })\n",
        "        # Compute the probabilities of starting and ending token and the selection probability.\n",
        "        search_encodings = full_encodings[:,0,:]\n",
        "        out_S, out_E, out_SEL = self.reader_evaluator(full_encodings, search_encodings)\n",
        "        return top100_indices, out_S, out_E, out_SEL\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        return [self.loss_tracker, self.start_acc_metric, self.end_acc_metric, self.sel_acc_metric]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhAi3lsPUOZ"
      },
      "source": [
        "### Training\n",
        "\n",
        "To train the model we need to:\n",
        "\n",
        "- Compile it defining the losses and optimizer.\n",
        "- Create a ground truth batch that we use for comparing the model's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCWx9AOlqaMq"
      },
      "outputs": [],
      "source": [
        "def create_trainable_QA_model(freeze_reader_up_to:int, \n",
        "                              questions_representations:dict, \n",
        "                              m:int=24):\n",
        "    local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "    print(\"Creating Reader model...\")\n",
        "    reader = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    \n",
        "    # Freeze some of the blocks of the reader.\n",
        "    for i in range(freeze_reader_up_to): # layers 0 to variable are frozen, successive layers learn\n",
        "        reader.distilbert.transformer.layer[i].trainable = False\n",
        "\n",
        "    print(\"Creating Question Answering model...\")\n",
        "    model = QuestionAnsweringModel(reader, questions_representations, m=m)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model (metrics are defined into the model)\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-5),\n",
        "        loss = keras.losses.CategoricalCrossentropy()\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5RLqioFsIon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d8a6fab-406b-4b88-b395-3b52f1347d2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Reader model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Question Answering model...\n",
            "Compiling...\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "questions_representations = {\n",
        "    'train': train_questions_encodings,\n",
        "    'val'  : val_questions_encodings,\n",
        "    'test' : test_questions_encodings,\n",
        "}\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    freeze_up_to = 3\n",
        "    with strategy.scope():\n",
        "        model_QA = create_trainable_QA_model(freeze_up_to, questions_representations, m=12)\n",
        "else:\n",
        "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        \"training_qa\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    freeze_up_to = 5\n",
        "    model_QA = create_trainable_QA_model(freeze_up_to, questions_representations, m=4)\n",
        "\n",
        "# Workaraound for saving locally when using cloud TPUs\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "# GPUs and local systems don't need the above specifications. We simply\n",
        "# create a pattern for the filename and let the callbacks deal with it.\n",
        "checkpoint_path = os.path.join(checkpoint_dir, \"qa_model.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffh1nGX0I9FT"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuXe_F8WmvlS"
      },
      "outputs": [],
      "source": [
        "DO_TRAINING = True\n",
        "PATIENCE    = 6\n",
        "EPOCHS      = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d3zQybm4mXNG",
        "outputId": "eb7afadd-0a52-4481-cc9a-3b43711a3f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-39bf4d800dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m# Show progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"<ipython-input-86-09b4c9ab4e73>\", line 213, in train_step\n        'training': True\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    TypeError: Exception encountered when calling layer \"question_answering_model_8\" (type QuestionAnsweringModel).\n    \n    in user code:\n    \n        File \"<ipython-input-86-09b4c9ab4e73>\", line 47, in call  *\n            full_encodings = self.reader({\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n            raise e.with_traceback(filtered_tb) from None\n    \n        TypeError: Exception encountered when calling layer \"tf_distil_bert_model_9\" (type TFDistilBertModel).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 544, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 546, in call  *\n                outputs = self.distilbert(\n            File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n        \n            TypeError: Exception encountered when calling layer \"distilbert\" (type TFDistilBertMainLayer).\n            \n            in user code:\n            \n                File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_tf_utils.py\", line 544, in run_call_with_unpacked_inputs  *\n                    return func(self, **unpacked_inputs)\n                File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 400, in call  *\n                    embedding_output = self.embeddings(input_ids, inputs_embeds=inputs_embeds)  # (bs, seq_length, dim)\n                File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n            \n                TypeError: Exception encountered when calling layer \"embeddings\" (type TFEmbeddings).\n                \n                in user code:\n                \n                    File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py\", line 122, in call  *\n                        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n                    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                \n                    TypeError: Exception encountered when calling layer \"LayerNorm\" (type LayerNormalization).\n                    \n                    Failed to convert elements of [1, 1, None, 1] to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n                    \n                    Call arguments received:\n                      • inputs=tf.Tensor(shape=(None, None, None, 768), dtype=float32)\n                \n                \n                Call arguments received:\n                  • input_ids=tf.Tensor(shape=(None, None, None), dtype=int32)\n                  • position_ids=None\n                  • inputs_embeds=None\n                  • training=tf.Tensor(shape=(), dtype=bool)\n            \n            \n            Call arguments received:\n              • self=tf.Tensor(shape=(None, None, None), dtype=int32)\n              • input_ids=None\n              • attention_mask=tf.Tensor(shape=(None, None, None), dtype=int32)\n              • head_mask=None\n              • inputs_embeds=None\n              • output_attentions=False\n              • output_hidden_states=False\n              • return_dict=True\n              • training=tf.Tensor(shape=(), dtype=bool)\n        \n        \n        Call arguments received:\n          • self={'input_ids': 'tf.Tensor(shape=(None, None, None), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None, None), dtype=int32)', 'training': 'tf.Tensor(shape=(), dtype=bool)'}\n          • input_ids=None\n          • attention_mask=None\n          • head_mask=None\n          • inputs_embeds=None\n          • output_attentions=None\n          • output_hidden_states=None\n          • return_dict=None\n          • training=False\n    \n    \n    Call arguments received:\n      • inputs={'input_ids': 'tf.Tensor(shape=(None, None, None), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(None, None, None), dtype=int32)', 'training': 'tf.Tensor(shape=(), dtype=bool)'}\n"
          ]
        }
      ],
      "source": [
        "if DO_TRAINING:\n",
        "    if not using_TPU:\n",
        "        # Tensorboard callback is not available on TPU\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        )\n",
        "    \n",
        "    # ModelCheckpoint callback is available both on TPU and GPU \n",
        "    # with the options parameter\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = checkpoint_path,\n",
        "        verbose=1,\n",
        "        save_weights_only = True,\n",
        "        save_best_only = True,\n",
        "        options=local_device_option\n",
        "    )\n",
        "\n",
        "    # Early stopping can be used by both hardware\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    callbacks = [es_callback, cp_callback]\n",
        "    if not using_TPU:\n",
        "        # These callback imply saving stuff on local disk, which cannot be \n",
        "        # done automatically using TPUs.\n",
        "        # Therefore, they are only active when using GPUs and local systems\n",
        "        callbacks.extend([tensorboard_callback])\n",
        "\n",
        "    # We fit the model\n",
        "    history = model_QA.fit(\n",
        "        dataset_train, \n",
        "        y=None,\n",
        "        validation_data=dataset_val,\n",
        "        epochs=EPOCHS, \n",
        "        callbacks=callbacks,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=0,\n",
        "        verbose=1 # Show progress bar\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I79iOHjGbE-"
      },
      "source": [
        "### Obtaining an answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWUaL844hyf6"
      },
      "outputs": [],
      "source": [
        "# def start_end_token_from_probabilities(\n",
        "#     pstartv: np.array, \n",
        "#     pendv: np.array, \n",
        "#     dim:int=512) -> List[List[int]]:\n",
        "#     '''\n",
        "#     Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
        "#     '''\n",
        "#     idxs = []\n",
        "#     for i in range(pstartv.shape[0]):\n",
        "#         # For each element in the batch, transform the vectors into matrices\n",
        "#         # by repeating them dim times:\n",
        "#         # - Vectors of starting probabilities are stacked on the columns\n",
        "#         pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
        "#         # - Vectors of ending probabilities are repeated on the rows\n",
        "#         pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
        "#         # Once we have the two matrices, we sum them (element-wise operation)\n",
        "#         # to obtain the scores of each combination\n",
        "#         sums = pstart + pend\n",
        "#         # We only care about the scores in the upper triangular part of the matrix\n",
        "#         # (where the ending index is greater than the starting index)\n",
        "#         # therefore we zero out the diagonal and the lower triangular area\n",
        "#         sums = np.triu(sums, k=1)\n",
        "#         # The most probable set of tokens is the one with highest score in the\n",
        "#         # remaining matrix. Through argmax we obtain its position.\n",
        "#         val = np.argmax(sums)\n",
        "#         # Since the starting probabilities are repeated on the columns, each element\n",
        "#         # is identified by the row. Ending probabilities are instead repeated on rows,\n",
        "#         # so each element is identified by the column.\n",
        "#         row = val // dim\n",
        "#         col = val - dim*row\n",
        "#         idxs.append([row,col])\n",
        "#     return idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CUVf45Cj0oI"
      },
      "outputs": [],
      "source": [
        "# answers_start_end = start_end_token_from_probabilities(probs_s, probs_e)\n",
        "# print(answers_start_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVfgDsb_GHIE"
      },
      "source": [
        "Finally, we can obtain the answers to the questions we have given the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcpI3LVz2vKY"
      },
      "outputs": [],
      "source": [
        "# best_indices = tf.squeeze(tf.gather(paragraphs['indexes'], tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "# best_offsets = tf.squeeze(tf.gather(pretokenized_val_paragraphs['offset_mapping'], best_indices))\n",
        "# best_offsets.shape, best_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWtARzUITEV"
      },
      "outputs": [],
      "source": [
        "# char_start_end = [(best_offsets[i][answers_start_end[i][0]][0].numpy(),\n",
        "#                    best_offsets[i][answers_start_end[i][1]][1].numpy())\n",
        "#                  for i in range(BATCH_SIZE)]\n",
        "# char_start_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktvdpz52fxOc"
      },
      "outputs": [],
      "source": [
        "# # Correction for answers arriving to the end of the sequence\n",
        "# for i in range(BATCH_SIZE):\n",
        "#     c = char_start_end[i]\n",
        "#     if c[1] >= c[0]:\n",
        "#         char_start_end[i] = (c[0], c[1])\n",
        "#     else:\n",
        "#         char_start_end[i] = (c[0], 1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6edjxYTw4k-t"
      },
      "outputs": [],
      "source": [
        "# for i, p in enumerate(best_indices.numpy()):\n",
        "#     print(val_paragraphs[p]['context'][char_start_end[i][0]:char_start_end[i][1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_B5U-_tKR6v"
      },
      "source": [
        "Of course, answers are extremely bad because we need to train the Dense layers selecting the start and end tokens, as well as the paragraph selector."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "68bcab510961ee730b63c1a3ea5fc4f15a05e7cdf00d3442986724f8b958dcdf"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32fab545874a4899a77bc60c1fe48fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c9112e1459a4ee79c8567fb98287f58",
              "IPY_MODEL_43a5c84a3c7c44b2b216b18c9949516b",
              "IPY_MODEL_40fd2b78300e4e3c8b1058d336426a86"
            ],
            "layout": "IPY_MODEL_a2f0d182e15148dd8a30e3d36e547ef6"
          }
        },
        "1c9112e1459a4ee79c8567fb98287f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01e5e40e05d14cfca245c7a626f88fa9",
            "placeholder": "​",
            "style": "IPY_MODEL_7f5a5db32b6e41139238fe1998c023a3",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "43a5c84a3c7c44b2b216b18c9949516b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d46bb2509d140e1b81b49b45371d83c",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1f46072e1b34001a5fd2ef3a78a4c16",
            "value": 28
          }
        },
        "40fd2b78300e4e3c8b1058d336426a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbcfd04cdd234503bc63c593cc903e79",
            "placeholder": "​",
            "style": "IPY_MODEL_73c87b0a961349c9a3ff4730259d612b",
            "value": " 28.0/28.0 [00:00&lt;00:00, 495B/s]"
          }
        },
        "a2f0d182e15148dd8a30e3d36e547ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01e5e40e05d14cfca245c7a626f88fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5a5db32b6e41139238fe1998c023a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d46bb2509d140e1b81b49b45371d83c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f46072e1b34001a5fd2ef3a78a4c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbcfd04cdd234503bc63c593cc903e79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c87b0a961349c9a3ff4730259d612b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b0dd8ad11f240b6a09588ec480f318f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47d173092b564043bad47f9c8a150fb3",
              "IPY_MODEL_14e1427d2de44a638df0ca3955bf42c5",
              "IPY_MODEL_05c33627cbe84b8f860187314ffbbe7b"
            ],
            "layout": "IPY_MODEL_e73d8a04d67e4e0989fdce1f37edadca"
          }
        },
        "47d173092b564043bad47f9c8a150fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bde53eb3e4d9407eb49f56c878c107fe",
            "placeholder": "​",
            "style": "IPY_MODEL_465db849ce304f23beb85644fd7f71ee",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "14e1427d2de44a638df0ca3955bf42c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03cb13ebcd864c469c0013d858040ca9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6b81abf31c3443aa2e44402355560f1",
            "value": 231508
          }
        },
        "05c33627cbe84b8f860187314ffbbe7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aa71f1c43ab4de2ab1d20d4f3d91b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_19c2f53cfbb34d74975c9d76e5d54bf2",
            "value": " 226k/226k [00:00&lt;00:00, 5.80kB/s]"
          }
        },
        "e73d8a04d67e4e0989fdce1f37edadca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bde53eb3e4d9407eb49f56c878c107fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465db849ce304f23beb85644fd7f71ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03cb13ebcd864c469c0013d858040ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6b81abf31c3443aa2e44402355560f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1aa71f1c43ab4de2ab1d20d4f3d91b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c2f53cfbb34d74975c9d76e5d54bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecdd6deeda8242c69f7ef9a395e17b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c28f7e123c33422bb4b6bbfa8d56c4a4",
              "IPY_MODEL_14dd8ebc961d4b229ec5c8873f63dbf4",
              "IPY_MODEL_514711368e02481e8427889bf9552145"
            ],
            "layout": "IPY_MODEL_9ff597a06b6943068ae8361d353d8145"
          }
        },
        "c28f7e123c33422bb4b6bbfa8d56c4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e78d741cb8b14472a000413f91ed72ec",
            "placeholder": "​",
            "style": "IPY_MODEL_2e8a8b2d08a1459084524a3ae4cdb4e7",
            "value": "Downloading tokenizer.json: 100%"
          }
        },
        "14dd8ebc961d4b229ec5c8873f63dbf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33105da64f614efc964546e5cd34f0c0",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18255275cb6b4550a45658257a040b4a",
            "value": 466062
          }
        },
        "514711368e02481e8427889bf9552145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0984a727b2754fa0bb03ba731d42980e",
            "placeholder": "​",
            "style": "IPY_MODEL_504a6b0dd2174000966a2341608709a2",
            "value": " 455k/455k [00:00&lt;00:00, 7.21kB/s]"
          }
        },
        "9ff597a06b6943068ae8361d353d8145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78d741cb8b14472a000413f91ed72ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8a8b2d08a1459084524a3ae4cdb4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33105da64f614efc964546e5cd34f0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18255275cb6b4550a45658257a040b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0984a727b2754fa0bb03ba731d42980e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "504a6b0dd2174000966a2341608709a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16ea6f9e732c43b6bdafa1c663fcf32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78061091bbf745cb8c644756254e1208",
              "IPY_MODEL_abfcc4c5fdda433084fdc7f7330a6c0b",
              "IPY_MODEL_39beab7e013c4d238bc6296fcbde1705"
            ],
            "layout": "IPY_MODEL_831af2b12ad8434e84d1c336d6e366bc"
          }
        },
        "78061091bbf745cb8c644756254e1208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f8bdc87f1244f108bbdc9578f79dc14",
            "placeholder": "​",
            "style": "IPY_MODEL_771d7b4b63e141ccad93ab6676f446bc",
            "value": "Downloading config.json: 100%"
          }
        },
        "abfcc4c5fdda433084fdc7f7330a6c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_606326a7455349f6861930451fd958e3",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94ac7940276342628e37f7699342906e",
            "value": 483
          }
        },
        "39beab7e013c4d238bc6296fcbde1705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2337551c0da4ec3945f12adf15e06b5",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb684657b04422da4f03552f45fc3a7",
            "value": " 483/483 [00:00&lt;00:00, 7.17kB/s]"
          }
        },
        "831af2b12ad8434e84d1c336d6e366bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f8bdc87f1244f108bbdc9578f79dc14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "771d7b4b63e141ccad93ab6676f446bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606326a7455349f6861930451fd958e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ac7940276342628e37f7699342906e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2337551c0da4ec3945f12adf15e06b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb684657b04422da4f03552f45fc3a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}