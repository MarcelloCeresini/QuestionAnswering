{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
    "# COLAB ONLY CELLS\n",
    "#try:\n",
    "#    import google.colab\n",
    "#    IN_COLAB = True\n",
    "#    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
    "#    !nvidia-smi                 # Check which GPU has been chosen for us\n",
    "#    !rm -rf logs\n",
    "#    # Download the dataset from personal drive\n",
    "#    !mkdir data\n",
    "#    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "#except:\n",
    "#    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QGC_45Iab0xm"
   },
   "outputs": [],
   "source": [
    "# PRIVATE CELL\n",
    "git_token = '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'\n",
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfNL_Qz3L2Iv",
    "outputId": "585e4ee5-55f3-43f3-aa48-f5c8924241c0"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !nvidia-smi             # Check which GPU has been chosen for us\n",
    "    !rm -rf logs\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/GitHub/\n",
    "    !git clone https://{git_token}@github.com/{username}/{repository}\n",
    "    %cd {repository}\n",
    "    %ls\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tX8o7g0cL2Iz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 23:39:06.641456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:06.718222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:06.719032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:06.720634: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-16 23:39:06.722067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:06.722408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:06.722674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:07.421649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:07.421961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:07.422224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-16 23:39:07.422458: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-01-16 23:39:07.422483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3735 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-01-16 23:39:07.444396: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from config import Config\n",
    "config = Config()\n",
    "import utils\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2060, compute capability 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 23:39:08.905684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = os.path.join(config.ROOT_PATH, 'data', 'training_set.json') # comment this if directory works differently\n",
    "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
    "questions = utils.read_question_set(TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
    "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
    "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
    "\n",
    "# we also create a small training set to test the model while building it, just to speed up\n",
    "\n",
    "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
    "small_train_dataset = {'data': small_data}\n",
    "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
    "small_val_dataset = {'data': small_val_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path training dataset:  /home/marcello/github/NLP/Assignements/QuestionAnswering/data/full_datasets/train_ds_training\n",
      "Path validation dataset:  /home/marcello/github/NLP/Assignements/QuestionAnswering/data/full_datasets/val_ds_training\n",
      "65064\n",
      "Number of samples in the training dataset:  None\n",
      "22535\n",
      "Number of samples in the validation dataset:  None\n",
      "2034\n",
      "Number of batches in the training dataset:  None\n",
      "705\n",
      "Number of batches in the validation dataset:  None\n"
     ]
    }
   ],
   "source": [
    "full_dataset = True # choose between full and small dataset\n",
    "\n",
    "if full_dataset:\n",
    "    TRAIN_DATASET = train_dataset\n",
    "    VAL_DATASET = val_dataset\n",
    "else:\n",
    "    TRAIN_DATASET = small_train_dataset\n",
    "    VAL_DATASET = small_val_dataset\n",
    "\n",
    "create_and_save = False     # fully create the dataset in RAM, and then save it on disk\n",
    "load = True                 # load a previously created dataset from disk\n",
    "generator = False           # if not enough RAM, create a dataset through a generator\n",
    "\n",
    "for_training = True         # returns a (feature, labels) dataset used in the fit method of the model\n",
    "NER_attention = False       # returns a (feature, id) dataset used during inference\n",
    "\n",
    "# if you need to save or load a model, choose the right path according to the previous 2 flags\n",
    "if create_and_save or load:\n",
    "    if for_training:\n",
    "        if NER_attention:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING_NER\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING_NER\n",
    "        else:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING\n",
    "    else:\n",
    "        if NER_attention:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE_NER\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE_NER\n",
    "        else:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE\n",
    "\n",
    "print(\"Path training dataset: \", PATH_TRAIN)\n",
    "print(\"Path validation dataset: \", PATH_VAL)\n",
    "\n",
    "# dataset creation\n",
    "# for small dataset, just create it and store it in RAM, it's fast\n",
    "if create_and_save or not full_dataset: # for full dataset, you can either create it and save it on disk\n",
    "    train_ds = utils.create_full_dataset(TRAIN_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
    "    val_ds = utils.create_full_dataset(VAL_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
    "    if for_training and full_dataset: # only for full datasets, save them on disk\n",
    "        tf.data.experimental.save(train_ds, PATH_TRAIN)\n",
    "        tf.data.experimental.save(val_ds, PATH_VAL)\n",
    "elif load and full_dataset: # only for full datasets, you can load the previously created dataset from disk\n",
    "    train_ds = tf.data.experimental.load(PATH_TRAIN)\n",
    "    val_ds = tf.data.experimental.load(PATH_VAL)\n",
    "elif generator and full_dataset: # only for full datasets, if there is not enough RAM, you can create a dataset from a generator\n",
    "    train_ds = utils.create_dataset_and_ids(TRAIN_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
    "    val_ds = utils.create_dataset_and_ids(VAL_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
    "else: # if you don't enter in any of the above, something is wrong\n",
    "    raise Exception(\"Something wrong with dataset creation\")\n",
    "\n",
    "\n",
    "print(\"Number of samples in the training dataset: \", len(train_ds))\n",
    "print(\"Number of samples in the validation dataset: \", len(val_ds))\n",
    "\n",
    "# batch the dataset and prefetch to increase speed\n",
    "train_ds = train_ds.batch(config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(config.VAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Number of batches in the training dataset: \", len(train_ds))\n",
    "print(\"Number of batches in the validation dataset: \", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['out_E', 'out_S'])\n"
     ]
    }
   ],
   "source": [
    "# Check if the dataset has the wanted data inside\n",
    "for batch in train_ds.take(1):\n",
    "    print(batch[0].keys())\n",
    "    print(batch[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Chioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_training = False\n",
    "train_separate_layers = True\n",
    "NER_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normal_training:\n",
    "    \n",
    "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_normal\",  \"cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    model = config.create_standard_model([3, 4, 5, 6])\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only = False\n",
    "    )\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = 3\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        epochs=10, \n",
    "        callbacks=[\n",
    "            cp_callback,\n",
    "            es_callback\n",
    "        ],\n",
    "        use_multiprocessing = True,\n",
    "        initial_epoch=0\n",
    "        )\n",
    "\n",
    "\n",
    "    history = history.history\n",
    "\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
    "    plt.plot(losses)\n",
    "    plt.legend(losses.columns)\n",
    "\n",
    "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
    "    plt.plot(accs)\n",
    "    plt.legend(accs.columns)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training model with head attached to layer number 6 -----------\n",
      "Epoch 1/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0232 - out_S_loss: 0.0118 - out_E_loss: 0.0114 - out_S_accuracy: 0.0445 - out_E_accuracy: 0.0423\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 00:06:08.333461: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5625815040 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034/2034 [==============================] - 1620s 794ms/step - loss: 0.0232 - out_S_loss: 0.0118 - out_E_loss: 0.0114 - out_S_accuracy: 0.0445 - out_E_accuracy: 0.0423 - val_loss: 0.0208 - val_out_S_loss: 0.0107 - val_out_E_loss: 0.0101 - val_out_S_accuracy: 0.0735 - val_out_E_accuracy: 0.0775\n",
      "Epoch 2/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0202 - out_S_loss: 0.0105 - out_E_loss: 0.0098 - out_S_accuracy: 0.0820 - out_E_accuracy: 0.0875\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 00:33:08.784642: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5625815040 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034/2034 [==============================] - 1620s 797ms/step - loss: 0.0202 - out_S_loss: 0.0105 - out_E_loss: 0.0098 - out_S_accuracy: 0.0820 - out_E_accuracy: 0.0875 - val_loss: 0.0195 - val_out_S_loss: 0.0101 - val_out_E_loss: 0.0094 - val_out_S_accuracy: 0.0965 - val_out_E_accuracy: 0.1032\n",
      "Epoch 3/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0962 - out_E_accuracy: 0.1027\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 01:00:13.462371: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5625815040 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034/2034 [==============================] - 1625s 799ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0962 - out_E_accuracy: 0.1027 - val_loss: 0.0190 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0091 - val_out_S_accuracy: 0.1061 - val_out_E_accuracy: 0.1155\n",
      "Epoch 4/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0190 - out_S_loss: 0.0099 - out_E_loss: 0.0091 - out_S_accuracy: 0.1043 - out_E_accuracy: 0.1114\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 01:26:22.752213: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5625815040 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034/2034 [==============================] - 1569s 772ms/step - loss: 0.0190 - out_S_loss: 0.0099 - out_E_loss: 0.0091 - out_S_accuracy: 0.1043 - out_E_accuracy: 0.1114 - val_loss: 0.0187 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0090 - val_out_S_accuracy: 0.1120 - val_out_E_accuracy: 0.1218\n",
      "Epoch 5/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0090 - out_S_accuracy: 0.1072 - out_E_accuracy: 0.1152\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 01:52:08.202899: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 5625815040 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0090 - out_S_accuracy: 0.1072 - out_E_accuracy: 0.1152 - val_loss: 0.0186 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.1153 - val_out_E_accuracy: 0.1257\n",
      "Epoch 6/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0097 - out_E_loss: 0.0090 - out_S_accuracy: 0.1123 - out_E_accuracy: 0.1196\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0187 - out_S_loss: 0.0097 - out_E_loss: 0.0090 - out_S_accuracy: 0.1123 - out_E_accuracy: 0.1196 - val_loss: 0.0185 - val_out_S_loss: 0.0096 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.1185 - val_out_E_accuracy: 0.1290\n",
      "Epoch 7/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0186 - out_S_loss: 0.0097 - out_E_loss: 0.0089 - out_S_accuracy: 0.1128 - out_E_accuracy: 0.1189\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 1544s 759ms/step - loss: 0.0186 - out_S_loss: 0.0097 - out_E_loss: 0.0089 - out_S_accuracy: 0.1128 - out_E_accuracy: 0.1189 - val_loss: 0.0184 - val_out_S_loss: 0.0096 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.1203 - val_out_E_accuracy: 0.1324\n",
      "Epoch 8/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0186 - out_S_loss: 0.0097 - out_E_loss: 0.0089 - out_S_accuracy: 0.1155 - out_E_accuracy: 0.1232\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0186 - out_S_loss: 0.0097 - out_E_loss: 0.0089 - out_S_accuracy: 0.1155 - out_E_accuracy: 0.1232 - val_loss: 0.0183 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.1213 - val_out_E_accuracy: 0.1340\n",
      "Epoch 9/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1159 - out_E_accuracy: 0.1245\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 1546s 760ms/step - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1159 - out_E_accuracy: 0.1245 - val_loss: 0.0183 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.1221 - val_out_E_accuracy: 0.1358\n",
      "Epoch 10/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1176 - out_E_accuracy: 0.1257\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 1545s 759ms/step - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1176 - out_E_accuracy: 0.1257 - val_loss: 0.0182 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1233 - val_out_E_accuracy: 0.1367\n",
      "Epoch 11/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1272\n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1272 - val_loss: 0.0182 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1246 - val_out_E_accuracy: 0.1380\n",
      "Epoch 12/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1186 - out_E_accuracy: 0.1261\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1186 - out_E_accuracy: 0.1261 - val_loss: 0.0182 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1249 - val_out_E_accuracy: 0.1394\n",
      "Epoch 13/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1280\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1280 - val_loss: 0.0182 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1251 - val_out_E_accuracy: 0.1395\n",
      "Epoch 14/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1188 - out_E_accuracy: 0.1278\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 1545s 760ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1188 - out_E_accuracy: 0.1278 - val_loss: 0.0181 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1254 - val_out_E_accuracy: 0.1405\n",
      "Epoch 15/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0095 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1300\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_6/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 1543s 759ms/step - loss: 0.0184 - out_S_loss: 0.0095 - out_E_loss: 0.0088 - out_S_accuracy: 0.1201 - out_E_accuracy: 0.1300 - val_loss: 0.0181 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1259 - val_out_E_accuracy: 0.1412\n",
      "\n",
      "----------- Training model with head attached to layer number 5 -----------\n",
      "Epoch 1/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0217 - out_S_loss: 0.0114 - out_E_loss: 0.0103 - out_S_accuracy: 0.0560 - out_E_accuracy: 0.0693\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0001.ckpt\n",
      "2034/2034 [==============================] - 1306s 639ms/step - loss: 0.0217 - out_S_loss: 0.0114 - out_E_loss: 0.0103 - out_S_accuracy: 0.0560 - out_E_accuracy: 0.0693 - val_loss: 0.0193 - val_out_S_loss: 0.0101 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0967 - val_out_E_accuracy: 0.1075\n",
      "Epoch 2/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.1030 - out_E_accuracy: 0.1158\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0002.ckpt\n",
      "2034/2034 [==============================] - 1300s 639ms/step - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.1030 - out_E_accuracy: 0.1158 - val_loss: 0.0184 - val_out_S_loss: 0.0096 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.1202 - val_out_E_accuracy: 0.1318\n",
      "Epoch 3/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1182 - out_E_accuracy: 0.1277\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0003.ckpt\n",
      "2034/2034 [==============================] - 1299s 639ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1182 - out_E_accuracy: 0.1277 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1296 - val_out_E_accuracy: 0.1431\n",
      "Epoch 4/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0181 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1265 - out_E_accuracy: 0.1374\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0004.ckpt\n",
      "2034/2034 [==============================] - 1328s 653ms/step - loss: 0.0181 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1265 - out_E_accuracy: 0.1374 - val_loss: 0.0179 - val_out_S_loss: 0.0093 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1349 - val_out_E_accuracy: 0.1484\n",
      "Epoch 5/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0180 - out_S_loss: 0.0094 - out_E_loss: 0.0086 - out_S_accuracy: 0.1301 - out_E_accuracy: 0.1392\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0005.ckpt\n",
      "2034/2034 [==============================] - 1275s 627ms/step - loss: 0.0180 - out_S_loss: 0.0094 - out_E_loss: 0.0086 - out_S_accuracy: 0.1301 - out_E_accuracy: 0.1392 - val_loss: 0.0178 - val_out_S_loss: 0.0093 - val_out_E_loss: 0.0085 - val_out_S_accuracy: 0.1379 - val_out_E_accuracy: 0.1514\n",
      "Epoch 6/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1333 - out_E_accuracy: 0.1436\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 1260s 620ms/step - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1333 - out_E_accuracy: 0.1436 - val_loss: 0.0177 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0085 - val_out_S_accuracy: 0.1404 - val_out_E_accuracy: 0.1543\n",
      "Epoch 7/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1344 - out_E_accuracy: 0.1445\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 1257s 618ms/step - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1344 - out_E_accuracy: 0.1445 - val_loss: 0.0177 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0085 - val_out_S_accuracy: 0.1432 - val_out_E_accuracy: 0.1566\n",
      "Epoch 8/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1359 - out_E_accuracy: 0.1455\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 1257s 618ms/step - loss: 0.0179 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1359 - out_E_accuracy: 0.1455 - val_loss: 0.0176 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0085 - val_out_S_accuracy: 0.1441 - val_out_E_accuracy: 0.1577\n",
      "Epoch 9/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1372 - out_E_accuracy: 0.1476\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 1257s 618ms/step - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1372 - out_E_accuracy: 0.1476 - val_loss: 0.0176 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1460 - val_out_E_accuracy: 0.1589\n",
      "Epoch 10/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1364 - out_E_accuracy: 0.1476\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 1262s 620ms/step - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1364 - out_E_accuracy: 0.1476 - val_loss: 0.0176 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1467 - val_out_E_accuracy: 0.1592\n",
      "Epoch 11/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1393 - out_E_accuracy: 0.1487\n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 1266s 622ms/step - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1393 - out_E_accuracy: 0.1487 - val_loss: 0.0176 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1471 - val_out_E_accuracy: 0.1596\n",
      "Epoch 12/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1376 - out_E_accuracy: 0.1483\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 1266s 622ms/step - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0085 - out_S_accuracy: 0.1376 - out_E_accuracy: 0.1483 - val_loss: 0.0176 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1468 - val_out_E_accuracy: 0.1602\n",
      "Epoch 13/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1399 - out_E_accuracy: 0.1494\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 1267s 623ms/step - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1399 - out_E_accuracy: 0.1494 - val_loss: 0.0176 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1478 - val_out_E_accuracy: 0.1604\n",
      "Epoch 14/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1381 - out_E_accuracy: 0.1493\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 1262s 621ms/step - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1381 - out_E_accuracy: 0.1493 - val_loss: 0.0175 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1480 - val_out_E_accuracy: 0.1609\n",
      "Epoch 15/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1387 - out_E_accuracy: 0.1494\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_5/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 1265s 622ms/step - loss: 0.0178 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1387 - out_E_accuracy: 0.1494 - val_loss: 0.0175 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1490 - val_out_E_accuracy: 0.1617\n",
      "\n",
      "----------- Training model with head attached to layer number 4 -----------\n",
      "Epoch 1/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0209 - out_S_loss: 0.0109 - out_E_loss: 0.0101 - out_S_accuracy: 0.0774 - out_E_accuracy: 0.0743\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0001.ckpt\n",
      "2034/2034 [==============================] - 1032s 505ms/step - loss: 0.0209 - out_S_loss: 0.0109 - out_E_loss: 0.0101 - out_S_accuracy: 0.0774 - out_E_accuracy: 0.0743 - val_loss: 0.0187 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0090 - val_out_S_accuracy: 0.1110 - val_out_E_accuracy: 0.1148\n",
      "Epoch 2/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1153 - out_E_accuracy: 0.1211\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0002.ckpt\n",
      "2034/2034 [==============================] - 1024s 503ms/step - loss: 0.0185 - out_S_loss: 0.0096 - out_E_loss: 0.0089 - out_S_accuracy: 0.1153 - out_E_accuracy: 0.1211 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1291 - val_out_E_accuracy: 0.1367\n",
      "Epoch 3/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0180 - out_S_loss: 0.0094 - out_E_loss: 0.0087 - out_S_accuracy: 0.1277 - out_E_accuracy: 0.1339\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0003.ckpt\n",
      "2034/2034 [==============================] - 1023s 503ms/step - loss: 0.0180 - out_S_loss: 0.0094 - out_E_loss: 0.0087 - out_S_accuracy: 0.1277 - out_E_accuracy: 0.1339 - val_loss: 0.0177 - val_out_S_loss: 0.0092 - val_out_E_loss: 0.0085 - val_out_S_accuracy: 0.1364 - val_out_E_accuracy: 0.1496\n",
      "Epoch 4/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1331 - out_E_accuracy: 0.1436\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0004.ckpt\n",
      "2034/2034 [==============================] - 1023s 503ms/step - loss: 0.0178 - out_S_loss: 0.0093 - out_E_loss: 0.0086 - out_S_accuracy: 0.1331 - out_E_accuracy: 0.1436 - val_loss: 0.0176 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1433 - val_out_E_accuracy: 0.1558\n",
      "Epoch 5/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0177 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1374 - out_E_accuracy: 0.1470\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0005.ckpt\n",
      "2034/2034 [==============================] - 1016s 500ms/step - loss: 0.0177 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1374 - out_E_accuracy: 0.1470 - val_loss: 0.0175 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0084 - val_out_S_accuracy: 0.1469 - val_out_E_accuracy: 0.1610\n",
      "Epoch 6/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1389 - out_E_accuracy: 0.1513\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 1014s 498ms/step - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0085 - out_S_accuracy: 0.1389 - out_E_accuracy: 0.1513 - val_loss: 0.0174 - val_out_S_loss: 0.0091 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1494 - val_out_E_accuracy: 0.1650\n",
      "Epoch 7/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0084 - out_S_accuracy: 0.1417 - out_E_accuracy: 0.1519\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 1014s 499ms/step - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0084 - out_S_accuracy: 0.1417 - out_E_accuracy: 0.1519 - val_loss: 0.0174 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1516 - val_out_E_accuracy: 0.1667\n",
      "Epoch 8/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0084 - out_S_accuracy: 0.1420 - out_E_accuracy: 0.1530\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 1012s 497ms/step - loss: 0.0176 - out_S_loss: 0.0092 - out_E_loss: 0.0084 - out_S_accuracy: 0.1420 - out_E_accuracy: 0.1530 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1534 - val_out_E_accuracy: 0.1683\n",
      "Epoch 9/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1417 - out_E_accuracy: 0.1546\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 1014s 498ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1417 - out_E_accuracy: 0.1546 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1546 - val_out_E_accuracy: 0.1693\n",
      "Epoch 10/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1425 - out_E_accuracy: 0.1565\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 1015s 499ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1425 - out_E_accuracy: 0.1565 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1549 - val_out_E_accuracy: 0.1697\n",
      "Epoch 11/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1446 - out_E_accuracy: 0.1570\n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 1014s 499ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1446 - out_E_accuracy: 0.1570 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1559 - val_out_E_accuracy: 0.1705\n",
      "Epoch 12/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1435 - out_E_accuracy: 0.1567\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 1020s 501ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1435 - out_E_accuracy: 0.1567 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1561 - val_out_E_accuracy: 0.1708\n",
      "Epoch 13/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1444 - out_E_accuracy: 0.1562\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 1013s 498ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1444 - out_E_accuracy: 0.1562 - val_loss: 0.0173 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1559 - val_out_E_accuracy: 0.1715\n",
      "Epoch 14/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1443 - out_E_accuracy: 0.1566\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 1013s 498ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1443 - out_E_accuracy: 0.1566 - val_loss: 0.0172 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1570 - val_out_E_accuracy: 0.1713\n",
      "Epoch 15/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1445 - out_E_accuracy: 0.1576\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_4/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 1011s 497ms/step - loss: 0.0175 - out_S_loss: 0.0091 - out_E_loss: 0.0084 - out_S_accuracy: 0.1445 - out_E_accuracy: 0.1576 - val_loss: 0.0172 - val_out_S_loss: 0.0090 - val_out_E_loss: 0.0083 - val_out_S_accuracy: 0.1570 - val_out_E_accuracy: 0.1720\n",
      "\n",
      "----------- Training model with head attached to layer number 3 -----------\n",
      "Epoch 1/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0213 - out_S_loss: 0.0111 - out_E_loss: 0.0102 - out_S_accuracy: 0.0592 - out_E_accuracy: 0.0681\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0001.ckpt\n",
      "2034/2034 [==============================] - 780s 381ms/step - loss: 0.0213 - out_S_loss: 0.0111 - out_E_loss: 0.0102 - out_S_accuracy: 0.0592 - out_E_accuracy: 0.0681 - val_loss: 0.0193 - val_out_S_loss: 0.0101 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0874 - val_out_E_accuracy: 0.0956\n",
      "Epoch 2/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0191 - out_S_loss: 0.0100 - out_E_loss: 0.0091 - out_S_accuracy: 0.0900 - out_E_accuracy: 0.0959\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0002.ckpt\n",
      "2034/2034 [==============================] - 774s 380ms/step - loss: 0.0191 - out_S_loss: 0.0100 - out_E_loss: 0.0091 - out_S_accuracy: 0.0900 - out_E_accuracy: 0.0959 - val_loss: 0.0186 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.0988 - val_out_E_accuracy: 0.1069\n",
      "Epoch 3/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0988 - out_E_accuracy: 0.1041\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0003.ckpt\n",
      "2034/2034 [==============================] - 774s 381ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0988 - out_E_accuracy: 0.1041 - val_loss: 0.0184 - val_out_S_loss: 0.0096 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.1043 - val_out_E_accuracy: 0.1131\n",
      "Epoch 4/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0185 - out_S_loss: 0.0097 - out_E_loss: 0.0088 - out_S_accuracy: 0.1008 - out_E_accuracy: 0.1080\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0004.ckpt\n",
      "2034/2034 [==============================] - 775s 381ms/step - loss: 0.0185 - out_S_loss: 0.0097 - out_E_loss: 0.0088 - out_S_accuracy: 0.1008 - out_E_accuracy: 0.1080 - val_loss: 0.0183 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1067 - val_out_E_accuracy: 0.1168\n",
      "Epoch 5/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1040 - out_E_accuracy: 0.1089\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0005.ckpt\n",
      "2034/2034 [==============================] - 776s 382ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1040 - out_E_accuracy: 0.1089 - val_loss: 0.0182 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1080 - val_out_E_accuracy: 0.1195\n",
      "Epoch 6/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1063 - out_E_accuracy: 0.1137\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 775s 381ms/step - loss: 0.0184 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1063 - out_E_accuracy: 0.1137 - val_loss: 0.0182 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1102 - val_out_E_accuracy: 0.1211\n",
      "Epoch 7/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0183 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1050 - out_E_accuracy: 0.1122\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 762s 375ms/step - loss: 0.0183 - out_S_loss: 0.0096 - out_E_loss: 0.0088 - out_S_accuracy: 0.1050 - out_E_accuracy: 0.1122 - val_loss: 0.0181 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0087 - val_out_S_accuracy: 0.1108 - val_out_E_accuracy: 0.1231\n",
      "Epoch 8/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0183 - out_S_loss: 0.0096 - out_E_loss: 0.0087 - out_S_accuracy: 0.1074 - out_E_accuracy: 0.1156\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 762s 375ms/step - loss: 0.0183 - out_S_loss: 0.0096 - out_E_loss: 0.0087 - out_S_accuracy: 0.1074 - out_E_accuracy: 0.1156 - val_loss: 0.0181 - val_out_S_loss: 0.0095 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1109 - val_out_E_accuracy: 0.1244\n",
      "Epoch 9/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0183 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1075 - out_E_accuracy: 0.1163\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 776s 382ms/step - loss: 0.0183 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1075 - out_E_accuracy: 0.1163 - val_loss: 0.0181 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1118 - val_out_E_accuracy: 0.1249\n",
      "Epoch 10/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0183 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1077 - out_E_accuracy: 0.1170\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 779s 383ms/step - loss: 0.0183 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1077 - out_E_accuracy: 0.1170 - val_loss: 0.0181 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1124 - val_out_E_accuracy: 0.1250\n",
      "Epoch 11/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1086 - out_E_accuracy: 0.1167\n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 903s 444ms/step - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1086 - out_E_accuracy: 0.1167 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1122 - val_out_E_accuracy: 0.1249\n",
      "Epoch 12/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1080 - out_E_accuracy: 0.1157\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 839s 413ms/step - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1080 - out_E_accuracy: 0.1157 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1128 - val_out_E_accuracy: 0.1260\n",
      "Epoch 13/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1086 - out_E_accuracy: 0.1175\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 773s 380ms/step - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1086 - out_E_accuracy: 0.1175 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1128 - val_out_E_accuracy: 0.1256\n",
      "Epoch 14/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1091 - out_E_accuracy: 0.1165\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 763s 375ms/step - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1091 - out_E_accuracy: 0.1165 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1132 - val_out_E_accuracy: 0.1269\n",
      "Epoch 15/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1078 - out_E_accuracy: 0.1169\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_3/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 763s 375ms/step - loss: 0.0182 - out_S_loss: 0.0095 - out_E_loss: 0.0087 - out_S_accuracy: 0.1078 - out_E_accuracy: 0.1169 - val_loss: 0.0180 - val_out_S_loss: 0.0094 - val_out_E_loss: 0.0086 - val_out_S_accuracy: 0.1134 - val_out_E_accuracy: 0.1270\n",
      "\n",
      "----------- Training model with head attached to layer number 2 -----------\n",
      "Epoch 1/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0215 - out_S_loss: 0.0111 - out_E_loss: 0.0104 - out_S_accuracy: 0.0551 - out_E_accuracy: 0.0618\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0001.ckpt\n",
      "2034/2034 [==============================] - 530s 259ms/step - loss: 0.0215 - out_S_loss: 0.0111 - out_E_loss: 0.0104 - out_S_accuracy: 0.0551 - out_E_accuracy: 0.0618 - val_loss: 0.0195 - val_out_S_loss: 0.0102 - val_out_E_loss: 0.0093 - val_out_S_accuracy: 0.0773 - val_out_E_accuracy: 0.0879\n",
      "Epoch 2/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0102 - out_E_loss: 0.0093 - out_S_accuracy: 0.0802 - out_E_accuracy: 0.0881\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0002.ckpt\n",
      "2034/2034 [==============================] - 525s 258ms/step - loss: 0.0194 - out_S_loss: 0.0102 - out_E_loss: 0.0093 - out_S_accuracy: 0.0802 - out_E_accuracy: 0.0881 - val_loss: 0.0190 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0091 - val_out_S_accuracy: 0.0882 - val_out_E_accuracy: 0.0971\n",
      "Epoch 3/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0191 - out_S_loss: 0.0100 - out_E_loss: 0.0091 - out_S_accuracy: 0.0841 - out_E_accuracy: 0.0950\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0003.ckpt\n",
      "2034/2034 [==============================] - 527s 259ms/step - loss: 0.0191 - out_S_loss: 0.0100 - out_E_loss: 0.0091 - out_S_accuracy: 0.0841 - out_E_accuracy: 0.0950 - val_loss: 0.0188 - val_out_S_loss: 0.0098 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.0912 - val_out_E_accuracy: 0.1010\n",
      "Epoch 4/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0890 - out_E_accuracy: 0.0966\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0004.ckpt\n",
      "2034/2034 [==============================] - 527s 259ms/step - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0890 - out_E_accuracy: 0.0966 - val_loss: 0.0187 - val_out_S_loss: 0.0098 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.0934 - val_out_E_accuracy: 0.1019\n",
      "Epoch 5/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0894 - out_E_accuracy: 0.0968  ETA: 11s - loss: 0.0189 - out_S_loss: 0.0099 - out_E_l\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0005.ckpt\n",
      "2034/2034 [==============================] - 528s 260ms/step - loss: 0.0189 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0894 - out_E_accuracy: 0.0968 - val_loss: 0.0186 - val_out_S_loss: 0.0098 - val_out_E_loss: 0.0089 - val_out_S_accuracy: 0.0939 - val_out_E_accuracy: 0.1030\n",
      "Epoch 6/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0188 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0891 - out_E_accuracy: 0.0995  ETA: 12s - loss: 0.0188 - out_S_loss\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 532s 262ms/step - loss: 0.0188 - out_S_loss: 0.0099 - out_E_loss: 0.0090 - out_S_accuracy: 0.0891 - out_E_accuracy: 0.0995 - val_loss: 0.0186 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0944 - val_out_E_accuracy: 0.1047\n",
      "Epoch 7/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0188 - out_S_loss: 0.0099 - out_E_loss: 0.0089 - out_S_accuracy: 0.0890 - out_E_accuracy: 0.0994\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 537s 264ms/step - loss: 0.0188 - out_S_loss: 0.0099 - out_E_loss: 0.0089 - out_S_accuracy: 0.0890 - out_E_accuracy: 0.0994 - val_loss: 0.0186 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0953 - val_out_E_accuracy: 0.1038\n",
      "Epoch 8/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0901 - out_E_accuracy: 0.0994\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 528s 260ms/step - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0901 - out_E_accuracy: 0.0994 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0948 - val_out_E_accuracy: 0.1045\n",
      "Epoch 9/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0914 - out_E_accuracy: 0.1000\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 528s 260ms/step - loss: 0.0188 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0914 - out_E_accuracy: 0.1000 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0953 - val_out_E_accuracy: 0.1042\n",
      "Epoch 10/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0918 - out_E_accuracy: 0.1008\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 529s 260ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0918 - out_E_accuracy: 0.1008 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0954 - val_out_E_accuracy: 0.1052\n",
      "Epoch 11/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0922 - out_E_accuracy: 0.1000- ETA: 4s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0922 \n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 528s 259ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0922 - out_E_accuracy: 0.1000 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0956 - val_out_E_accuracy: 0.1049\n",
      "Epoch 12/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0914 - out_E_accuracy: 0.1010\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 528s 260ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0914 - out_E_accuracy: 0.1010 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0966 - val_out_E_accuracy: 0.1050\n",
      "Epoch 13/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0920 - out_E_accuracy: 0.1022\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 527s 259ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0920 - out_E_accuracy: 0.1022 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0967 - val_out_E_accuracy: 0.1053\n",
      "Epoch 14/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0909 - out_E_accuracy: 0.1000\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 528s 260ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0909 - out_E_accuracy: 0.1000 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0966 - val_out_E_accuracy: 0.1051\n",
      "Epoch 15/15\n",
      "2034/2034 [==============================] - ETA: 0s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0922 - out_E_accuracy: 0.0992 - ETA: 9s - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 -\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_2/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 529s 260ms/step - loss: 0.0187 - out_S_loss: 0.0098 - out_E_loss: 0.0089 - out_S_accuracy: 0.0922 - out_E_accuracy: 0.0992 - val_loss: 0.0185 - val_out_S_loss: 0.0097 - val_out_E_loss: 0.0088 - val_out_S_accuracy: 0.0974 - val_out_E_accuracy: 0.1060\n",
      "\n",
      "----------- Training model with head attached to layer number 1 -----------\n",
      "Epoch 1/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0221 - out_S_loss: 0.0113 - out_E_loss: 0.0107 - out_S_accuracy: 0.0509 - out_E_accuracy: 0.0516\n",
      "Epoch 00001: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0001.ckpt\n",
      "2034/2034 [==============================] - 293s 142ms/step - loss: 0.0221 - out_S_loss: 0.0113 - out_E_loss: 0.0107 - out_S_accuracy: 0.0509 - out_E_accuracy: 0.0516 - val_loss: 0.0200 - val_out_S_loss: 0.0104 - val_out_E_loss: 0.0097 - val_out_S_accuracy: 0.0737 - val_out_E_accuracy: 0.0771\n",
      "Epoch 2/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0200 - out_S_loss: 0.0104 - out_E_loss: 0.0097 - out_S_accuracy: 0.0721 - out_E_accuracy: 0.0738\n",
      "Epoch 00002: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0002.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0200 - out_S_loss: 0.0104 - out_E_loss: 0.0097 - out_S_accuracy: 0.0721 - out_E_accuracy: 0.0739 - val_loss: 0.0195 - val_out_S_loss: 0.0101 - val_out_E_loss: 0.0094 - val_out_S_accuracy: 0.0809 - val_out_E_accuracy: 0.0821\n",
      "Epoch 3/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0197 - out_S_loss: 0.0102 - out_E_loss: 0.0095 - out_S_accuracy: 0.0770 - out_E_accuracy: 0.0788\n",
      "Epoch 00003: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0003.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0197 - out_S_loss: 0.0102 - out_E_loss: 0.0095 - out_S_accuracy: 0.0770 - out_E_accuracy: 0.0788 - val_loss: 0.0194 - val_out_S_loss: 0.0100 - val_out_E_loss: 0.0093 - val_out_S_accuracy: 0.0842 - val_out_E_accuracy: 0.0845\n",
      "Epoch 4/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0196 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0792 - out_E_accuracy: 0.0799\n",
      "Epoch 00004: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0004.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0196 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0792 - out_E_accuracy: 0.0799 - val_loss: 0.0193 - val_out_S_loss: 0.0100 - val_out_E_loss: 0.0093 - val_out_S_accuracy: 0.0848 - val_out_E_accuracy: 0.0861\n",
      "Epoch 5/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0195 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0793 - out_E_accuracy: 0.0820\n",
      "Epoch 00005: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0005.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0195 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0793 - out_E_accuracy: 0.0820 - val_loss: 0.0192 - val_out_S_loss: 0.0100 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0859 - val_out_E_accuracy: 0.0876\n",
      "Epoch 6/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0195 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0802 - out_E_accuracy: 0.0819\n",
      "Epoch 00006: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0006.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0195 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0802 - out_E_accuracy: 0.0819 - val_loss: 0.0192 - val_out_S_loss: 0.0100 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0862 - val_out_E_accuracy: 0.0885\n",
      "Epoch 7/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0801 - out_E_accuracy: 0.0828\n",
      "Epoch 00007: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0007.ckpt\n",
      "2034/2034 [==============================] - 287s 141ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0801 - out_E_accuracy: 0.0827 - val_loss: 0.0192 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0876 - val_out_E_accuracy: 0.0879\n",
      "Epoch 8/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0805 - out_E_accuracy: 0.0824\n",
      "Epoch 00008: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0008.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0094 - out_S_accuracy: 0.0805 - out_E_accuracy: 0.0824 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0876 - val_out_E_accuracy: 0.0880\n",
      "Epoch 9/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0809 - out_E_accuracy: 0.0833\n",
      "Epoch 00009: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0009.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0808 - out_E_accuracy: 0.0833 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0876 - val_out_E_accuracy: 0.0883\n",
      "Epoch 10/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0804 - out_E_accuracy: 0.0833\n",
      "Epoch 00010: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0010.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0804 - out_E_accuracy: 0.0833 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0878 - val_out_E_accuracy: 0.0891\n",
      "Epoch 11/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0797 - out_E_accuracy: 0.0837\n",
      "Epoch 00011: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0011.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0797 - out_E_accuracy: 0.0837 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0875 - val_out_E_accuracy: 0.0882\n",
      "Epoch 12/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0825 - out_E_accuracy: 0.0834\n",
      "Epoch 00012: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0012.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0824 - out_E_accuracy: 0.0834 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0879 - val_out_E_accuracy: 0.0894\n",
      "Epoch 13/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0805 - out_E_accuracy: 0.0839\n",
      "Epoch 00013: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0013.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0805 - out_E_accuracy: 0.0840 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0877 - val_out_E_accuracy: 0.0898\n",
      "Epoch 14/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0813 - out_E_accuracy: 0.0834\n",
      "Epoch 00014: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0014.ckpt\n",
      "2034/2034 [==============================] - 288s 142ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0813 - out_E_accuracy: 0.0835 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0882 - val_out_E_accuracy: 0.0900\n",
      "Epoch 15/15\n",
      "2033/2034 [============================>.] - ETA: 0s - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0815 - out_E_accuracy: 0.0833\n",
      "Epoch 00015: saving model to /home/marcello/github/NLP/Assignements/QuestionAnswering/data/training/training_separate/layer_1/cp-0015.ckpt\n",
      "2034/2034 [==============================] - 288s 141ms/step - loss: 0.0194 - out_S_loss: 0.0101 - out_E_loss: 0.0093 - out_S_accuracy: 0.0815 - out_E_accuracy: 0.0834 - val_loss: 0.0191 - val_out_S_loss: 0.0099 - val_out_E_loss: 0.0092 - val_out_S_accuracy: 0.0879 - val_out_E_accuracy: 0.0903\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (14,) and (15,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13480/2768976600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/NLP/Assignements/QuestionAnswering/env/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/github/NLP/Assignements/QuestionAnswering/env/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/NLP/Assignements/QuestionAnswering/env/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/NLP/Assignements/QuestionAnswering/env/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (14,) and (15,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if train_separate_layers:\n",
    "    ### FREEZE #### the layers to only train the head if needed\n",
    "    for layer in config.transformer_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # training cell example: train layers separately\n",
    "    histories = []\n",
    "    EPOCHS = 15\n",
    "    PATIENCE = 3\n",
    "    for hidden_state in range(6, 0, -1): # starting from heavier one, it takes longer to train and also uses more GPU memory, safer to start with the harder one\n",
    "        checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_separate\", \"layer_\" + str(hidden_state), \"cp-{epoch:04d}.ckpt\")\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "        log_dir = os.path.join(config.ROOT_PATH, \"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        model = config.create_standard_model(hidden_state)\n",
    "\n",
    "        model.compile(tf.keras.optimizers.Adam(1e-4), \n",
    "                    loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                    metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_weights_only = True,\n",
    "            save_best_only = False\n",
    "        )\n",
    "\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            patience = PATIENCE\n",
    "        )\n",
    "\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "\n",
    "        model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "        print(\"\\n----------- Training model with head attached to layer number \" + str(hidden_state)+ \" -----------\")\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds, \n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS, \n",
    "            callbacks=[\n",
    "                cp_callback,\n",
    "                es_callback,\n",
    "                tensorboard_callback\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "        history = history.history\n",
    "\n",
    "        with open(os.path.join(checkpoint_dir, \"history.json\"), 'w') as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "        histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final plot\n",
    "x = [i for i in range(1,EPOCHS+1)]\n",
    "for history in histories:\n",
    "    plt.plot(x, history['val_loss'])\n",
    "\n",
    "plt.xticks(x)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "plt.legend([\"layer_\" + str(i) for i in range(6, 0, -1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with NER attention enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    main_layer = config.transformer_model.layers[0]\n",
    "    transformer_layers = main_layer.transformer\n",
    "    first_transformer_block = transformer_layers.layer[0]\n",
    "    attention_layer = first_transformer_block.attention\n",
    "\n",
    "    print(attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
    "\n",
    "    class TFInjectMultiHeadSelfAttention(MHSA):\n",
    "\n",
    "        def load_NER_attention(self, NER_attention):\n",
    "            self.NER_attention = NER_attention\n",
    "\n",
    "        def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
    "            # key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
    "            key = key * tf.expand_dims(self.NER_attention, axis=-1)\n",
    "            return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    CHOSEN_ENHANCED_LAYER = 0\n",
    "    CHOSEN_OUTPUT_STATES_IDX = [3, 4, 5, 6]\n",
    "    from transformers import TFDistilBertModel\n",
    "\n",
    "    class QuestionAnsweringModel(keras.Model):\n",
    "\n",
    "        def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
    "            super(QuestionAnsweringModel, self).__init__()\n",
    "\n",
    "            self.transformer_model = transformer_model\n",
    "            # Apply layer change to first attention block\n",
    "            self.transformer_model.layers[0].transformer.layer[CHOSEN_ENHANCED_LAYER].attention = \\\n",
    "                TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
    "            \n",
    "            # Add all remaining layers\n",
    "            self.dense_S = layers.Dense(1)\n",
    "            self.dense_E = layers.Dense(1)\n",
    "            self.flatten = layers.Flatten()\n",
    "            self.softmax_S = layers.Softmax(name='out_S')\n",
    "            self.softmax_E = layers.Softmax(name='out_E')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            NER_attention = inputs[\"NER_attention\"]\n",
    "\n",
    "            # Load the NER tensor into the custom layer\n",
    "            self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
    "\n",
    "            out = self.transformer_model(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            hidden_states = out.hidden_states\n",
    "            chosen_states_idx = CHOSEN_OUTPUT_STATES_IDX\n",
    "\n",
    "            chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
    "\n",
    "            out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
    "            out_S = self.flatten(out_S)\n",
    "            out_S = self.softmax_S(out_S)\n",
    "\n",
    "            out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
    "            out_E = self.flatten(out_E)\n",
    "            out_E = self.softmax_E(out_E)\n",
    "\n",
    "            return {'out_S': out_S, 'out_E': out_E}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_NER\", \"cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    model = QuestionAnsweringModel(config.transformer_model)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(3e-6), \n",
    "                    loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                    metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only = False\n",
    "    )\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = 3\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        epochs=10, \n",
    "        callbacks=[\n",
    "            cp_callback,\n",
    "            es_callback\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    history = history.history\n",
    "\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
    "    plt.plot(losses)\n",
    "    plt.legend(losses.columns)\n",
    "\n",
    "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
    "    plt.plot(accs)\n",
    "    plt.legend(accs.columns)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_ds.take(1):\n",
    "#     random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
    "#     input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
    "#     # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
    "#     print(\"Random sample n\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
    "    \n",
    "#     print(\"Question + context: \")\n",
    "#     print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "#     real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
    "#     real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
    "#     real_limits = [real_start, real_end]\n",
    "\n",
    "#     # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
    "    \n",
    "#     print(\"Real limits: \", real_limits)\n",
    "#     print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
    "#     print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
    "    \n",
    "#     predicted_limits = utils.start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
    "#     print(\"Predicted_limits: \", predicted_limits)\n",
    "#     print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
    "#     print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
