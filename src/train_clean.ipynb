{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
    "# COLAB ONLY CELLS\n",
    "#try:\n",
    "#    import google.colab\n",
    "#    IN_COLAB = True\n",
    "#    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
    "#    !nvidia-smi                 # Check which GPU has been chosen for us\n",
    "#    !rm -rf logs\n",
    "#    # Download the dataset from personal drive\n",
    "#    !mkdir data\n",
    "#    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "#except:\n",
    "#    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGC_45Iab0xm"
   },
   "outputs": [],
   "source": [
    "# PRIVATE CELL\n",
    "git_token = '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'\n",
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfNL_Qz3L2Iv",
    "outputId": "585e4ee5-55f3-43f3-aa48-f5c8924241c0"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !nvidia-smi             # Check which GPU has been chosen for us\n",
    "    !rm -rf logs\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/GitHub/\n",
    "    !git clone https://{git_token}@github.com/{username}/{repository}\n",
    "    %cd {repository}\n",
    "    %ls\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX8o7g0cL2Iz"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from config import Config\n",
    "config = Config()\n",
    "import utils\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = os.path.join(config.ROOT_PATH, 'data', 'training_set.json') # comment this if directory works differently\n",
    "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
    "questions = utils.read_question_set(TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
    "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
    "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
    "\n",
    "# we also create a small training set to test the model while building it, just to speed up\n",
    "\n",
    "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
    "small_train_dataset = {'data': small_data}\n",
    "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
    "small_val_dataset = {'data': small_val_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset=True # choose between full and small dataset\n",
    "if full_dataset:\n",
    "    TRAIN_DATASET = train_dataset\n",
    "    VAL_DATASET = val_dataset\n",
    "else:\n",
    "    TRAIN_DATASET = small_train_dataset\n",
    "    VAL_DATASET = small_val_dataset\n",
    "\n",
    "create_and_save = False     # fully create the dataset in RAM, and then save it on disk\n",
    "load = True                 # load a previously created dataset from disk\n",
    "generator = False           # if not enough RAM, create a dataset through a generator\n",
    "\n",
    "for_training = True         # returns a (feature, labels) dataset used in the fit method of the model\n",
    "NER_attention = False       # returns a (feature, id) dataset used during inference\n",
    "\n",
    "# if you need to save or load a model, choose the right path according to the previous 2 flags\n",
    "if create_and_save or load:\n",
    "    if for_training:\n",
    "        if NER_attention:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING_NER\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING_NER\n",
    "        else:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING\n",
    "    else:\n",
    "        if NER_attention:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE_NER\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE_NER\n",
    "        else:\n",
    "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE\n",
    "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE\n",
    "\n",
    "# dataset creation\n",
    "# for small dataset, just create it and store it in RAM, it's fast\n",
    "if create_and_save or not full_dataset: # for full dataset, you can either create it and save it on disk\n",
    "    train_ds = utils.create_full_dataset(TRAIN_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
    "    val_ds = utils.create_full_dataset(VAL_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
    "    if for_training and full_dataset: # only for full datasets, save them on disk\n",
    "        tf.data.experimental.save(train_ds, PATH_TRAIN)\n",
    "        tf.data.experimental.save(val_ds, PATH_VAL)\n",
    "elif load and full_dataset: # only for full datasets, you can load the previously created dataset from disk\n",
    "    train_ds = tf.data.experimental.load(PATH_TRAIN)\n",
    "    val_ds = tf.data.experimental.load(PATH_VAL)\n",
    "elif generator and full_dataset: # only for full datasets, if there is not enough RAM, you can create a dataset from a generator\n",
    "    train_ds = utils.create_dataset_and_ids(TRAIN_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
    "    val_ds = utils.create_dataset_and_ids(VAL_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
    "else: # if you don't enter in any of the above, something is wrong\n",
    "    raise Exception(\"Something wrong with dataset creation\")\n",
    "\n",
    "# batch the dataset and prefetch to increase speed\n",
    "train_ds = train_ds.batch(config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(config.VAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset has the wanted data inside\n",
    "for batch in train_ds.take(1):\n",
    "    print(batch[0].keys())\n",
    "    print(batch[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Chioce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_training = True\n",
    "train_separate_layers = False\n",
    "NER_training = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normal_training:\n",
    "    \n",
    "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_normal\",  \"cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    model = config.create_standard_model([3, 4, 5, 6])\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(3e-6), \n",
    "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only = False\n",
    "    )\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = 3\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        epochs=10, \n",
    "        callbacks=[\n",
    "            cp_callback,\n",
    "            es_callback\n",
    "        ],\n",
    "        use_multiprocessing = True,\n",
    "        initial_epoch=0\n",
    "        )\n",
    "\n",
    "\n",
    "    history = history.history\n",
    "\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
    "    plt.plot(losses)\n",
    "    plt.legend(losses.columns)\n",
    "\n",
    "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
    "    plt.plot(accs)\n",
    "    plt.legend(accs.columns)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_separate_layers:\n",
    "    ### FREEZE #### the layers to only train the head if needed\n",
    "    for layer in config.transformer_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # training cell example: train layers separately\n",
    "    histories = []\n",
    "    for hidden_state in range(1, 7):\n",
    "        checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_separate\", \"layer_\" + str(hidden_state), \"cp-{epoch:04d}.ckpt\")\n",
    "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "        \n",
    "        model = config.create_model(hidden_state)\n",
    "\n",
    "        model.compile(tf.keras.optimizers.Adam(3e-6), \n",
    "                    loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                    metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_weights_only = True,\n",
    "            save_best_only = False\n",
    "        )\n",
    "\n",
    "        model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "        print(\"----------- Training model with head attached to layer number \" + str(hidden_state)+ \" -----------\\n\")\n",
    "\n",
    "        history = model.fit(\n",
    "            train_ds, \n",
    "            validation_data=val_ds,\n",
    "            epochs=6, \n",
    "            callbacks=[cp_callback]\n",
    "            )\n",
    "        \n",
    "        history = history.history\n",
    "\n",
    "        with open(os.path.join(checkpoint_dir, \"history.json\"), 'w') as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "        histories.append(history)\n",
    "\n",
    "    # final plot\n",
    "    x = [i for i in range(1, 6)]\n",
    "    for history in histories:\n",
    "        plt.plot(x, history['val_loss'])\n",
    "\n",
    "    plt.xticks([i for i in range(1,6)])\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"val_loss\")\n",
    "    plt.legend([str(i) for i in range(1,7)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with NER attention enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    main_layer = config.transformer_model.layers[0]\n",
    "    transformer_layers = main_layer.transformer\n",
    "    first_transformer_block = transformer_layers.layer[0]\n",
    "    attention_layer = first_transformer_block.attention\n",
    "\n",
    "    print(attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
    "\n",
    "    class TFInjectMultiHeadSelfAttention(MHSA):\n",
    "\n",
    "        def load_NER_attention(self, NER_attention):\n",
    "            self.NER_attention = NER_attention\n",
    "\n",
    "        def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
    "            # key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
    "            key = key * tf.expand_dims(self.NER_attention, axis=-1)\n",
    "            return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    CHOSEN_ENHANCED_LAYER = 0\n",
    "    CHOSEN_OUTPUT_STATES_IDX = [3, 4, 5, 6]\n",
    "    from transformers import TFDistilBertModel\n",
    "\n",
    "    class QuestionAnsweringModel(keras.Model):\n",
    "\n",
    "        def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
    "            super(QuestionAnsweringModel, self).__init__()\n",
    "\n",
    "            self.transformer_model = transformer_model\n",
    "            # Apply layer change to first attention block\n",
    "            self.transformer_model.layers[0].transformer.layer[CHOSEN_ENHANCED_LAYER].attention = \\\n",
    "                TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
    "            \n",
    "            # Add all remaining layers\n",
    "            self.dense_S = layers.Dense(1)\n",
    "            self.dense_E = layers.Dense(1)\n",
    "            self.flatten = layers.Flatten()\n",
    "            self.softmax_S = layers.Softmax(name='out_S')\n",
    "            self.softmax_E = layers.Softmax(name='out_E')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_ids = inputs[\"input_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            NER_attention = inputs[\"NER_attention\"]\n",
    "\n",
    "            # Load the NER tensor into the custom layer\n",
    "            self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
    "\n",
    "            out = self.transformer_model(\n",
    "                {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            hidden_states = out.hidden_states\n",
    "            chosen_states_idx = CHOSEN_OUTPUT_STATES_IDX\n",
    "\n",
    "            chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
    "\n",
    "            out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
    "            out_S = self.flatten(out_S)\n",
    "            out_S = self.softmax_S(out_S)\n",
    "\n",
    "            out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
    "            out_E = self.flatten(out_E)\n",
    "            out_E = self.softmax_E(out_E)\n",
    "\n",
    "            return {'out_S': out_S, 'out_E': out_E}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NER_training:\n",
    "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_NER\", \"cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    model = QuestionAnsweringModel(config.transformer_model)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(3e-6), \n",
    "                    loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                    metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only = False\n",
    "    )\n",
    "\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = 3\n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        epochs=10, \n",
    "        callbacks=[\n",
    "            cp_callback,\n",
    "            es_callback\n",
    "        ]\n",
    "        )\n",
    "\n",
    "    history = history.history\n",
    "\n",
    "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
    "    plt.plot(losses)\n",
    "    plt.legend(losses.columns)\n",
    "\n",
    "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
    "    plt.plot(accs)\n",
    "    plt.legend(accs.columns)\n",
    "\n",
    "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
    "        json.dump(history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_ds.take(1):\n",
    "#     random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
    "#     input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
    "#     # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
    "#     print(\"Random sample n°\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
    "    \n",
    "#     print(\"Question + context: \")\n",
    "#     print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "#     real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
    "#     real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
    "#     real_limits = [real_start, real_end]\n",
    "\n",
    "#     # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
    "    \n",
    "#     print(\"Real limits: \", real_limits)\n",
    "#     print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
    "#     print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
    "    \n",
    "#     predicted_limits = utils.start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
    "#     print(\"Predicted_limits: \", predicted_limits)\n",
    "#     print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
    "#     print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
