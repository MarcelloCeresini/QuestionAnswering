{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
    "# COLAB ONLY CELLS\n",
    "#try:\n",
    "#    import google.colab\n",
    "#    IN_COLAB = True\n",
    "#    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
    "#    !nvidia-smi                 # Check which GPU has been chosen for us\n",
    "#    !rm -rf logs\n",
    "#    # Download the dataset from personal drive\n",
    "#    !mkdir data\n",
    "#    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "#except:\n",
    "#    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGC_45Iab0xm"
   },
   "outputs": [],
   "source": [
    "# PRIVATE CELL\n",
    "git_token = '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'\n",
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfNL_Qz3L2Iv",
    "outputId": "585e4ee5-55f3-43f3-aa48-f5c8924241c0"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !nvidia-smi             # Check which GPU has been chosen for us\n",
    "    !rm -rf logs\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/GitHub/\n",
    "    !git clone https://{git_token}@github.com/{username}/{repository}\n",
    "    %cd {repository}\n",
    "    %ls\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX8o7g0cL2Iz"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from config import Config\n",
    "config = Config()\n",
    "import utils\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json') # comment this if directory works differently\n",
    "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
    "questions = utils.read_question_set(TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
    "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
    "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
    "\n",
    "# we also create a small training set to test the model while building it, just to speed up\n",
    "\n",
    "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
    "small_train_dataset = {'data': small_data}\n",
    "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
    "small_val_dataset = {'data': small_val_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = config.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA4fRVeTR_Xg",
    "outputId": "00b0e2a9-7766-4d34-85c1-eb4ea7168918"
   },
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "#####################################################################\n",
    " ######## TODO: CHANGE LINE BELOW WITH \"train_dataset\" #############\n",
    "#####################################################################\n",
    "train_ds = utils.create_dataset_and_ids(small_train_dataset, config, for_training=True)\n",
    "val_ds = utils.create_dataset_and_ids(small_train_dataset, config, for_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_ds, range(len(train_ds)):\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(config.BATCH_SIZE)\n",
    "val_ds = val_ds.batch(config.VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    print(batch[0].keys())\n",
    "    print(batch[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = config.transformer_model\n",
    "# FREEZE the layers to only train the head if needed\n",
    "# for layer in transformer_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = config.create_standard_model([3, 4, 5, 6])\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "            loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "            metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "checkpoint_path = \"../data/training_normal\" + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only = True,\n",
    "    save_best_only = False # only in this case, \n",
    ")\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=5, \n",
    "    callbacks=[cp_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "print(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training cell example: train layers separately\n",
    "# histories = []\n",
    "# for hidden_state in range(1, 7):\n",
    "\n",
    "#     model = config.create_model(hidden_state)\n",
    "\n",
    "#     model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "#                 loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "#                 metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "#     checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "#     checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "    \n",
    "#     cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath = checkpoint_path,\n",
    "#         verbose=1,\n",
    "#         save_weights_only = True,\n",
    "#         save_best_only = False # only in this case, \n",
    "#     )\n",
    "\n",
    "#     model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "#     history = model.fit(\n",
    "#         train_ds, \n",
    "#         validation_data=val_ds,\n",
    "#         epochs=5, \n",
    "#         callbacks=[cp_callback]\n",
    "#         )\n",
    "    \n",
    "#     histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot\n",
    "# x = [i for i in range(1, 6)]\n",
    "# for history in histories:\n",
    "#     plt.plot(x, history.history['val_loss'])\n",
    "\n",
    "# plt.xticks([i for i in range(1,6)])\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.ylabel(\"val_loss\")\n",
    "# plt.legend([str(i) for i in range(1,7)])\n",
    "# plt.show()\n",
    "\n",
    "# with open(\"training_histories.json\", 'w') as f:\n",
    "#     json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with NER attention enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_layer = transformer_model.layers[0]\n",
    "# transformer_layers = main_layer.transformer\n",
    "# first_transformer_block = transformer_layers.layer[0]\n",
    "# attention_layer = first_transformer_block.attention\n",
    "\n",
    "# print(attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
    "\n",
    "class TFInjectMultiHeadSelfAttention(MHSA):\n",
    "\n",
    "    def load_NER_attention(self, NER_attention):\n",
    "        self.NER_attention = NER_attention\n",
    "\n",
    "    def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
    "        key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
    "        return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class QuestionAnsweringModel(keras.Model):\n",
    "\n",
    "    def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "\n",
    "        self.transformer_model = transformer_model\n",
    "        # Apply layer change to first attention block\n",
    "        transformer_model.layers[0].transformer.layer[0].attention = \\\n",
    "            TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
    "        \n",
    "        # Add all remaining layers\n",
    "        self.dense_S = layers.Dense(1)\n",
    "        self.dense_E = layers.Dense(1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.softmax_S = layers.Softmax(name='out_S')\n",
    "        self.softmax_E = layers.Softmax(name='out_E')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        NER_attention = inputs[\"NER_attention\"]\n",
    "        # token_type_ids = inputs[\"token_type_ids\"] # uncomment if using BERT\n",
    "\n",
    "        # Load the NER tensor into the custom layer\n",
    "        self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
    "\n",
    "        out = self.transformer_model(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                # \"token_type_ids\": token_type_ids # uncomment if using BERT\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # TODO: chose which layers\n",
    "        hidden_states = out.hidden_states\n",
    "        chosen_states_idx = [3, 4, 5, 6]\n",
    "\n",
    "        # TODO: chose merging method\n",
    "        chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
    "\n",
    "        # output = layers.Bidirectional(layers.LSTM(64, return_sequences = True, activation = \"relu\"))(chosen_hidden_states)\n",
    "        # output = layers.Dense(2, activation = \"softmax\")(output) # 2 because we need both \n",
    "\n",
    "        out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
    "        out_S = self.flatten(out_S)\n",
    "        out_S = self.softmax_S(out_S)\n",
    "\n",
    "        out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
    "        out_E = self.flatten(out_E)\n",
    "        out_E = self.softmax_E(out_E)\n",
    "\n",
    "        return {'out_S': out_S, 'out_E': out_E}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = QuestionAnsweringModel(transformer_model)\n",
    "\n",
    "# model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "#                 loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "#                 metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "# checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath = checkpoint_path,\n",
    "#     verbose=1,\n",
    "#     save_weights_only = True,\n",
    "#     save_best_only = False # only in this case, \n",
    "# )\n",
    "\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds, \n",
    "#     validation_data=val_ds,\n",
    "#     epochs=5, \n",
    "#     callbacks=[cp_callback]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_ds.take(1):\n",
    "#     random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
    "#     input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
    "#     # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
    "#     print(\"Random sample n°\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
    "    \n",
    "#     print(\"Question + context: \")\n",
    "#     print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "#     real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
    "#     real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
    "#     real_limits = [real_start, real_end]\n",
    "\n",
    "#     # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
    "    \n",
    "#     print(\"Real limits: \", real_limits)\n",
    "#     print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
    "#     print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
    "    \n",
    "#     predicted_limits = utils.start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
    "#     print(\"Predicted_limits: \", predicted_limits)\n",
    "#     print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
    "#     print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
