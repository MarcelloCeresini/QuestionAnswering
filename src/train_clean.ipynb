{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
    "# COLAB ONLY CELLS\n",
    "#try:\n",
    "#    import google.colab\n",
    "#    IN_COLAB = True\n",
    "#    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
    "#    !nvidia-smi                 # Check which GPU has been chosen for us\n",
    "#    !rm -rf logs\n",
    "#    # Download the dataset from personal drive\n",
    "#    !mkdir data\n",
    "#    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "#except:\n",
    "#    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGC_45Iab0xm"
   },
   "outputs": [],
   "source": [
    "# PRIVATE CELL\n",
    "git_token = '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'\n",
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfNL_Qz3L2Iv",
    "outputId": "585e4ee5-55f3-43f3-aa48-f5c8924241c0"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !nvidia-smi             # Check which GPU has been chosen for us\n",
    "    !rm -rf logs\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/GitHub/\n",
    "    !git clone https://{git_token}@github.com/{username}/{repository}\n",
    "    %cd {repository}\n",
    "    %ls\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX8o7g0cL2Iz"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "\n",
    "from typing import List, Dict, Callable, Sequence, Tuple\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from config import ConfigFile\n",
    "config = ConfigFile()\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json') # comment this if directory works differently\n",
    "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
    "with open(TRAINING_FILE, 'r') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
    "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
    "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
    "\n",
    "# we also create a small training set to test the model while building it, just to speed up\n",
    "\n",
    "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
    "small_train_dataset = {'data': small_data}\n",
    "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
    "small_val_dataset = {'data': small_val_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = config.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCEuqEMIR_Xf"
   },
   "outputs": [],
   "source": [
    "def find_start_end_token_one_hot_encoded(answers: Dict, offsets: List[Tuple[int]]) -> int:\n",
    "    '''\n",
    "    This function returns the starting and ending token of the answer, already one hot encoded and ready for binary crossentropy\n",
    "    Inputs:\n",
    "        answers: List[Dict] --> for each question, a list of answers. Each answer contains:\n",
    "            - answer_start: the index of the starting character\n",
    "            - text: the text of the answer, that we exploit through the number of chars that it containts\n",
    "        offsets: List[Tuple[int]] --> the tokenizer from HuggingFace transforms the sentence (question+context)\n",
    "            into a sequence of tokens. Offsets keeps track of the character start and end indexes for each token\n",
    "    Output:\n",
    "        result: Dict --> each key contains only one array, the one-hot encoded version of, respectively, the start\n",
    "            and end token of the answer in the sentence (question+context)\n",
    "    '''\n",
    "    result = {\n",
    "        \"out_S\": np.zeros(len(offsets)),\n",
    "        \"out_E\": np.zeros(len(offsets))\n",
    "    }   \n",
    "\n",
    "    for answer in answers:\n",
    "        starting_char = answer['answer_start']\n",
    "        answer_len = len(answer['text'])\n",
    "\n",
    "        for i in range(1, len(offsets)): # we skip the first token, [CLS], that has (0,0) as a tuple\n",
    "            # We cycle through all the tokens of the question, until we find (0,0), which determines the separator\n",
    "            if offsets[i] == (0,0): # The [SEP] special char --> this indicates the beginning of the context\n",
    "                for j in range(1, len(offsets)-i-1): # We skip the first and the last tokens, both special tokens\n",
    "                    # If the starting char is in the interval, the index (j) of its position inside the context, \n",
    "                    # plus the length of the question (i) is the right index\n",
    "                    if (starting_char >= offsets[i+j][0]) and (starting_char <= offsets[i+j][1]):\n",
    "                        result[\"out_S\"][i+j] += 1\n",
    "                    # if the ending char (starting + length -1) is in the interval, same as above\n",
    "                    if (starting_char + answer_len - 1 >= offsets[i+j][0]) and (starting_char + answer_len - 1 < offsets[i+j][1]):\n",
    "                        result[\"out_E\"][i+j] += 1\n",
    "                        break\n",
    "                # After this cycle, we must check other answers\n",
    "                break\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_data_for_dataset(data):\n",
    "    '''\n",
    "    This function takes in input the whole data structure and iteratively composes question+context pairs, plus their label\n",
    "    Inputs:\n",
    "        data: Dict --> the data structure containing the data\n",
    "    Outputs:\n",
    "        tf.data.Dataset --> the data structure containing (features, labels) that will be fed to the model during fitting\n",
    "        more specifically:\n",
    "        features: Dict --> keys:\n",
    "            - input_ids: array of token ids\n",
    "            - attention_mask: array indicating if the corresponding token is padding or not\n",
    "        labels: Dict --> keys:\n",
    "            - gt_S: array representing the index of the initial token of the answer, one-hot encoded\n",
    "            - gt_E: array representing the index of the final token of the answer, one-hot encoded\n",
    "\n",
    "    This function, for each article in \"data\", extracts all paragraphs (and their text, the \"context\"), for each paragraph, all questions_and_answers\n",
    "    At this point, it tokenizes (question+context) while truncating and padding up to MAX_LEN_PAIRS\n",
    "    Moreover, it also returns the \"attention_mask\", an array that tells if the token is padding or normal, that will be used by the model\n",
    "\n",
    "    It also keeps track, through \"find_start_end_token_one_hot_encoded\", of the index of the initial and final token of the answer, the labels for the model\n",
    "\n",
    "    In the end, it returns a tf.data.Dataset with the structure (features, labels), to be injected directly in the fit method of the model\n",
    "    '''\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for question_and_answer in paragraph[\"qas\"]:\n",
    "                ### QUESTION AND CONTEXT TOKENIZATION ###\n",
    "                # For question answering with BERT we need to encode both \n",
    "                # question and context, and this is the way in which \n",
    "                # HuggingFace's BertTokenizer does it.\n",
    "                # The tokenizer returns a dictionary containing all the information we need\n",
    "                encoded_inputs = tokenizer(\n",
    "                    question_and_answer[\"question\"],    # First we pass the question\n",
    "                    paragraph[\"context\"],               # Then the context\n",
    "                    max_length = config.INPUT_LEN,         # We want to pad and truncate to this length\n",
    "                    truncation = True,\n",
    "                    padding = 'max_length',             # Pads all sequences to 512.\n",
    "                                                        # If \"True\" it would pad to the longest sentence in the batch \n",
    "                                                        # (in this case we only use 1 sentence, so no padding at all)\n",
    "                    # return_token_type_ids = True,     # IF USING BERT, DistilBert does not need it \n",
    "                    return_token_type_ids = False,      # Return if the token is from sentence 0 or sentence 1 \n",
    "                    return_attention_mask = True,       # Return if it's a pad token or not\n",
    "                    return_offsets_mapping = True       # Really important --> returns each token's first and last char position in the original sentence \n",
    "                )\n",
    "                \n",
    "                ### MAPPING OF THE START OF THE ANSWER BETWEEN CHARS AND TOKENS ###\n",
    "                # We want to pass from the starting position in chars to the starting position in tokens\n",
    "                label = find_start_end_token_one_hot_encoded(\n",
    "                    # We pass the list of answers (usually there is still one per question,\n",
    "                    #   but we mustn't assume anything)\n",
    "                    answers = question_and_answer[\"answers\"],\n",
    "                    # And also the inputs offset mapping just recieved from the tokenizer\n",
    "                    offsets = encoded_inputs[\"offset_mapping\"]\n",
    "                )\n",
    "                \n",
    "                encoded_inputs.pop(\"offset_mapping\", None) # Removes the offset mapping, not useful anymore \n",
    "                                                           # (\"None\" is used because otherwise KeyError could be raised if the key wasn't present)\n",
    "                \n",
    "                # TODO: Add NER attention vector\n",
    "                encoded_inputs['NER_attention'] = np.ones(512)\n",
    "                \n",
    "                features.append(encoded_inputs)\n",
    "                labels.append(label)\n",
    "\n",
    "                # DO NOT KNOW IF IT IS NEEDED\n",
    "                '''\n",
    "                ### ANSWER TOKENIZATION ###\n",
    "                # use the same tokenizer also to tokenize the answers\n",
    "                encoded_answer = tokenizer(\n",
    "                    question_and_answer[\"answers\"][0][\"text\"],  # here we only need to pass the answer\n",
    "                    max_length=MAX_LEN_ANSWERS,\n",
    "                    truncation = True,\n",
    "                    padding = 'max_length',\n",
    "                    add_special_tokens = False,                 # the answer will only be used for the loss, not as input to the model, it does not need special tokens [CLS] and [SEP]\n",
    "                    return_token_type_ids = False,              # only one sentence\n",
    "                    return_attention_mask = True)               # still interested in padding tokens\n",
    "                \n",
    "                # we add to the dictionary of the pair question-context the token ids of the answer and its mask\n",
    "                encoded_inputs[\"answer_ids\"] = encoded_answer[\"input_ids\"]\n",
    "                encoded_inputs[\"answer_mask\"] = encoded_answer[\"attention_mask\"]\n",
    "                '''\n",
    "\n",
    "    print(\"Creating dataset\")\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        pd.DataFrame.from_dict(features).to_dict(orient=\"list\"),  # dataframe for features \n",
    "        pd.DataFrame.from_dict(labels).to_dict(orient=\"list\")                                                    # dataframe for labels \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA4fRVeTR_Xg",
    "outputId": "00b0e2a9-7766-4d34-85c1-eb4ea7168918"
   },
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "#####################################################################\n",
    " ######## TODO: CHANGE LINE BELOW WITH \"train_dataset\" #############\n",
    "#####################################################################\n",
    "train_ds = create_data_for_dataset(small_train_dataset)\n",
    "val_ds = create_data_for_dataset(small_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(config.BATCH_SIZE)\n",
    "val_ds = val_ds.batch(config.VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = config.transformer_model\n",
    "# FREEZE the layers to only train the head if needed\n",
    "for layer in transformer_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training cell example: train layers separately\n",
    "histories = []\n",
    "for hidden_state in range(1, 7):\n",
    "\n",
    "    model = config.create_model(hidden_state)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "    checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only = True,\n",
    "        save_best_only = False # only in this case, \n",
    "    )\n",
    "\n",
    "    model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds,\n",
    "        epochs=5, \n",
    "        callbacks=[cp_callback]\n",
    "        )\n",
    "    \n",
    "    histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "x = [i for i in range(1, 6)]\n",
    "for history in histories:\n",
    "    plt.plot(x, history.history['val_loss'])\n",
    "\n",
    "plt.xticks([i for i in range(1,6)])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "plt.legend([str(i) for i in range(1,7)])\n",
    "plt.show()\n",
    "\n",
    "with open(\"training_histories.json\", 'w') as f:\n",
    "    json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with NER attention enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_layer = transformer_model.layers[0]\n",
    "transformer_layers = main_layer.transformer\n",
    "first_transformer_block = transformer_layers.layer[0]\n",
    "attention_layer = first_transformer_block.attention\n",
    "\n",
    "print(attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
    "\n",
    "class TFInjectMultiHeadSelfAttention(MHSA):\n",
    "\n",
    "    def load_NER_attention(self, NER_attention):\n",
    "        self.NER_attention = NER_attention\n",
    "\n",
    "    def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
    "        key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
    "        return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class QuestionAnsweringModel(keras.Model):\n",
    "\n",
    "    def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "\n",
    "        self.transformer_model = transformer_model\n",
    "        # Apply layer change to first attention block\n",
    "        transformer_model.layers[0].transformer.layer[0].attention = \\\n",
    "            TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
    "        \n",
    "        # Add all remaining layers\n",
    "        self.dense_S = layers.Dense(1)\n",
    "        self.dense_E = layers.Dense(1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.softmax_S = layers.Softmax(name='out_S')\n",
    "        self.softmax_E = layers.Softmax(name='out_E')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        NER_attention = inputs[\"NER_attention\"]\n",
    "        # token_type_ids = inputs[\"token_type_ids\"] # uncomment if using BERT\n",
    "\n",
    "        # Load the NER tensor into the custom layer\n",
    "        self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
    "\n",
    "        out = self.transformer_model(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                # \"token_type_ids\": token_type_ids # uncomment if using BERT\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # TODO: chose which layers\n",
    "        hidden_states = out.hidden_states\n",
    "        chosen_states_idx = [3, 4, 5, 6]\n",
    "\n",
    "        # TODO: chose merging method\n",
    "        chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
    "\n",
    "        # output = layers.Bidirectional(layers.LSTM(64, return_sequences = True, activation = \"relu\"))(chosen_hidden_states)\n",
    "        # output = layers.Dense(2, activation = \"softmax\")(output) # 2 because we need both \n",
    "\n",
    "        out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
    "        out_S = self.flatten(out_S)\n",
    "        out_S = self.softmax_S(out_S)\n",
    "\n",
    "        out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
    "        out_E = self.flatten(out_E)\n",
    "        out_E = self.softmax_E(out_E)\n",
    "\n",
    "        return {'out_S': out_S, 'out_E': out_E}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuestionAnsweringModel(transformer_model)\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only = True,\n",
    "    save_best_only = False # only in this case, \n",
    ")\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=5, \n",
    "    callbacks=[cp_callback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_token_from_probabilities(pstartv: np.array, \n",
    "                                       pendv: np.array, \n",
    "                                       dim:int=512) -> List[List[int]]:\n",
    "    '''\n",
    "    Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
    "    '''\n",
    "    idxs = []\n",
    "    for i in range(pstartv.shape[0]):\n",
    "        pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
    "        pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
    "        sums = pstart + pend\n",
    "        sums = np.triu(sums, k=1) # Zero out lower triangular matrix + diagonal\n",
    "        val = np.argmax(sums)\n",
    "        row = val // dim\n",
    "        col = val - dim*row\n",
    "        idxs.append([row,col])\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
    "    input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
    "    # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
    "    print(\"Random sample n°\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
    "    \n",
    "    print(\"Question + context: \")\n",
    "    print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "    real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
    "    real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
    "    real_limits = [real_start, real_end]\n",
    "\n",
    "    # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
    "    \n",
    "    print(\"Real limits: \", real_limits)\n",
    "    print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
    "    print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
    "    \n",
    "    predicted_limits = start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
    "    print(\"Predicted_limits: \", predicted_limits)\n",
    "    print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
    "    print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
