{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
    "# COLAB ONLY CELLS\n",
    "#try:\n",
    "#    import google.colab\n",
    "#    IN_COLAB = True\n",
    "#    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
    "#    !nvidia-smi                 # Check which GPU has been chosen for us\n",
    "#    !rm -rf logs\n",
    "#    # Download the dataset from personal drive\n",
    "#    !mkdir data\n",
    "#    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "#except:\n",
    "#    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QGC_45Iab0xm"
   },
   "outputs": [],
   "source": [
    "# PRIVATE CELL\n",
    "git_token = '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'\n",
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfNL_Qz3L2Iv",
    "outputId": "585e4ee5-55f3-43f3-aa48-f5c8924241c0"
   },
   "outputs": [],
   "source": [
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !nvidia-smi             # Check which GPU has been chosen for us\n",
    "    !rm -rf logs\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    %cd /content/drive/MyDrive/GitHub/\n",
    "    !git clone https://{git_token}@github.com/{username}/{repository}\n",
    "    %cd {repository}\n",
    "    %ls\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tX8o7g0cL2Iz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 13:28:48.300688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.346311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.347221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.350295: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-12 13:28:48.352872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.353925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.354875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.950123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.950671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.950955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-12 13:28:48.951313: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-01-12 13:28:48.951413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3473 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-01-12 13:28:49.238172: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-01-12 13:28:51.324231: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "\n",
    "from typing import List, Dict, Callable, Sequence, Tuple\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from config import ConfigFile\n",
    "config = ConfigFile()\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "random.seed(config.RANDOM_SEED)\n",
    "tf.random.set_seed(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json') # comment this if directory works differently\n",
    "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
    "with open(TRAINING_FILE, 'r') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
    "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
    "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
    "\n",
    "# we also create a small training set to test the model while building it, just to speed up\n",
    "\n",
    "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
    "small_train_dataset = {'data': small_data}\n",
    "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
    "small_val_dataset = {'data': small_val_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = config.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JCEuqEMIR_Xf"
   },
   "outputs": [],
   "source": [
    "def find_start_end_token_one_hot_encoded(answers: Dict, offsets: List[Tuple[int]]) -> int:\n",
    "    '''\n",
    "    This function returns the starting and ending token of the answer, already one hot encoded and ready for binary crossentropy\n",
    "    Inputs:\n",
    "        answers: List[Dict] --> for each question, a list of answers. Each answer contains:\n",
    "            - answer_start: the index of the starting character\n",
    "            - text: the text of the answer, that we exploit through the number of chars that it containts\n",
    "        offsets: List[Tuple[int]] --> the tokenizer from HuggingFace transforms the sentence (question+context)\n",
    "            into a sequence of tokens. Offsets keeps track of the character start and end indexes for each token\n",
    "    Output:\n",
    "        result: Dict --> each key contains only one array, the one-hot encoded version of, respectively, the start\n",
    "            and end token of the answer in the sentence (question+context)\n",
    "    '''\n",
    "    result = {\n",
    "        \"out_S\": np.zeros(len(offsets)),\n",
    "        \"out_E\": np.zeros(len(offsets))\n",
    "    }   \n",
    "\n",
    "    for answer in answers:\n",
    "        starting_char = answer['answer_start']\n",
    "        answer_len = len(answer['text'])\n",
    "\n",
    "        for i in range(1, len(offsets)): # we skip the first token, [CLS], that has (0,0) as a tuple\n",
    "            # We cycle through all the tokens of the question, until we find (0,0), which determines the separator\n",
    "            if offsets[i] == (0,0): # The [SEP] special char --> this indicates the beginning of the context\n",
    "                for j in range(1, len(offsets)-i-1): # We skip the first and the last tokens, both special tokens\n",
    "                    # If the starting char is in the interval, the index (j) of its position inside the context, \n",
    "                    # plus the length of the question (i) is the right index\n",
    "                    if (starting_char >= offsets[i+j][0]) and (starting_char <= offsets[i+j][1]):\n",
    "                        result[\"out_S\"][i+j] += 1\n",
    "                    # if the ending char (starting + length -1) is in the interval, same as above\n",
    "                    if (starting_char + answer_len - 1 >= offsets[i+j][0]) and (starting_char + answer_len - 1 < offsets[i+j][1]):\n",
    "                        result[\"out_E\"][i+j] += 1\n",
    "                        break\n",
    "                # After this cycle, we must check other answers\n",
    "                break\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_data_for_dataset(data, NER_bool = False):\n",
    "    '''\n",
    "    This function takes in input the whole data structure and iteratively composes question+context pairs, plus their label\n",
    "    Inputs:\n",
    "        data: Dict --> the data structure containing the data\n",
    "    Outputs:\n",
    "        tf.data.Dataset --> the data structure containing (features, labels) that will be fed to the model during fitting\n",
    "        more specifically:\n",
    "        features: Dict --> keys:\n",
    "            - input_ids: array of token ids\n",
    "            - attention_mask: array indicating if the corresponding token is padding or not\n",
    "        labels: Dict --> keys:\n",
    "            - gt_S: array representing the index of the initial token of the answer, one-hot encoded\n",
    "            - gt_E: array representing the index of the final token of the answer, one-hot encoded\n",
    "\n",
    "    This function, for each article in \"data\", extracts all paragraphs (and their text, the \"context\"), for each paragraph, all questions_and_answers\n",
    "    At this point, it tokenizes (question+context) while truncating and padding up to MAX_LEN_PAIRS\n",
    "    Moreover, it also returns the \"attention_mask\", an array that tells if the token is padding or normal, that will be used by the model\n",
    "\n",
    "    It also keeps track, through \"find_start_end_token_one_hot_encoded\", of the index of the initial and final token of the answer, the labels for the model\n",
    "\n",
    "    In the end, it returns a tf.data.Dataset with the structure (features, labels), to be injected directly in the fit method of the model\n",
    "    '''\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for article in tqdm(data[\"data\"]):\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for question_and_answer in paragraph[\"qas\"]:\n",
    "                ### QUESTION AND CONTEXT TOKENIZATION ###\n",
    "                # For question answering with BERT we need to encode both \n",
    "                # question and context, and this is the way in which \n",
    "                # HuggingFace's BertTokenizer does it.\n",
    "                # The tokenizer returns a dictionary containing all the information we need\n",
    "                encoded_inputs = tokenizer(\n",
    "                    question_and_answer[\"question\"],    # First we pass the question\n",
    "                    paragraph[\"context\"],               # Then the context\n",
    "                    max_length = config.INPUT_LEN,         # We want to pad and truncate to this length\n",
    "                    truncation = True,\n",
    "                    padding = 'max_length',             # Pads all sequences to 512.\n",
    "                                                        # If \"True\" it would pad to the longest sentence in the batch \n",
    "                                                        # (in this case we only use 1 sentence, so no padding at all)\n",
    "                    # return_token_type_ids = True,     # IF USING BERT, DistilBert does not need it \n",
    "                    return_token_type_ids = False,      # Return if the token is from sentence 0 or sentence 1 \n",
    "                    return_attention_mask = True,       # Return if it's a pad token or not\n",
    "                    return_offsets_mapping = True       # Really important --> returns each token's first and last char position in the original sentence \n",
    "                )\n",
    "                \n",
    "                ### MAPPING OF THE START OF THE ANSWER BETWEEN CHARS AND TOKENS ###\n",
    "                # We want to pass from the starting position in chars to the starting position in tokens\n",
    "                label = find_start_end_token_one_hot_encoded(\n",
    "                    # We pass the list of answers (usually there is still one per question,\n",
    "                    #   but we mustn't assume anything)\n",
    "                    answers = question_and_answer[\"answers\"],\n",
    "                    # And also the inputs offset mapping just recieved from the tokenizer\n",
    "                    offsets = encoded_inputs[\"offset_mapping\"]\n",
    "                )\n",
    "                \n",
    "                encoded_inputs.pop(\"offset_mapping\", None) # Removes the offset mapping, not useful anymore \n",
    "                                                           # (\"None\" is used because otherwise KeyError could be raised if the key wasn't present)\n",
    "                \n",
    "                # TODO: Add NER attention vector\n",
    "                if NER_bool:\n",
    "                    encoded_inputs['NER_attention'] = np.ones(512)\n",
    "                \n",
    "                features.append(encoded_inputs)\n",
    "                labels.append(label)\n",
    "\n",
    "                # DO NOT KNOW IF IT IS NEEDED\n",
    "                '''\n",
    "                ### ANSWER TOKENIZATION ###\n",
    "                # use the same tokenizer also to tokenize the answers\n",
    "                encoded_answer = tokenizer(\n",
    "                    question_and_answer[\"answers\"][0][\"text\"],  # here we only need to pass the answer\n",
    "                    max_length=MAX_LEN_ANSWERS,\n",
    "                    truncation = True,\n",
    "                    padding = 'max_length',\n",
    "                    add_special_tokens = False,                 # the answer will only be used for the loss, not as input to the model, it does not need special tokens [CLS] and [SEP]\n",
    "                    return_token_type_ids = False,              # only one sentence\n",
    "                    return_attention_mask = True)               # still interested in padding tokens\n",
    "                \n",
    "                # we add to the dictionary of the pair question-context the token ids of the answer and its mask\n",
    "                encoded_inputs[\"answer_ids\"] = encoded_answer[\"input_ids\"]\n",
    "                encoded_inputs[\"answer_mask\"] = encoded_answer[\"attention_mask\"]\n",
    "                '''\n",
    "\n",
    "    print(\"Creating dataset\")\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        pd.DataFrame.from_dict(features).to_dict(orient=\"list\"),  # dataframe for features \n",
    "        pd.DataFrame.from_dict(labels).to_dict(orient=\"list\")                                                    # dataframe for labels \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA4fRVeTR_Xg",
    "outputId": "00b0e2a9-7766-4d34-85c1-eb4ea7168918"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 13:28:59.611782: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16707584 exceeds 10% of free system memory.\n",
      "2022-01-12 13:28:59.731315: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16707584 exceeds 10% of free system memory.\n",
      "2022-01-12 13:28:59.826170: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16707584 exceeds 10% of free system memory.\n",
      "2022-01-12 13:28:59.835230: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16707584 exceeds 10% of free system memory.\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset\n"
     ]
    }
   ],
   "source": [
    "# create the datasets\n",
    "#####################################################################\n",
    " ######## TODO: CHANGE LINE BELOW WITH \"train_dataset\" #############\n",
    "#####################################################################\n",
    "train_ds = create_data_for_dataset(small_train_dataset, NER_bool=False) # True if needed for NER augmentation\n",
    "val_ds = create_data_for_dataset(small_val_dataset, NER_bool=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(config.BATCH_SIZE)\n",
    "val_ds = val_ds.batch(config.VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = config.transformer_model\n",
    "# FREEZE the layers to only train the head if needed\n",
    "for layer in transformer_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0280 - out_S_loss: 0.0146 - out_E_loss: 0.0134 - out_S_accuracy: 0.0071 - out_E_accuracy: 0.0091\n",
      "Epoch 00001: saving model to training_normal/cp-0001.ckpt\n",
      "64/64 [==============================] - 104s 1s/step - loss: 0.0280 - out_S_loss: 0.0146 - out_E_loss: 0.0134 - out_S_accuracy: 0.0071 - out_E_accuracy: 0.0091 - val_loss: 0.0267 - val_out_S_loss: 0.0138 - val_out_E_loss: 0.0129 - val_out_S_accuracy: 0.0165 - val_out_E_accuracy: 0.0224\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0262 - out_S_loss: 0.0136 - out_E_loss: 0.0126 - out_S_accuracy: 0.0250 - out_E_accuracy: 0.0177\n",
      "Epoch 00002: saving model to training_normal/cp-0002.ckpt\n",
      "64/64 [==============================] - 98s 2s/step - loss: 0.0262 - out_S_loss: 0.0136 - out_E_loss: 0.0126 - out_S_accuracy: 0.0250 - out_E_accuracy: 0.0177 - val_loss: 0.0252 - val_out_S_loss: 0.0130 - val_out_E_loss: 0.0122 - val_out_S_accuracy: 0.0341 - val_out_E_accuracy: 0.0318\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0248 - out_S_loss: 0.0128 - out_E_loss: 0.0120 - out_S_accuracy: 0.0363 - out_E_accuracy: 0.0282\n",
      "Epoch 00003: saving model to training_normal/cp-0003.ckpt\n",
      "64/64 [==============================] - 99s 2s/step - loss: 0.0248 - out_S_loss: 0.0128 - out_E_loss: 0.0120 - out_S_accuracy: 0.0363 - out_E_accuracy: 0.0282 - val_loss: 0.0242 - val_out_S_loss: 0.0125 - val_out_E_loss: 0.0117 - val_out_S_accuracy: 0.0435 - val_out_E_accuracy: 0.0400\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0238 - out_S_loss: 0.0123 - out_E_loss: 0.0115 - out_S_accuracy: 0.0392 - out_E_accuracy: 0.0402\n",
      "Epoch 00004: saving model to training_normal/cp-0004.ckpt\n",
      "64/64 [==============================] - 96s 1s/step - loss: 0.0238 - out_S_loss: 0.0123 - out_E_loss: 0.0115 - out_S_accuracy: 0.0392 - out_E_accuracy: 0.0402 - val_loss: 0.0234 - val_out_S_loss: 0.0121 - val_out_E_loss: 0.0113 - val_out_S_accuracy: 0.0447 - val_out_E_accuracy: 0.0482\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0230 - out_S_loss: 0.0119 - out_E_loss: 0.0111 - out_S_accuracy: 0.0458 - out_E_accuracy: 0.0427\n",
      "Epoch 00005: saving model to training_normal/cp-0005.ckpt\n",
      "64/64 [==============================] - 97s 2s/step - loss: 0.0230 - out_S_loss: 0.0119 - out_E_loss: 0.0111 - out_S_accuracy: 0.0458 - out_E_accuracy: 0.0427 - val_loss: 0.0229 - val_out_S_loss: 0.0118 - val_out_E_loss: 0.0110 - val_out_S_accuracy: 0.0482 - val_out_E_accuracy: 0.0506\n"
     ]
    }
   ],
   "source": [
    "model = config.create_standard_model([3, 4, 5, 6])\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "            loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "            metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "checkpoint_path = \"../data/training_normal\" + \"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only = True,\n",
    "    save_best_only = False # only in this case, \n",
    ")\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds,\n",
    "    epochs=5, \n",
    "    callbacks=[cp_callback]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_normal/cp-0005.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "print(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training separate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training cell example: train layers separately\n",
    "# histories = []\n",
    "# for hidden_state in range(1, 7):\n",
    "\n",
    "#     model = config.create_standard_model(hidden_state)\n",
    "\n",
    "#     model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "#                 loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "#                 metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "#     checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "#     checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "    \n",
    "#     cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath = checkpoint_path,\n",
    "#         verbose=1,\n",
    "#         save_weights_only = True,\n",
    "#         save_best_only = False # only in this case, \n",
    "#     )\n",
    "\n",
    "#     model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "#     history = model.fit(\n",
    "#         train_ds, \n",
    "#         validation_data=val_ds,\n",
    "#         epochs=5, \n",
    "#         callbacks=[cp_callback]\n",
    "#         )\n",
    "    \n",
    "#     histories.append(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot\n",
    "# x = [i for i in range(1, 6)]\n",
    "# for history in histories:\n",
    "#     plt.plot(x, history.history['val_loss'])\n",
    "\n",
    "# plt.xticks([i for i in range(1,6)])\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.ylabel(\"val_loss\")\n",
    "# plt.legend([str(i) for i in range(1,7)])\n",
    "# plt.show()\n",
    "\n",
    "# with open(\"./data/results/training_histories.json\", 'w') as f:\n",
    "#     json.dump(histories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with NER attention enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_layer = transformer_model.layers[0]\n",
    "# transformer_layers = main_layer.transformer\n",
    "# first_transformer_block = transformer_layers.layer[0]\n",
    "# attention_layer = first_transformer_block.attention\n",
    "\n",
    "# print(attention_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
    "\n",
    "class TFInjectMultiHeadSelfAttention(MHSA):\n",
    "\n",
    "    def load_NER_attention(self, NER_attention):\n",
    "        self.NER_attention = NER_attention\n",
    "\n",
    "    def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
    "        key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
    "        return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel\n",
    "\n",
    "class QuestionAnsweringModel(keras.Model):\n",
    "\n",
    "    def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "\n",
    "        self.transformer_model = transformer_model\n",
    "        # Apply layer change to first attention block\n",
    "        transformer_model.layers[0].transformer.layer[0].attention = \\\n",
    "            TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
    "        \n",
    "        # Add all remaining layers\n",
    "        self.dense_S = layers.Dense(1)\n",
    "        self.dense_E = layers.Dense(1)\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.softmax_S = layers.Softmax(name='out_S')\n",
    "        self.softmax_E = layers.Softmax(name='out_E')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        NER_attention = inputs[\"NER_attention\"]\n",
    "        # token_type_ids = inputs[\"token_type_ids\"] # uncomment if using BERT\n",
    "\n",
    "        # Load the NER tensor into the custom layer\n",
    "        self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
    "\n",
    "        out = self.transformer_model(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                # \"token_type_ids\": token_type_ids # uncomment if using BERT\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # TODO: chose which layers\n",
    "        hidden_states = out.hidden_states\n",
    "        chosen_states_idx = [3, 4, 5, 6]\n",
    "\n",
    "        # TODO: chose merging method\n",
    "        chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
    "\n",
    "        # output = layers.Bidirectional(layers.LSTM(64, return_sequences = True, activation = \"relu\"))(chosen_hidden_states)\n",
    "        # output = layers.Dense(2, activation = \"softmax\")(output) # 2 because we need both \n",
    "\n",
    "        out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
    "        out_S = self.flatten(out_S)\n",
    "        out_S = self.softmax_S(out_S)\n",
    "\n",
    "        out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
    "        out_E = self.flatten(out_E)\n",
    "        out_E = self.softmax_E(out_E)\n",
    "\n",
    "        return {'out_S': out_S, 'out_E': out_E}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = QuestionAnsweringModel(transformer_model)\n",
    "\n",
    "# model.compile(tf.keras.optimizers.Adam(3e-5), \n",
    "#                 loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
    "#                 metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
    "\n",
    "# checkpoint_path = \"training\" + str(hidden_state) + \"/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(os.path.join(ROOT_PATH, 'data', checkpoint_path))\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath = checkpoint_path,\n",
    "#     verbose=1,\n",
    "#     save_weights_only = True,\n",
    "#     save_best_only = False # only in this case, \n",
    "# )\n",
    "\n",
    "# model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# history = model.fit(\n",
    "#     train_ds, \n",
    "#     validation_data=val_ds,\n",
    "#     epochs=5, \n",
    "#     callbacks=[cp_callback]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_end_token_from_probabilities(pstartv: np.array, \n",
    "#                                        pendv: np.array, \n",
    "#                                        dim:int=512) -> List[List[int]]:\n",
    "#     '''\n",
    "#     Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
    "#     '''\n",
    "#     idxs = []\n",
    "#     for i in range(pstartv.shape[0]):\n",
    "#         pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
    "#         pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
    "#         sums = pstart + pend\n",
    "#         sums = np.triu(sums, k=1) # Zero out lower triangular matrix + diagonal\n",
    "#         val = np.argmax(sums)\n",
    "#         row = val // dim\n",
    "#         col = val - dim*row\n",
    "#         idxs.append([row,col])\n",
    "#     return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_ds.take(1):\n",
    "#     random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
    "#     input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
    "#     # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
    "#     print(\"Random sample n°\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
    "    \n",
    "#     print(\"Question + context: \")\n",
    "#     print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "#     real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
    "#     real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
    "#     real_limits = [real_start, real_end]\n",
    "\n",
    "#     # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
    "    \n",
    "#     print(\"Real limits: \", real_limits)\n",
    "#     print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
    "#     print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
    "    \n",
    "#     predicted_limits = start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
    "#     print(\"Predicted_limits: \", predicted_limits)\n",
    "#     print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
    "#     print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
