{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgvhj_fdjwd"
      },
      "outputs": [],
      "source": [
        "# Authentication & Google Drive-free version of the below cells, uncomment if there are problems\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers  # https://huggingface.co/docs/transformers/installation\n",
        "    !nvidia-smi                 # Check which GPU has been chosen for us\n",
        "    !rm -rf logs\n",
        "    # Download the dataset from personal drive\n",
        "    !mkdir data\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU setup\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "metadata": {
        "id": "dlTLhbBLeOsa",
        "outputId": "91f7abd7-90d0-4bf7-8ad4-5337672c5a84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.47.10:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.47.10:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QGC_45Iab0xm"
      },
      "outputs": [],
      "source": [
        "# PRIVATE CELL\n",
        "git_token = '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'\n",
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZfNL_Qz3L2Iv"
      },
      "outputs": [],
      "source": [
        "# COLAB ONLY CELLS\n",
        "#try:\n",
        "#    import google.colab\n",
        "#    IN_COLAB = True\n",
        "#    !pip3 install transformers\n",
        "#    !nvidia-smi             # Check which GPU has been chosen for us\n",
        "#    !rm -rf logs\n",
        "#    from google.colab import drive\n",
        "#    drive.mount('/content/drive')\n",
        "#    %cd /content/drive/MyDrive/GitHub/\n",
        "#    !git clone https://{git_token}@github.com/{username}/{repository}\n",
        "#    %cd {repository}\n",
        "#    %ls\n",
        "#except:\n",
        "#    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tX8o7g0cL2Iz",
        "outputId": "03f28b19-504b-488b-e983-3c8b9a7360ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from config import Config\n",
        "config = Config()\n",
        "import utils\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "random.seed(config.RANDOM_SEED)\n",
        "tf.random.set_seed(config.RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HXUc4EfVdjwi"
      },
      "outputs": [],
      "source": [
        "TRAINING_FILE = os.path.join('.', 'data', 'training_set.json') # comment this if directory works differently\n",
        "# TRAINING_FILE = os.path.join('data', 'training_set.json') # uncomment this if directory works differently\n",
        "questions = utils.read_question_set(TRAINING_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WQOfGygbdjwj"
      },
      "outputs": [],
      "source": [
        "TRAIN_SPLIT_ELEM = int(len(questions['data']) * config.TRAIN_SPLIT)\n",
        "data = random.sample(questions['data'], len(questions['data'])) # reshuffle the samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X-P6e7n1djwj"
      },
      "outputs": [],
      "source": [
        "train_dataset = {'data': data[:TRAIN_SPLIT_ELEM]} # recreate the original dataset structure lost by shuffling through the dictionary\n",
        "val_dataset = {'data': data[TRAIN_SPLIT_ELEM:]}\n",
        "\n",
        "# we also create a small training set to test the model while building it, just to speed up\n",
        "\n",
        "small_data = random.sample(train_dataset[\"data\"], config.SMALL_TRAIN_LEN)\n",
        "small_train_dataset = {'data': small_data}\n",
        "small_val_data = random.sample(val_dataset[\"data\"], config.SMALL_VAL_LEN)\n",
        "small_val_dataset = {'data': small_val_data}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3yVtTZPdjwk"
      },
      "source": [
        "### Dataset choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jCi5YTc1djwm",
        "outputId": "f22010ff-5660-44c5-cd62-ef57ab0bad3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 331/331 [00:58<00:00,  5.61it/s]\n",
            "100%|██████████| 111/111 [00:21<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-25d10634b231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# batch the dataset and prefetch to increase speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(self, batch_size, drop_remainder, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1701\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1702\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mBatchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m       return ParallelBatchDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, batch_size, drop_remainder, name)\u001b[0m\n\u001b[1;32m   5124\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5125\u001b[0m         \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_remainder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5126\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   5127\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mbatch_dataset_v2\u001b[0;34m(input_dataset, batch_size, drop_remainder, output_types, output_shapes, parallel_copy, metadata, name)\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: './data/full_datasets/train_ds_training/16349502926194597089')\n\tEncountered when executing an operation using EagerExecutor. This error cancels all future operations and poisons their output tensors. [Op:BatchDatasetV2]"
          ]
        }
      ],
      "source": [
        "full_dataset=True # choose between full and small dataset\n",
        "if full_dataset:\n",
        "    TRAIN_DATASET = train_dataset\n",
        "    VAL_DATASET = val_dataset\n",
        "else:\n",
        "    TRAIN_DATASET = small_train_dataset\n",
        "    VAL_DATASET = small_val_dataset\n",
        "\n",
        "create_and_save = True     # fully create the dataset in RAM, and then save it on disk\n",
        "load = False                 # load a previously created dataset from disk\n",
        "generator = False           # if not enough RAM, create a dataset through a generator\n",
        "\n",
        "for_training = True         # returns a (feature, labels) dataset used in the fit method of the model\n",
        "NER_attention = False       # returns a (feature, id) dataset used during inference\n",
        "\n",
        "# if you need to save or load a model, choose the right path according to the previous 2 flags\n",
        "if create_and_save or load:\n",
        "    if for_training:\n",
        "        if NER_attention:\n",
        "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING_NER\n",
        "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING_NER\n",
        "        else:\n",
        "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_TRAINING\n",
        "            PATH_VAL = config.SAVE_PATH_VAL_DS_TRAINING\n",
        "    else:\n",
        "        if NER_attention:\n",
        "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE_NER\n",
        "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE_NER\n",
        "        else:\n",
        "            PATH_TRAIN = config.SAVE_PATH_TRAIN_DS_INFERENCE\n",
        "            PATH_VAL = config.SAVE_PATH_VAL_DS_INFERENCE\n",
        "\n",
        "# dataset creation\n",
        "# for small dataset, just create it and store it in RAM, it's fast\n",
        "if create_and_save or not full_dataset: # for full dataset, you can either create it and save it on disk\n",
        "    train_ds = utils.create_full_dataset(TRAIN_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
        "    val_ds = utils.create_full_dataset(VAL_DATASET, config, return_labels=for_training, return_NER_attention=NER_attention, return_question_id=(not for_training))\n",
        "    #if for_training and full_dataset: # only for full datasets, save them on disk\n",
        "    #    tf.data.experimental.save(train_ds, PATH_TRAIN)\n",
        "    #    tf.data.experimental.save(val_ds, PATH_VAL)\n",
        "elif load and full_dataset: # only for full datasets, you can load the previously created dataset from disk\n",
        "    train_ds = tf.data.experimental.load(PATH_TRAIN)\n",
        "    val_ds = tf.data.experimental.load(PATH_VAL)\n",
        "elif generator and full_dataset: # only for full datasets, if there is not enough RAM, you can create a dataset from a generator\n",
        "    train_ds = utils.create_dataset_and_ids(TRAIN_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
        "    val_ds = utils.create_dataset_and_ids(VAL_DATASET, config, for_training=for_training, use_NER_attention=NER_attention)\n",
        "else: # if you don't enter in any of the above, something is wrong\n",
        "    raise Exception(\"Something wrong with dataset creation\")\n",
        "\n",
        "# batch the dataset and prefetch to increase speed\n",
        "train_ds = train_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(128).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgIrtYDNdjwn"
      },
      "outputs": [],
      "source": [
        "# Check if the dataset has the wanted data inside\n",
        "for batch in train_ds.take(1):\n",
        "    print(batch[0].keys())\n",
        "    print(batch[1].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fU9ePSGdjwn"
      },
      "source": [
        "## Training Chioce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMvdBdIsdjwn"
      },
      "outputs": [],
      "source": [
        "normal_training = True\n",
        "train_separate_layers = False\n",
        "NER_training = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qlUBpPsdjwn"
      },
      "source": [
        "#### Normal Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import DistilBertTokenizerFast, TFDistilBertModel\n",
        "\n",
        " \n",
        " def create_standard_model(hidden_state_list=[3,4,5,6]) -> keras.Model:\n",
        "    '''\n",
        "    A utility method to create our \"standard\" model.\n",
        "\n",
        "    Inputs:\n",
        "        - The index or list of indexes of transformer hidden states to be \n",
        "            concatenated before the final classification (default: [3,4,5,6])\n",
        "    \n",
        "    Outputs:\n",
        "        - The complete Keras model\n",
        "    '''\n",
        "    input_ids = tf.keras.Input(shape=(512, ), \n",
        "        name=\"input_ids\", dtype='int32'\n",
        "    )\n",
        "    attention_mask = tf.keras.Input(shape=(512, ), \n",
        "        name=\"attention_mask\", dtype='int32'\n",
        "    )\n",
        "    # token_type_ids = tf.keras.Input(shape=(SHAPE_ATTENTION_MASK, ), dtype='int32') # uncomment if using BERT\n",
        "\n",
        "    transformer = TFDistilBertModel.from_pretrained( # The instantiation of the transformer model\n",
        "            'distilbert-base-uncased', output_hidden_states = True)(\n",
        "        {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            # \"token_type_ids\": token_type_ids # uncomment if using BERT\n",
        "        }\n",
        "    )\n",
        "\n",
        "    hidden_states = transformer.hidden_states\n",
        "    \n",
        "    if isinstance(hidden_state_list, int):\n",
        "        chosen_hidden_states = hidden_states[hidden_state_list]\n",
        "    elif len(hidden_state_list) == 1:\n",
        "        chosen_hidden_states = hidden_states[hidden_state_list[0]]\n",
        "    else:\n",
        "        chosen_hidden_states = layers.concatenate(\n",
        "            tuple([hidden_states[i] for i in hidden_state_list])\n",
        "        )\n",
        "    \n",
        "    out_S = layers.Dense(1)(chosen_hidden_states) # Dot product between token representation and start vector\n",
        "    out_S = layers.Flatten()(out_S)\n",
        "    out_S = layers.Softmax(name=\"out_S\", dtype='float32')(out_S)\n",
        "\n",
        "    out_E = layers.Dense(1)(chosen_hidden_states) # Dot product between token representation and end vector\n",
        "    out_E = layers.Flatten()(out_E)\n",
        "    out_E = layers.Softmax(name=\"out_E\", dtype='float32')(out_E)\n",
        "\n",
        "    return keras.Model(\n",
        "        inputs=[input_ids, attention_mask],\n",
        "        outputs = [out_S, out_E]\n",
        "    )"
      ],
      "metadata": {
        "id": "xc9yt8wL2JHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bhF1yI9djwo"
      },
      "outputs": [],
      "source": [
        "if normal_training:\n",
        "    \n",
        "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_normal\",  \"cp-{epoch:04d}.ckpt\")\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "    with strategy.scope():\n",
        "        model = create_standard_model([3, 4, 5, 6])\n",
        "        model.compile(tf.keras.optimizers.Adam(3e-6),\n",
        "                # Anything between 2 and `steps_per_epoch` could help here.\n",
        "                steps_per_execution = 100,\n",
        "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
        "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
        "\n",
        "\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = 3\n",
        "    )\n",
        "\n",
        "    model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds, \n",
        "        validation_data=val_ds,\n",
        "        epochs=10, \n",
        "        callbacks=[\n",
        "            es_callback\n",
        "        ],\n",
        "        use_multiprocessing = True,\n",
        "        initial_epoch=0\n",
        "        )\n",
        "\n",
        "\n",
        "    history = history.history\n",
        "\n",
        "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
        "    plt.plot(losses)\n",
        "    plt.legend(losses.columns)\n",
        "\n",
        "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
        "    plt.plot(accs)\n",
        "    plt.legend(accs.columns)\n",
        "\n",
        "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
        "        json.dump(history, f)\n",
        "\n",
        "    model.save_weights('normal_model_3456')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iiB17KKdjwo"
      },
      "source": [
        "#### Training separate layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0pdEzStdjwp"
      },
      "outputs": [],
      "source": [
        "if train_separate_layers:\n",
        "    ### FREEZE #### the layers to only train the head if needed\n",
        "    for layer in config.transformer_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # training cell example: train layers separately\n",
        "    histories = []\n",
        "    for hidden_state in range(1, 7):\n",
        "        checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_separate\", \"layer_\" + str(hidden_state), \"cp-{epoch:04d}.ckpt\")\n",
        "        checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "        \n",
        "        with strategy.scope():\n",
        "            model = config.create_model(hidden_state)\n",
        "            model.compile(tf.keras.optimizers.Adam(3e-6),\n",
        "                # Anything between 2 and `steps_per_epoch` could help here.\n",
        "                steps_per_execution = 100,\n",
        "                loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
        "                metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
        "\n",
        "        print(\"----------- Training model with head attached to layer number \" + str(hidden_state)+ \" -----------\\n\")\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds, \n",
        "            validation_data=val_ds,\n",
        "            epochs=10, \n",
        "            callbacks=[cp_callback]\n",
        "            )\n",
        "        \n",
        "        history = history.history\n",
        "\n",
        "        with open(os.path.join(checkpoint_dir, \"history.json\"), 'w') as f:\n",
        "            json.dump(history, f)\n",
        "\n",
        "        histories.append(history)\n",
        "        model.save_weights(checkpoint_path.format(epoch=10))\n",
        "\n",
        "    # final plot\n",
        "    x = [i for i in range(1, 6)]\n",
        "    for history in histories:\n",
        "        plt.plot(x, history['val_loss'])\n",
        "\n",
        "    plt.xticks([i for i in range(1,6)])\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"val_loss\")\n",
        "    plt.legend([str(i) for i in range(1,7)])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH8PBo_mdjwp"
      },
      "source": [
        "#### Training with NER attention enhancement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nIi-K5Gdjwp"
      },
      "outputs": [],
      "source": [
        "if NER_training:\n",
        "    main_layer = config.transformer_model.layers[0]\n",
        "    transformer_layers = main_layer.transformer\n",
        "    first_transformer_block = transformer_layers.layer[0]\n",
        "    attention_layer = first_transformer_block.attention\n",
        "\n",
        "    print(attention_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3bgD5o7djwp"
      },
      "outputs": [],
      "source": [
        "if NER_training:\n",
        "    from transformers.models.distilbert.modeling_tf_distilbert import TFMultiHeadSelfAttention as MHSA\n",
        "\n",
        "    class TFInjectMultiHeadSelfAttention(MHSA):\n",
        "\n",
        "        def load_NER_attention(self, NER_attention):\n",
        "            self.NER_attention = NER_attention\n",
        "\n",
        "        def call(self, query, key, value, mask, head_mask, output_attentions, training=False):\n",
        "            # key = key*tf.reshape(self.NER_attention, [self.NER_attention.shape[0], self.NER_attention.shape[1], 1])\n",
        "            key = key * tf.expand_dims(self.NER_attention, axis=-1)\n",
        "            return super().call(query, key, value, mask, head_mask, output_attentions, training=training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i4olkCzdjwq"
      },
      "outputs": [],
      "source": [
        "if NER_training:\n",
        "    CHOSEN_ENHANCED_LAYER = 0\n",
        "    CHOSEN_OUTPUT_STATES_IDX = [3, 4, 5, 6]\n",
        "    from transformers import TFDistilBertModel\n",
        "\n",
        "    class QuestionAnsweringModel(keras.Model):\n",
        "\n",
        "        def __init__(self, transformer_model: TFDistilBertModel) -> None:\n",
        "            super(QuestionAnsweringModel, self).__init__()\n",
        "\n",
        "            self.transformer_model = transformer_model\n",
        "            # Apply layer change to first attention block\n",
        "            self.transformer_model.layers[0].transformer.layer[CHOSEN_ENHANCED_LAYER].attention = \\\n",
        "                TFInjectMultiHeadSelfAttention(transformer_model.config)\n",
        "            \n",
        "            # Add all remaining layers\n",
        "            self.dense_S = layers.Dense(1)\n",
        "            self.dense_E = layers.Dense(1)\n",
        "            self.flatten = layers.Flatten()\n",
        "            self.softmax_S = layers.Softmax(name='out_S')\n",
        "            self.softmax_E = layers.Softmax(name='out_E')\n",
        "\n",
        "        def call(self, inputs, training=False):\n",
        "            input_ids = inputs[\"input_ids\"]\n",
        "            attention_mask = inputs[\"attention_mask\"]\n",
        "            NER_attention = inputs[\"NER_attention\"]\n",
        "\n",
        "            # Load the NER tensor into the custom layer\n",
        "            self.transformer_model.layers[0].transformer.layer[0].attention.load_NER_attention(NER_attention)\n",
        "\n",
        "            out = self.transformer_model(\n",
        "                {\n",
        "                    \"input_ids\": input_ids,\n",
        "                    \"attention_mask\": attention_mask,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            hidden_states = out.hidden_states\n",
        "            chosen_states_idx = CHOSEN_OUTPUT_STATES_IDX\n",
        "\n",
        "            chosen_hidden_states = tf.concat([hidden_states[i] for i in chosen_states_idx], axis=2)\n",
        "\n",
        "            out_S = self.dense_S(chosen_hidden_states) # dot product between token representation and start vector\n",
        "            out_S = self.flatten(out_S)\n",
        "            out_S = self.softmax_S(out_S)\n",
        "\n",
        "            out_E = self.dense_E(chosen_hidden_states) # dot product between token representation and end vector\n",
        "            out_E = self.flatten(out_E)\n",
        "            out_E = self.softmax_E(out_E)\n",
        "\n",
        "            return {'out_S': out_S, 'out_E': out_E}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLd3vg2kdjwq"
      },
      "outputs": [],
      "source": [
        "if NER_training:\n",
        "    checkpoint_path = os.path.join(config.ROOT_PATH, \"data\", \"training\", \"training_NER\", \"cp-{epoch:04d}.ckpt\")\n",
        "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "    model = QuestionAnsweringModel(config.transformer_model)\n",
        "\n",
        "    model.compile(tf.keras.optimizers.Adam(3e-6), \n",
        "                    loss={'out_S': 'binary_crossentropy', 'out_E': 'binary_crossentropy'},\n",
        "                    metrics={'out_S': 'accuracy', 'out_E': 'accuracy'})\n",
        "\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = checkpoint_path,\n",
        "        verbose=1,\n",
        "        save_weights_only = True,\n",
        "        save_best_only = False\n",
        "    )\n",
        "\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = 3\n",
        "    )\n",
        "\n",
        "    model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds, \n",
        "        validation_data=val_ds,\n",
        "        epochs=10, \n",
        "        callbacks=[\n",
        "            cp_callback,\n",
        "            es_callback\n",
        "        ]\n",
        "        )\n",
        "\n",
        "    history = history.history\n",
        "\n",
        "    print(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "    losses = pd.DataFrame(history, columns=[\"loss\", \"val_loss\", \"out_S_loss\", \"out_E_loss\", \"val_out_S_loss\", \"val_out_E_loss\"])\n",
        "    plt.plot(losses)\n",
        "    plt.legend(losses.columns)\n",
        "\n",
        "    accs = pd.DataFrame(history, columns=[\"out_S_accuracy\", \"out_E_accuracy\", \"val_out_S_accuracy\", \"val_out_E_accuracy\"])\n",
        "    plt.plot(accs)\n",
        "    plt.legend(accs.columns)\n",
        "\n",
        "    with open(os.path.join(checkpoint_dir, \"history.json\"), \"w\") as f:\n",
        "        json.dump(history, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvD2q0gqdjwr"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGnfP_agdjwr"
      },
      "outputs": [],
      "source": [
        "# for batch in train_ds.take(1):\n",
        "#     random_in_batch = np.random.randint(0, config.BATCH_SIZE-1)\n",
        "#     input_ids = batch[0][\"input_ids\"][random_in_batch]\n",
        "#     # attention_mask = sample[0][\"attention_mask\"][random_in_batch]\n",
        "#     print(\"Random sample n°\", random_in_batch, \"in batch of\", config.BATCH_SIZE)\n",
        "    \n",
        "#     print(\"Question + context: \")\n",
        "#     print(tokenizer.decode(input_ids, skip_special_tokens=True), \"\\n\")\n",
        "\n",
        "#     real_start = np.argmax(batch[1][\"out_S\"][random_in_batch])\n",
        "#     real_end = np.argmax(batch[1][\"out_E\"][random_in_batch])\n",
        "#     real_limits = [real_start, real_end]\n",
        "\n",
        "#     # print(np.shape(model.predict(batch[0])[0][random_in_batch]))\n",
        "    \n",
        "#     print(\"Real limits: \", real_limits)\n",
        "#     print(\"Real answer tokens: \", input_ids[real_limits[0]:real_limits[1]+1].numpy())\n",
        "#     print(\"Real answer: \", tokenizer.decode(input_ids[real_limits[0]:real_limits[1]+1], skip_special_tokens=False))\n",
        "    \n",
        "#     predicted_limits = utils.start_end_token_from_probabilities(*model.predict(batch[0]))[random_in_batch]\n",
        "#     print(\"Predicted_limits: \", predicted_limits)\n",
        "#     print(\"Predicted answer tokens: \", input_ids[predicted_limits[0]:predicted_limits[1]+1].numpy())\n",
        "#     print(\"Predicted answer: \", tokenizer.decode(input_ids[predicted_limits[0]:predicted_limits[1]+1], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "01c8d5b6c814520bf3d0a47db1a4339a225d88b20bf2135f185f380fb4a5b723"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "train_clean.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}