{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aokK8JVRVCZq",
    "outputId": "9bbd2fac-4865-4506-85ae-895926b78655"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install benepar\n",
    "    !nvidia-smi\n",
    "    !mkdir data\n",
    "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r7EbGcpU_gT"
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DAc6XGZU_gZ"
   },
   "source": [
    "In this notebook we explore some baselines for the problem of question answering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M084ZCuyU_ga",
    "outputId": "12af8c4e-af60-449c-d8b3-c9a9e16e5ad1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import spacy\n",
    "import benepar\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "# Download spacy corpora of text in case it's needed\n",
    "!python -m spacy download en_core_web_sm\n",
    "# Download the benepar neural constituency parser\n",
    "benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz0raoxpU_gd"
   },
   "source": [
    "Load the training dataset and create a DataFrame containing:\n",
    "- The paragraph's text\n",
    "- The question's text\n",
    "- The questions's ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cFLPG6UIU_ge"
   },
   "outputs": [],
   "source": [
    "dataset = utils.read_question_set(os.path.join('..', 'data','dev_set.json'))\n",
    "\n",
    "# Create a more useful data structure using list comprehensions\n",
    "questions = pd.DataFrame([{\n",
    "        'context': paragraph['context'],\n",
    "        'question': qa['question'],\n",
    "        'questionID': qa['id'],\n",
    "    }   for article in dataset['data']\n",
    "        for paragraph in article['paragraphs']\n",
    "        for qa in paragraph['qas'] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim the contexts to 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3BfXHN2fU_gf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>questionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>56be4db0acb8001400a502ec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>56be4db0acb8001400a502ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>56be4db0acb8001400a502ee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>56be4db0acb8001400a502ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>56be4db0acb8001400a502f0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "3  Super Bowl 50 was an American football game to...   \n",
       "4  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                            question                questionID  \n",
       "0  Which NFL team represented the AFC at Super Bo...  56be4db0acb8001400a502ec  \n",
       "1  Which NFL team represented the NFC at Super Bo...  56be4db0acb8001400a502ed  \n",
       "2                Where did Super Bowl 50 take place?  56be4db0acb8001400a502ee  \n",
       "3                  Which NFL team won Super Bowl 50?  56be4db0acb8001400a502ef  \n",
       "4  What color was used to emphasize the 50th anni...  56be4db0acb8001400a502f0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(questions))\n",
    "display(questions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAcp--h_sAdT"
   },
   "source": [
    "Create a function that given a prediction generator produces the spans of text in the context containing the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1FygrubesG3h"
   },
   "outputs": [],
   "source": [
    "def get_predictions(prediction_generator, limit=None):\n",
    "    predictions = {}\n",
    "    limit = range(limit) if limit is not None else range(len(questions))\n",
    "    # Instantiate prediction generator\n",
    "    predictor_iterator = prediction_generator()\n",
    "    for q in tqdm(limit):\n",
    "        # Obtain start and end probabilities from the baseline function\n",
    "        pstartv, pendv = next(predictor_iterator)\n",
    "        # Obtain the indices of the best answer\n",
    "        start, end = utils.start_end_token_from_probabilities(\n",
    "            pstartv, pendv, dim=pstartv.shape[1]\n",
    "        )[0]\n",
    "        # Add the ID-answer pair in the predictions dictionary\n",
    "        id = questions['questionID'].iloc[q]\n",
    "        text = questions['context'].iloc[q]\n",
    "        predictions[id] = text[start:end]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZr1s1_SU_gj"
   },
   "source": [
    "## 1. Random prediction baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHSYKH1NU_gk"
   },
   "source": [
    "We implement a predictor that returns random start and end probabilities. Then, we use the function `start_end_token_from_probabilities` to obtain the max-scoring randomly generated span of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M64GMTMIU_gl"
   },
   "outputs": [],
   "source": [
    "def random_baseline_predict():\n",
    "    '''\n",
    "    Creates random prediction vectors.\n",
    "    '''\n",
    "    for context in questions['context']:\n",
    "        pstartv = np.random.random((1, len(context)))\n",
    "        pendv = np.random.random((1, len(context)))\n",
    "        yield pstartv, pendv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MkAfNtD_U_gl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [00:52<00:00, 202.45it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('eval', 'random_predictions_eval.txt'), 'w') as f:\n",
    "    json.dump(get_predictions(random_baseline_predict), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlCC0WnSU_gm"
   },
   "source": [
    "## 2. Sliding window baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhVgQIrLU_gm"
   },
   "source": [
    "The sliding window baseline is implemented in the same way it was presented in the\n",
    "SQuAD v1 paper and in the [MCTest paper](https://aclanthology.org/D13-1020.pdf) by Richardson et al.\n",
    "which originally proposed it.\n",
    "\n",
    "Apart from the paragraph and the question, the implementation also needs a set of candidate answers.\n",
    "SQuAD's paper proposes to \"*only use spans which are constituents in the constituency parse generated by\n",
    "Stanford CoreNLP*\". In our case, we use a neural parser: **Berkeley Neural Parser**, which is the option\n",
    "proposed by the `spacy` library, which we are already using as a named entity extractor in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slw_questions = questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [05:31<00:00, 31.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy's pipeline which we'll use for analysis\n",
    "spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "# Disable all elements but the tokenizer\n",
    "spacy_pipeline.disable_pipes(\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\")\n",
    "# Add the \"sentencizer\" component and the neural parser\n",
    "spacy_pipeline.add_pipe('sentencizer')\n",
    "spacy_pipeline.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\",\n",
    "                                           \"disable_tagger\": \"true\"})\n",
    "\n",
    "def run_pipeline(text):\n",
    "    try: \n",
    "        doc = spacy_pipeline(text)\n",
    "    except ValueError:\n",
    "        doc = spacy_pipeline(text[:512])\n",
    "    return doc\n",
    "\n",
    "tqdm.pandas()\n",
    "# Preprocess questions and context\n",
    "slw_questions['context'] = slw_questions['context'].progress_apply(run_pipeline)\n",
    "slw_questions['question'] = slw_questions['question'].progress_apply(run_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(w, P):\n",
    "    # Sum of all words in the sentence where the word is equal to w\n",
    "    return sum([1 for p in P if w == p])\n",
    "\n",
    "def IC(w, P):\n",
    "    # A scoring function based on inverse word frequency\n",
    "    return math.log(1 + (1/C(w, P)))\n",
    "\n",
    "def sliding_window_baseline_predict():\n",
    "    # Extract all processed questions and contexts from the generators (more efficient)\n",
    "    for i in range(len(slw_questions)):\n",
    "        question = slw_questions['question'].iloc[i]\n",
    "        context = slw_questions['context'].iloc[i]\n",
    "\n",
    "        # Step 1: Create a set of words present in the question (always ignore punctuation)\n",
    "        Q = set([str(token) for token in question if not token.is_punct])\n",
    "\n",
    "        # Step 2: From the processed text we can obtain the list of constituents,\n",
    "        # which will be the proposed answers to each question.\n",
    "        proposed_answers = [ {\n",
    "                'answer': answer,\n",
    "                'start': answer.start_char,\n",
    "                'end': answer.end_char,\n",
    "                'sentence': sentence,\n",
    "                'start_in_sentence': answer.start_char - sentence.start_char,\n",
    "                'end_in_sentence': answer.end_char - sentence.start_char,\n",
    "                'token_set': set(str(tok) for tok in answer if not tok.is_punct)\n",
    "            }\n",
    "            for sentence in list(context.sents)         # Iterate over sentences\n",
    "            for answer in list(sentence._.children) # Iterate over constitents\n",
    "            if not len(set(tok for tok in answer if not tok.is_punct)) == 0     # Ignore span if it only contains punctuation\n",
    "        ]\n",
    "        \n",
    "        # Step 3: Select a subset of the proposed answer based on unigram overlap\n",
    "        # with the rest of the sentence\n",
    "        spans_before = [ { str(tok) for tok in \n",
    "                   p['sentence'][0:p['start_in_sentence']] }\n",
    "                   for p in proposed_answers ]\n",
    "        spans_after = [ { str(tok) for tok in \n",
    "                          p['sentence'][p['end_in_sentence']:] }\n",
    "                          for p in proposed_answers]\n",
    "        # Compute unigram overlap between before/after spans and question\n",
    "        uni_overlaps = [ len(Q.intersection(spans_before[i])) + \n",
    "                         len(Q.intersection(spans_after[i]))\n",
    "                         for i in range(len(proposed_answers)) ]\n",
    "        \n",
    "        # Keep only candidates with maximal overlap\n",
    "        proposed_answers = [proposed_answers[i] for i in \n",
    "                  np.where(uni_overlaps == np.max(uni_overlaps))[0]]\n",
    "        \n",
    "        # Step 4: Now that we have the question's text, the proposed answers and the context,\n",
    "        # we can apply the sliding window algorithm, which computes a score based on the n-gram\n",
    "        # overlap between the question's words and the proposed spans of text.\n",
    "        scores = []\n",
    "        for i in range(len(proposed_answers)):     # Iterate over all possible answers\n",
    "            S = {str(s) for s in proposed_answers[i]['token_set'].union(Q)}  # Unite the question and the answer words\n",
    "            # SQuAD uses only the sentence containing the answer for context\n",
    "            P = [str(t) for t in proposed_answers[i]['sentence'] if not t.is_punct]\n",
    "            # Create a LUT of word scores for efficiency \n",
    "            adder = {\n",
    "                p: IC(p, P)\n",
    "                for p in P\n",
    "            }\n",
    "            sw = max([                # Select the maximum score from the...\n",
    "                    sum([             # ...sum over the scores iterating over words,\n",
    "                    adder[P[j+w]]    # Obtain pre-computed score for word P[j,w]\n",
    "                    if P[j+w] in S else 0   # If the word is in S\n",
    "                    for w in range(len(S))  # For each index w in S\n",
    "                    if j+w < len(P)])       # Unless we go out of bounds\n",
    "                for j in range(len(P)) ])  # Obtain a full list for all words in the passage\n",
    "            scores.append(sw)\n",
    "\n",
    "        # Create the pstartv and pendv vectors \n",
    "        pstartv = np.zeros((1, len(context.text)+1))\n",
    "        pendv = np.zeros((1, len(context.text)+1))\n",
    "        best_scoring_answer = proposed_answers[np.argmax(scores)]\n",
    "        pstartv[0, best_scoring_answer['start']] = 1\n",
    "        pendv[0, best_scoring_answer['end']] = 1\n",
    "\n",
    "        yield pstartv, pendv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "folWKUq9U_gp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [01:08<00:00, 154.55it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('eval', 'sliding_predictions_eval.txt'), 'w') as f:\n",
    "    json.dump(get_predictions(sliding_window_baseline_predict), f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "baselines.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c90f87e5ee49fb7af5481158977e62ba01ae4f54defb58032c9ba197b530ea3c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
