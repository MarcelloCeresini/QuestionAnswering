{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aokK8JVRVCZq",
    "outputId": "9bbd2fac-4865-4506-85ae-895926b78655"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    !pip install benepar\n",
    "    !nvidia-smi\n",
    "    !mkdir data\n",
    "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19jcMX4KFwVAp4yvgvw1GXSnSgpoQytqg' -O data/training_set.json\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r7EbGcpU_gT"
   },
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DAc6XGZU_gZ"
   },
   "source": [
    "In this notebook we explore some baselines for the problem of question answering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M084ZCuyU_ga",
    "outputId": "12af8c4e-af60-449c-d8b3-c9a9e16e5ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: jinja2 in /home/volpepe/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/volpepe/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/volpepe/.local/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/volpepe/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/volpepe/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/volpepe/miniconda3/envs/NLP/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/volpepe/.local/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     /home/volpepe/nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import spacy\n",
    "import benepar\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "# Download spacy corpora of text in case it's needed\n",
    "!python -m spacy download en_core_web_sm\n",
    "# Download the benepar neural constituency parser\n",
    "benepar.download('benepar_en3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz0raoxpU_gd"
   },
   "source": [
    "Load the training dataset and create a DataFrame containing:\n",
    "- The paragraph's text\n",
    "- The question's text\n",
    "- The questions's ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cFLPG6UIU_ge"
   },
   "outputs": [],
   "source": [
    "dataset = utils.read_question_set(os.path.join('..', 'data','training_set.json'))\n",
    "\n",
    "# Create a more useful data structure using list comprehensions\n",
    "questions = pd.DataFrame([{\n",
    "        'context': paragraph['context'],\n",
    "        'question': qa['question'],\n",
    "        'questionID': qa['id'],\n",
    "    }   for article in dataset['data']\n",
    "        for paragraph in article['paragraphs']\n",
    "        for qa in paragraph['qas'] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim the contexts to 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3BfXHN2fU_gf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87599\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>questionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question                questionID  \n",
       "0  To whom did the Virgin Mary allegedly appear i...  5733be284776f41900661182  \n",
       "1  What is in front of the Notre Dame Main Building?  5733be284776f4190066117f  \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  5733be284776f41900661180  \n",
       "3                  What is the Grotto at Notre Dame?  5733be284776f41900661181  \n",
       "4  What sits on top of the Main Building at Notre...  5733be284776f4190066117e  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(questions))\n",
    "display(questions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAcp--h_sAdT"
   },
   "source": [
    "Create a function that given a prediction generator produces the spans of text in the context containing the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1FygrubesG3h"
   },
   "outputs": [],
   "source": [
    "def get_predictions(prediction_generator, limit=None):\n",
    "    predictions = {}\n",
    "    limit = range(limit) if limit is not None else range(len(questions))\n",
    "    # Instantiate prediction generator\n",
    "    predictor_iterator = prediction_generator()\n",
    "    for q in tqdm(limit):\n",
    "        # Obtain start and end probabilities from the baseline function\n",
    "        pstartv, pendv = next(predictor_iterator)\n",
    "        # Obtain the indices of the best answer\n",
    "        start, end = utils.start_end_token_from_probabilities(\n",
    "            pstartv, pendv, dim=pstartv.shape[1]\n",
    "        )[0]\n",
    "        # Add the ID-answer pair in the predictions dictionary\n",
    "        id = questions['questionID'].iloc[q]\n",
    "        text = questions['context'].iloc[q]\n",
    "        predictions[id] = text[start:end]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZr1s1_SU_gj"
   },
   "source": [
    "## 1. Random prediction baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHSYKH1NU_gk"
   },
   "source": [
    "We implement a predictor that returns random start and end probabilities. Then, we use the function `start_end_token_from_probabilities` to obtain the max-scoring randomly generated span of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M64GMTMIU_gl"
   },
   "outputs": [],
   "source": [
    "def random_baseline_predict():\n",
    "    '''\n",
    "    Creates random prediction vectors.\n",
    "    '''\n",
    "    for context in questions['context']:\n",
    "        pstartv = np.random.random((1, len(context)))\n",
    "        pendv = np.random.random((1, len(context)))\n",
    "        yield pstartv, pendv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MkAfNtD_U_gl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [06:45<00:00, 216.27it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('eval', 'random_predictions.txt'), 'w') as f:\n",
    "    json.dump(get_predictions(random_baseline_predict), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlCC0WnSU_gm"
   },
   "source": [
    "## 2. Sliding window baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhVgQIrLU_gm"
   },
   "source": [
    "The sliding window baseline is implemented in the same way it was presented in the\n",
    "SQuAD v1 paper and in the [MCTest paper](https://aclanthology.org/D13-1020.pdf) by Richardson et al.\n",
    "which originally proposed it.\n",
    "\n",
    "Apart from the paragraph and the question, the implementation also needs a set of candidate answers.\n",
    "SQuAD's paper proposes to \"*only use spans which are constituents in the constituency parse generated by\n",
    "Stanford CoreNLP*\". In our case, we use a neural parser: **Berkeley Neural Parser**, which is the option\n",
    "proposed by the `spacy` library, which we are already using as a named entity extractor in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "slw_questions = questions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/87599 [00:00<?, ?it/s]/home/volpepe/.local/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n",
      " 56%|█████▌    | 49091/87599 [41:52<39:14, 16.35it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 87599/87599 [1:16:54<00:00, 18.98it/s]\n",
      "100%|██████████| 87599/87599 [30:15<00:00, 48.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy's pipeline which we'll use for analysis\n",
    "spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
    "# Disable all elements but the tokenizer\n",
    "spacy_pipeline.disable_pipes(\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\")\n",
    "# Add the \"sentencizer\" component and the neural parser\n",
    "spacy_pipeline.add_pipe('sentencizer')\n",
    "spacy_pipeline.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\",\n",
    "                                           \"disable_tagger\": \"true\"})\n",
    "\n",
    "def run_pipeline(text):\n",
    "    try: \n",
    "        doc = spacy_pipeline(text)\n",
    "    except ValueError:\n",
    "        doc = spacy_pipeline(text[:512])\n",
    "    return doc\n",
    "\n",
    "tqdm.pandas()\n",
    "# Preprocess questions and context\n",
    "slw_questions['context'] = slw_questions['context'].progress_apply(run_pipeline)\n",
    "slw_questions['question'] = slw_questions['question'].progress_apply(run_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(w, P):\n",
    "    # Sum of all words in the sentence where the word is equal to w\n",
    "    return sum([1 for p in P if w == p])\n",
    "\n",
    "def IC(w, P):\n",
    "    # A scoring function based on inverse word frequency\n",
    "    return math.log(1 + (1/C(w, P)))\n",
    "\n",
    "def sliding_window_baseline_predict():\n",
    "    # Extract all processed questions and contexts from the generators (more efficient)\n",
    "    for i in range(len(slw_questions)):\n",
    "        question = slw_questions['question'].iloc[i]\n",
    "        context = slw_questions['context'].iloc[i]\n",
    "\n",
    "        # Step 1: Create a set of words present in the question (always ignore punctuation)\n",
    "        Q = set([token for token in question if not token.is_punct])\n",
    "        # Obtain a list of tokens in the paragraph\n",
    "        P = [str(token) for token in context if not token.is_punct]\n",
    "\n",
    "        # Step 2: From the processed text we can obtain the list of constituents,\n",
    "        # which will be the proposed answers to each question.\n",
    "        proposed_answers = [ {\n",
    "                'answer': answer,\n",
    "                'start': answer.start_char,\n",
    "                'end': answer.end_char,\n",
    "                'token_set': set(tok for tok in answer if not tok.is_punct)\n",
    "            }\n",
    "            for sentence in list(context.sents)         # Iterate over sentences\n",
    "            for answer in list(sentence._.children) # Iterate over constitents\n",
    "            if not len(set(tok for tok in answer if not tok.is_punct)) == 0     # Ignore span if it only contains punctuation\n",
    "        ]\n",
    "\n",
    "        # Step 3: Now that we have the question's text, the proposed answers and the context,\n",
    "        # we can apply the sliding window algorithm, which computes a score based on the n-gram\n",
    "        # overlap between the question's words and the proposed spans of text.\n",
    "        # Create a LUT of word scores for efficiency \n",
    "        adder = {\n",
    "            p: IC(p, P)\n",
    "            for p in P\n",
    "        }\n",
    "        scores = []\n",
    "        for i in range(len(proposed_answers)):     # Iterate over all possible answers\n",
    "            S = set([str(s) for s in proposed_answers[i]['token_set'].union(Q)])  # Unite the question and the answer words\n",
    "            sw = max([                # Select the maximum score from the...\n",
    "                    sum([             # ...sum over the scores iterating over words,\n",
    "                    adder[P[j+w]]    # Obtain pre-computed score for word P[j,w]\n",
    "                    if P[j+w] in S else 0   # If the word is in S\n",
    "                    for w in range(len(S))  # For each index w in S\n",
    "                    if j+w < len(P)])       # Unless we go out of bounds\n",
    "                for j in range(len(P)) ])  # Obtain a full list for all words in the passage\n",
    "            scores.append(sw)\n",
    "\n",
    "        # Create the pstartv and pendv vectors \n",
    "        pstartv = np.zeros((1, len(context.text)))\n",
    "        pendv = np.zeros((1, len(context.text)))\n",
    "        best_scoring_answer = proposed_answers[np.argmax(scores)]\n",
    "        pstartv[0, best_scoring_answer['start']] = 1\n",
    "        pendv[0, best_scoring_answer['end']-1] = 1\n",
    "\n",
    "        yield pstartv, pendv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "folWKUq9U_gp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [15:30<00:00, 94.18it/s] \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('eval', 'sliding_predictions.txt'), 'w') as f:\n",
    "    json.dump(get_predictions(sliding_window_baseline_predict), f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "baselines.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c90f87e5ee49fb7af5481158977e62ba01ae4f54defb58032c9ba197b530ea3c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
