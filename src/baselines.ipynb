{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtwZRE_0E9Fr"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aokK8JVRVCZq"
      },
      "outputs": [],
      "source": [
        "# PRIVATE CELL\n",
        "git_token = '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'\n",
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install -U transformers benepar spacy\n",
        "    !nvidia-smi             # Check which GPU has been chosen for us\n",
        "    !rm -rf logs\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive')\n",
        "    #%cd /content/drive/MyDrive/GitHub/\n",
        "    # !git clone https://{git_token}@github.com/{username}/{repository}\n",
        "    !git clone -b enhanced_refactor https://{git_token}@github.com/{username}/{repository}\n",
        "    %cd {repository}/src\n",
        "    %ls\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r7EbGcpU_gT"
      },
      "source": [
        "# Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DAc6XGZU_gZ"
      },
      "source": [
        "In this notebook we explore two baselines for the problem of question answering. The first one is a **random baseline**, where the selection of the starting and ending token is completely random, while the second one is a more sophisticated approach called **Sliding Window** (introduced in [[1]](https://aclanthology.org/D13-1020/) and adopted in [[2]](https://arxiv.org/abs/1606.05250)), where we select a set of possible answers from the *constituents* of the original paragraph and score them based on the unigram overlap between the question and the sentence containing them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsxcgIvrE9F3"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M084ZCuyU_ga"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import spacy\n",
        "import benepar\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import utils\n",
        "\n",
        "# Download spacy corpora of text in case it's needed\n",
        "!python -m spacy download en_core_web_sm\n",
        "# Download the benepar neural constituency parser\n",
        "benepar.download('benepar_en3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz0raoxpU_gd"
      },
      "source": [
        "We load the training dataset and create a DataFrame containing:\n",
        "- The paragraph's text\n",
        "- The question's text\n",
        "- The questions's ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cFLPG6UIU_ge"
      },
      "outputs": [],
      "source": [
        "val_dataset = utils.read_question_set(os.path.join('..', 'data','validation_set.json'))\n",
        "test_dataset = utils.read_question_set(os.path.join('..', 'data','dev_set.json'))\n",
        "\n",
        "def create_questions(dataset) -> pd.DataFrame:\n",
        "    # Create a more useful data structure using list comprehensions\n",
        "    return pd.DataFrame([{\n",
        "            'context': paragraph['context'],\n",
        "            'question': qa['question'],\n",
        "            'questionID': qa['id'],\n",
        "        }   for article in dataset['data']\n",
        "            for paragraph in article['paragraphs']\n",
        "            for qa in paragraph['qas'] ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "78XyVU2YF2Td"
      },
      "outputs": [],
      "source": [
        "val_questions = create_questions(val_dataset)\n",
        "test_questions = create_questions(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "3BfXHN2fU_gf",
        "outputId": "2c9c7e57-5a7b-4651-cbf4-1aa3447c7e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION SET\n",
            "Items:  22535\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>questionID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>22530</th>\n",
              "      <td>Estimates for the percentage of the population...</td>\n",
              "      <td>What is another way studies can view bisexuality?</td>\n",
              "      <td>5710353fa58dae1900cd696e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22531</th>\n",
              "      <td>Estimates for the percentage of the population...</td>\n",
              "      <td>What is the percentage of asexuals?</td>\n",
              "      <td>5710353fa58dae1900cd696f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22532</th>\n",
              "      <td>Some historians and researchers argue that the...</td>\n",
              "      <td>What do historians and researches argue about ...</td>\n",
              "      <td>571035f5a58dae1900cd6974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22533</th>\n",
              "      <td>Some historians and researchers argue that the...</td>\n",
              "      <td>What can be assumed in english speaking nation...</td>\n",
              "      <td>571035f5a58dae1900cd6975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22534</th>\n",
              "      <td>Some historians and researchers argue that the...</td>\n",
              "      <td>What do some cultures have formal ceremonies for?</td>\n",
              "      <td>571035f5a58dae1900cd6976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 context  \\\n",
              "22530  Estimates for the percentage of the population...   \n",
              "22531  Estimates for the percentage of the population...   \n",
              "22532  Some historians and researchers argue that the...   \n",
              "22533  Some historians and researchers argue that the...   \n",
              "22534  Some historians and researchers argue that the...   \n",
              "\n",
              "                                                question  \\\n",
              "22530  What is another way studies can view bisexuality?   \n",
              "22531                What is the percentage of asexuals?   \n",
              "22532  What do historians and researches argue about ...   \n",
              "22533  What can be assumed in english speaking nation...   \n",
              "22534  What do some cultures have formal ceremonies for?   \n",
              "\n",
              "                     questionID  \n",
              "22530  5710353fa58dae1900cd696e  \n",
              "22531  5710353fa58dae1900cd696f  \n",
              "22532  571035f5a58dae1900cd6974  \n",
              "22533  571035f5a58dae1900cd6975  \n",
              "22534  571035f5a58dae1900cd6976  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEST SET\n",
            "Items:  10570\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>questionID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
              "      <td>56be4db0acb8001400a502ec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
              "      <td>56be4db0acb8001400a502ed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Where did Super Bowl 50 take place?</td>\n",
              "      <td>56be4db0acb8001400a502ee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>Which NFL team won Super Bowl 50?</td>\n",
              "      <td>56be4db0acb8001400a502ef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Super Bowl 50 was an American football game to...</td>\n",
              "      <td>What color was used to emphasize the 50th anni...</td>\n",
              "      <td>56be4db0acb8001400a502f0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             context  \\\n",
              "0  Super Bowl 50 was an American football game to...   \n",
              "1  Super Bowl 50 was an American football game to...   \n",
              "2  Super Bowl 50 was an American football game to...   \n",
              "3  Super Bowl 50 was an American football game to...   \n",
              "4  Super Bowl 50 was an American football game to...   \n",
              "\n",
              "                                            question                questionID  \n",
              "0  Which NFL team represented the AFC at Super Bo...  56be4db0acb8001400a502ec  \n",
              "1  Which NFL team represented the NFC at Super Bo...  56be4db0acb8001400a502ed  \n",
              "2                Where did Super Bowl 50 take place?  56be4db0acb8001400a502ee  \n",
              "3                  Which NFL team won Super Bowl 50?  56be4db0acb8001400a502ef  \n",
              "4  What color was used to emphasize the 50th anni...  56be4db0acb8001400a502f0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"VALIDATION SET\")\n",
        "print(\"Items: \", len(val_questions))\n",
        "display(val_questions.tail(5))\n",
        "print()\n",
        "print(\"TEST SET\")\n",
        "print(\"Items: \", len(test_questions))\n",
        "display(test_questions.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAcp--h_sAdT"
      },
      "source": [
        "We also create a function that, given a prediction generator iterates over it and produces the spans of text in the context containing the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1FygrubesG3h"
      },
      "outputs": [],
      "source": [
        "def get_predictions(questions, prediction_generator, limit=None):\n",
        "    '''\n",
        "    Given a prediction generator and the questions, iterates over all questions \n",
        "    in the dataset and produces answer predictions for them. \n",
        "    Optionally, a `limit` argument can be passed in order to reduce the \n",
        "    amount of questions to be considered.\n",
        "    '''\n",
        "    predictions = {}\n",
        "    limit = range(limit) if limit is not None else range(len(questions))\n",
        "    # Instantiate the prediction generator\n",
        "    predictor_iterator = prediction_generator(questions)\n",
        "    # Iterate over the number of questions\n",
        "    for q in tqdm(limit):\n",
        "        # Obtain start and end probabilities from the baseline function\n",
        "        pstartv, pendv = next(predictor_iterator)\n",
        "        # Obtain the indices of the best answer\n",
        "        start, end = utils.start_end_token_from_probabilities(\n",
        "            pstartv, pendv, dim=pstartv.shape[1]\n",
        "        )[0]\n",
        "        # Add the ID-answer pair in the predictions dictionary\n",
        "        id = questions['questionID'].iloc[q]\n",
        "        text = questions['context'].iloc[q]\n",
        "        # Note: in some cases, \"text\" may be something else, like a \n",
        "        # spacy span: therefore, we cast it back to string\n",
        "        predictions[id] = str(text)[start:end]\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZr1s1_SU_gj"
      },
      "source": [
        "## 1. Random prediction baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHSYKH1NU_gk"
      },
      "source": [
        "We implement a predictor that returns random start and end probabilities. Then, we use the function `start_end_token_from_probabilities` to obtain the max-scoring randomly generated span of text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M64GMTMIU_gl"
      },
      "outputs": [],
      "source": [
        "def random_baseline_predict(questions):\n",
        "    '''\n",
        "    A generator that creates random prediction vectors.\n",
        "    '''\n",
        "    for context in questions['context']:\n",
        "        pstartv = np.random.random((1, len(context)))\n",
        "        pendv = np.random.random((1, len(context)))\n",
        "        yield pstartv, pendv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MkAfNtD_U_gl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22535/22535 [05:57<00:00, 63.11it/s] \n",
            "100%|██████████| 10570/10570 [03:34<00:00, 49.37it/s]\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join('eval', 'random_predictions_val.txt'), 'w') as f:\n",
        "    json.dump(get_predictions(val_questions, random_baseline_predict), f)\n",
        "\n",
        "with open(os.path.join('eval', 'random_predictions_test.txt'), 'w') as f:\n",
        "    json.dump(get_predictions(test_questions, random_baseline_predict), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlCC0WnSU_gm"
      },
      "source": [
        "## 2. Sliding window baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhVgQIrLU_gm"
      },
      "source": [
        "The sliding window baseline is implemented in the same way it was presented in the\n",
        "[SQuAD v1 paper](https://arxiv.org/abs/1606.05250) and in the [MCTest paper](https://aclanthology.org/D13-1020.pdf) by Richardson et al.\n",
        "which originally proposed it.\n",
        "\n",
        "Apart from the paragraph and the question, the implementation also needs a set of candidate answers.\n",
        "SQuAD's paper proposes to \"*only use spans which are constituents in the constituency parse generated by\n",
        "Stanford CoreNLP*\". In our case, we use a neural parser: **Berkeley Neural Parser**, which is the option\n",
        "proposed by the `spacy` library that we are already using as a named entity extractor in the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fbmY0eUwE9GD"
      },
      "outputs": [],
      "source": [
        "# Initialize spacy's pipeline which we'll use for analysis\n",
        "spacy_pipeline = spacy.load(\"en_core_web_sm\")\n",
        "# Disable all elements but the tokenizer\n",
        "spacy_pipeline.disable_pipes(\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\")\n",
        "# Add the \"sentencizer\" component (that splits a paragraph in sentences) and the neural parser.\n",
        "spacy_pipeline.add_pipe('sentencizer')\n",
        "spacy_pipeline.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def preprocess_questions(questions):\n",
        "    # We make a fresh copy of the dataset to avoid ruining the original\n",
        "    slw_questions = questions.copy()\n",
        "\n",
        "    # We preprocess all questions and context so that the rest of the answer generation is relatively\n",
        "    # lightweight. Pandas's `apply` function is the fastest method we could find for applying the \n",
        "    # pipeline function over the large set of data.\n",
        "    def run_pipeline(text:str):\n",
        "        '''\n",
        "        Runs the tokenization + sentence extraction + neural parser pipeline\n",
        "        on a question or context. Since the neural parser only deals with at most\n",
        "        512 tokens, in the cases where the text has more tokens we trim them down\n",
        "        to 512.\n",
        "        '''\n",
        "        try: \n",
        "            doc = spacy_pipeline(text)\n",
        "        except ValueError:\n",
        "            doc = spacy_pipeline(text[:512])\n",
        "        return doc\n",
        "\n",
        "    # We actually use `progress_apply` which is `apply` + a neat tqdm progress bar, \n",
        "    # since it might take a while for preprocessing to end.\n",
        "    tqdm.pandas()\n",
        "    slw_questions['context'] = slw_questions['context'].progress_apply(run_pipeline)\n",
        "    slw_questions['question'] = slw_questions['question'].progress_apply(run_pipeline)\n",
        "    return slw_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HMfEYVtiG7ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/22535 [00:00<?, ?it/s]C:\\Users\\Volpe\\Anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\distributions\\distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n",
            "100%|██████████| 22535/22535 [4:05:08<00:00,  1.53it/s]     \n",
            "100%|██████████| 22535/22535 [1:27:33<00:00,  4.29it/s]     \n",
            "  0%|          | 0/10570 [00:00<?, ?it/s]C:\\Users\\Volpe\\Anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\distributions\\distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n",
            "100%|██████████| 10570/10570 [2:03:58<00:00,  1.42it/s]    \n",
            "100%|██████████| 10570/10570 [19:36<00:00,  8.98it/s] \n"
          ]
        }
      ],
      "source": [
        "preprocessed_val = preprocess_questions(val_questions)\n",
        "preprocessed_test = preprocess_questions(test_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMpAX3BE9GE"
      },
      "source": [
        "The implementation of the original algorithm goes as follows:\n",
        "- Given a passage $P$, its set of tokens $PW$ and the $i$-th word in the passage given by $P[i]$\n",
        "- A set of words in the question $Q$\n",
        "- A set of words in a *proposal answer* $A$\n",
        "\n",
        "For each *proposal answer*, we create $S = A \\cup Q$, then we *score* the answer using the formula:\n",
        "$$\n",
        "sw_i = \\max_{j=1\\dots |P|} \\sum_{w = 1 \\dots |S|} \n",
        "    \\begin{cases}\n",
        "        IC(P[j+w])     & \\text{if } P[j+w] \\in S \\\\\n",
        "        0              & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "C(w) = \\sum_i(one\\_hot(P[i] = w))\n",
        "$$\n",
        "$$\n",
        "IC(w) = \\log(1+\\frac{1}{C(w)})\n",
        "$$\n",
        "\n",
        "Basically: \n",
        "- The score for a proposal answer is computed by sliding a window over all words in the paragraph.\n",
        "    - Index $j$ represents the start of the window, while the window width is given by the cardinality of the \"question $\\cup$ answer\" set.\n",
        "- Given a window, we compute its score by iterating over all of its words:\n",
        "    - If a word is inside the \"question $\\cup$ answer\" set, we assign it a number ($0 < n < 1$) that gets smaller if there are many instances of that word into the paragraph.\n",
        "        - It's a similar idea to *Inverse Document Frequency*: a word that appears many times in an answer isn't probably very discriminative.\n",
        "    - We sum the scores for the words in that window.\n",
        "    - We move index $j$ of 1, selecting the next window, and repeat until we have gone through all the words in the paragraph.\n",
        "- At the end, the *maximum* of window scores is taken as global score of the answer.\n",
        "\n",
        "The SQuAD paper makes some minor modifications:\n",
        "- The proposal answers are generated by taking the *constituents* in the paragraph, ignoring punctuation and articles.\n",
        "- For each candidate answer, they computed \"the unigram/bigram overlap between the sentence containing it (excluding the candidate itself) and the question\". \n",
        "- Only candidates that have the maximal overlap are kept and presented to the sliding window algorithm.\n",
        "- Instead of using the entire passage $P$ as context for an answer, only the sentence containing the candidate answer is used (for efficiency).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iCwcIYJeE9GE"
      },
      "outputs": [],
      "source": [
        "def C(w, P):\n",
        "    # Sum of all words in the sentence where the word is equal to w.\n",
        "    return sum([1 for p in P if w == p])\n",
        "\n",
        "def IC(w, P):\n",
        "    # A scoring function based on inverse word frequency of word w in sentence P\n",
        "    return math.log(1 + (1/C(w, P)))\n",
        "\n",
        "# Candidate answers to keep after unigram overlap computation\n",
        "K = 10\n",
        "\n",
        "def sliding_window_baseline_predict(questions):\n",
        "    for i in range(len(questions)):\n",
        "        # We collect the preprocessed question and context\n",
        "        question = questions['question'].iloc[i]\n",
        "        context = questions['context'].iloc[i]\n",
        "\n",
        "        # Step 1: Create a set of words present in the question (ignoring punctuation)\n",
        "        Q = set([str(token) for token in question if not token.is_punct])\n",
        "\n",
        "        # We define a function to ignore punctuation tokens and articles\n",
        "        def ignored(tok):\n",
        "            return tok.is_punct or str(tok) in {'a', 'an', 'the'}\n",
        "\n",
        "        # Step 2: From the processed text we can obtain the list of constituents,\n",
        "        # which will be the proposed answers to each question.\n",
        "        proposed_answers = [ {\n",
        "                'answer': answer,               # Each proposed answer is a dictionary containing the answer text\n",
        "                'start': answer.start_char,     # The start and end within the paragraph\n",
        "                'end': answer.end_char,\n",
        "                'sentence': sentence,           # The sentence which contains it\n",
        "                'start_in_sentence': answer.start_char - sentence.start_char,  # The start and end within the sentence\n",
        "                'end_in_sentence': answer.end_char - sentence.start_char,\n",
        "                'token_set': set(str(tok) for tok in answer if not tok.is_punct)    # The set of tokens (excluding punctuation)\n",
        "            }\n",
        "            for sentence in list(context.sents)             # Iterate over sentences\n",
        "            for answer in list(sentence._.constituents)     # Iterate over constituents\n",
        "            if not len(set(tok for tok in answer if not ignored(tok))) == 0                    \n",
        "        ]\n",
        "        \n",
        "        # Step 3: Select a subset of the proposed answer based on unigram overlap\n",
        "        # with the rest of the sentence\n",
        "        spans_before = [ { str(tok) for tok in \n",
        "                    p['sentence'][0:p['start_in_sentence']] }\n",
        "                    for p in proposed_answers ]\n",
        "        spans_after = [ { str(tok) for tok in \n",
        "                    p['sentence'][p['end_in_sentence']:] }\n",
        "                    for p in proposed_answers]\n",
        "        # Compute unigram overlap between before/after spans and question\n",
        "        # Differently from SQuAD we only use unigram overlaps\n",
        "        uni_overlaps = [ len(Q.intersection(spans_before[i])) + \n",
        "                         len(Q.intersection(spans_after[i]))\n",
        "                         for i in range(len(proposed_answers)) ]\n",
        "        \n",
        "        # Keep the k=10 best scoring candidates\n",
        "        proposed_answers = sorted(proposed_answers, \n",
        "            key=lambda x: -uni_overlaps[proposed_answers.index(x)])[:K]\n",
        "        \n",
        "        # Step 4: Now that we have the question's text, the proposed answers and the context,\n",
        "        # we can apply the sliding window algorithm, which computes a score based on the n-gram\n",
        "        # overlap between the question's words and the proposed spans of text.\n",
        "        scores = []\n",
        "        for i in range(len(proposed_answers)):     # Iterate over all remaining possible answers (may be less than K)\n",
        "            S = {str(s) for s in proposed_answers[i]['token_set'].union(Q)}  # Unite the question and the answer words\n",
        "            # SQuAD uses only the sentence containing the answer for context, so we\n",
        "            # create a list of tokens of the sentence excluding punctuation\n",
        "            P = [str(t) for t in proposed_answers[i]['sentence'] if not t.is_punct]\n",
        "            # Create a LUT of word scores for efficiency \n",
        "            adder = {\n",
        "                p: IC(p, P)\n",
        "                for p in P\n",
        "            }\n",
        "            sw = max([                      # Select the maximum score from the...\n",
        "                    sum([                   # ...sums over...\n",
        "                    adder[P[j+w]]           # ...the scores of the words in the window...\n",
        "                    if P[j+w] in S else 0   # ...if the word P[j+w] is in S.\n",
        "                    for w in range(len(S))  # The window's length is |S|...\n",
        "                    if j+w < len(P)])       # ...but we may go out of bounds!\n",
        "                for j in range(len(P)) ])   # The maximum considers all possible windows in P\n",
        "            scores.append(sw)\n",
        "\n",
        "        # Obtain the best answer\n",
        "        best_scoring_answer = proposed_answers[np.argmax(scores)]\n",
        "        # Create the pstartv and pendv vectors. They will be vectors of 0s,\n",
        "        # except for the starting and ending token of the best scoring answer, \n",
        "        # which will be 1s.\n",
        "        pstartv = np.zeros((1, len(context.text)+1))\n",
        "        pendv = np.zeros((1, len(context.text)+1))\n",
        "        pstartv[0, best_scoring_answer['start']] = 1\n",
        "        pendv[0, best_scoring_answer['end']] = 1\n",
        "\n",
        "        yield pstartv, pendv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GKr1WVt8HUC-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 22535/22535 [15:24<00:00, 24.37it/s] \n",
            "100%|██████████| 10570/10570 [08:24<00:00, 20.97it/s]\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join('eval', 'sliding_predictions_val.txt'), 'w') as f:\n",
        "    json.dump(get_predictions(preprocessed_val, sliding_window_baseline_predict), f)\n",
        "\n",
        "with open(os.path.join('eval', 'sliding_predictions_test.txt'), 'w') as f:\n",
        "    json.dump(get_predictions(preprocessed_test, sliding_window_baseline_predict), f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baselines.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c90f87e5ee49fb7af5481158977e62ba01ae4f54defb58032c9ba197b530ea3c"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
