{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpIoIhpnPIhplB3PQ0Kj0B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XgjiKCtAO7v"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "iEPJtOrPAw4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Imports & paths definitions"
      ],
      "metadata": {
        "id": "19ibRNmwBCaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from config import Config\n",
        "config = Config()\n",
        "import utils\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "random.seed(config.RANDOM_SEED)\n",
        "tf.random.set_seed(config.RANDOM_SEED)\n",
        "\n",
        "from typing import List, Dict\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
        "\n",
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')\n",
        "BEST_WEIGHTS_PATH = \"./checkpoints/normal.h5\" if not IN_COLAB else \\\n",
        "    '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/normal_100_tpu_h5_cval/normal.h5'\n",
        "\n",
        "if IN_COLAB:\n",
        "    checkpoint_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "    datasets_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
        "else:\n",
        "    checkpoint_dir = os.path.join(\"checkpoints\", \"training_dpr\")\n",
        "    datasets_dir = os.path.join(\"checkpoints\", \"training_dpr\", \"dataset\")\n",
        "\n",
        "representations_dir = os.path.join(datasets_dir, 'representations')\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(datasets_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Impbrs6HAhGu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing paragraphs and questions"
      ],
      "metadata": {
        "id": "7iwYAhVoBEWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]"
      ],
      "metadata": {
        "id": "o9mjH7IrA4W7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)"
      ],
      "metadata": {
        "id": "rqhQ8QKCA-QJ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing questions and paragraphs representations (according to DPR)"
      ],
      "metadata": {
        "id": "XnMyq2r7LOkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paragraphs_encodings = np.load(os.path.join(representations_dir, 'train_paragraphs_encodings.npy'))\n",
        "val_paragraphs_encodings   = np.load(os.path.join(representations_dir, 'val_paragraphs_encodings.npy'))\n",
        "test_paragraphs_encodings  = np.load(os.path.join(representations_dir, 'test_paragraphs_encodings.npy'))\n",
        "\n",
        "train_questions_encodings  = np.load(os.path.join(representations_dir, 'train_questions_encodings.npy'))\n",
        "val_questions_encodings    = np.load(os.path.join(representations_dir, 'val_questions_encodings.npy'))\n",
        "test_questions_encodings   = np.load(os.path.join(representations_dir, 'test_questions_encodings.npy'))"
      ],
      "metadata": {
        "id": "RMdVEBaGLNhw"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing vectorizers"
      ],
      "metadata": {
        "id": "IOKpJFvTBV-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "val_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "test_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')"
      ],
      "metadata": {
        "id": "KAopuYerBXn5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the vectorizers and simultaneously create representations of the paragraphs."
      ],
      "metadata": {
        "id": "6xTltnnTBfHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs = train_vectorizer.fit_transform([train_paragraphs[i]['context'] for i in range(len(train_paragraphs))])\n",
        "val_docs = val_vectorizer.fit_transform([val_paragraphs[i]['context'] for i in range(len(val_paragraphs))])\n",
        "test_docs = test_vectorizer.fit_transform([test_paragraphs[i]['context'] for i in range(len(test_paragraphs))])"
      ],
      "metadata": {
        "id": "icBjREWBBd6e"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use the train vectorizer to create representations of the test paragraphs"
      ],
      "metadata": {
        "id": "EzFVDv3FPtwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_docs_with_train_vectorizer = train_vectorizer.transform([test_paragraphs[i]['context'] \n",
        "                                                              for i in range(len(test_paragraphs))])"
      ],
      "metadata": {
        "id": "6HEfQHCeP1H1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "deS7yFHeByBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some functions that can be used to facilitate the scoring of paragraphs with respect to a query question."
      ],
      "metadata": {
        "id": "Noygxt4JxnSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_documents(vectorizer, query, docs):\n",
        "    '''\n",
        "    Obtain the TfIdf scores between the question and the matrix of paragraphs.\n",
        "    '''\n",
        "    q = query['qas']['question']\n",
        "    q = vectorizer.transform([q]) # q will be a (sparse) matrix with dimensionality 1 x vocab_dim\n",
        "    # We can compute a vector of all dot products scores and transform it from dense matrix to numpy array like this:\n",
        "    return np.asarray(np.dot(docs, q.T).todense()).flatten()\n",
        "\n",
        "def top_n_for_question(paragraphs, vectorizer, query, docs, n=5):\n",
        "    '''\n",
        "    Obtain the most relevant paragraph for the presented query question according to the vectorizer.\n",
        "    '''\n",
        "    scores = score_documents(vectorizer, query, docs)\n",
        "    sorted_scores = np.argsort(-scores) # Negated scores for descending order\n",
        "    return [paragraphs[i] for i in sorted_scores[:n]], scores[sorted_scores[:n]], sorted_scores[:n]\n",
        "\n",
        "def get_paragraph_encoding_index(question, dataset):\n",
        "    '''\n",
        "    Obtain the index of paragraph the question refers to inside a specific dataset.\n",
        "    '''\n",
        "    art_id, par_id = question['context_id']\n",
        "    idx = sum([len(dataset[i]['paragraphs']) for i in range(art_id)]) + par_id\n",
        "    return idx"
      ],
      "metadata": {
        "id": "_JrfugDDBpQH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset creation"
      ],
      "metadata": {
        "id": "PzWj3U6cCnQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create a dataset to link each question to the **predicted** best paragraph.\n",
        "\n",
        "First we create a generator that yields encoded pairs of the form (question - best predicted paragraph)."
      ],
      "metadata": {
        "id": "vySRK-wZCxMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predicted_paragraphs_dataset_generator(questions: List[Dict], predicted_paragraphs: List, \n",
        "                                            config: Config, return_question_id:bool=False):\n",
        "    # Iterate over questions\n",
        "    for i, q in enumerate(questions):\n",
        "        # We use the paragraph obtained by the vectorizer to compute the best scoring paragraph for the question\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        # Then encode the input as usual using Bert's tokenizer\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question text\n",
        "            paragraph['context'],               # Then the best scoring paragraph text\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to the max length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "            return_token_type_ids = config.bert,# Return if the token is from sentence 0 or sentence 1\n",
        "            return_attention_mask = True,       # Return if it's a pad token or not\n",
        "        )\n",
        "        if return_question_id:\n",
        "            yield dict(encoded_inputs), q['qas']['id']\n",
        "        else:\n",
        "            yield dict(encoded_inputs)"
      ],
      "metadata": {
        "id": "fjDDB_STyFOt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we generate the \"original\" dataset containing only the text of the predicted paragraph and the offset mappings of its tokens, which is useful to retrieve the answer to the question."
      ],
      "metadata": {
        "id": "HZtDafLEyGwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_original_dataset_with_tf_idf(questions: List[Dict], \n",
        "                                        predicted_paragraphs: List,\n",
        "                                        config: Config):\n",
        "    features = []\n",
        "    for i, q in enumerate(questions):\n",
        "        inputs={}\n",
        "        # The paragraph is collected from those that were pre-predicted\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question\n",
        "            paragraph[\"context\"],               # Then the context\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to this length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "            return_token_type_ids = False,      # Return if the token is from sentence  0 or sentence 1\n",
        "            return_attention_mask = False,      # Return if it's a pad token or not\n",
        "            return_offsets_mapping = True       # Returns each token's first and last char positions in the original sentence\n",
        "        )\n",
        "        # We fill the inputs dictionary\n",
        "        inputs[\"context\"] = paragraph[\"context\"]\n",
        "        inputs[\"offset_mapping\"] = encoded_inputs[\"offset_mapping\"]\n",
        "        features.append(inputs)\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        pd.DataFrame.from_dict(features).to_dict(orient=\"list\"))"
      ],
      "metadata": {
        "id": "oMg15LSGySu1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create the actual dataset using the generator we defined above."
      ],
      "metadata": {
        "id": "UJg3kbUsyaPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_using_tf_idf_vectorizer( questions: List[Dict],\n",
        "                                            predicted_paragraphs: List,\n",
        "                                            config: Config  ) -> tf.data.Dataset:\n",
        "    # Create expected signature for the generator output\n",
        "    if config.bert:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
        "            'token_type_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    else:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    # The dataset contains the features and the question IDs (strings)\n",
        "    signature = (features, tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    # Instantiates a partial generator\n",
        "    data_gen = partial(predicted_paragraphs_dataset_generator, \n",
        "        questions, predicted_paragraphs, config, return_question_id=True)\n",
        "    # Creates the dataset with the computed signature\n",
        "    dataset = tf.data.Dataset.from_generator(data_gen,\n",
        "        output_signature=signature)\n",
        "    # Compute dataset length, to be used by tensorflow internals\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    # Return the dataset\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "-T6kPu5JCiXO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare some utility classes, objects and functions for handling the score mixing prior to the paragraph selection."
      ],
      "metadata": {
        "id": "oAJx8VYhXALa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum, auto\n",
        "\n",
        "class MixingType(Enum):\n",
        "    TF_IDF_ONLY = auto()\n",
        "    DPR_ONLY = auto()\n",
        "    SUM = auto()\n",
        "    MAX = auto()\n",
        "    WEIGHTED_SUM = auto()\n",
        "\n",
        "class DatasetType(Enum):\n",
        "    TRAIN = auto()\n",
        "    VAL = auto()\n",
        "    TEST = auto()\n",
        "    TEST_WITH_TRAIN_VECT = auto()\n",
        "\n",
        "datasets_info = {\n",
        "    DatasetType.TRAIN: {\n",
        "        'questions': train_questions,\n",
        "        'question_encodings': train_questions_encodings,\n",
        "        'paragraphs': train_paragraphs,\n",
        "        'paragraph_encodings': train_paragraphs_encodings,\n",
        "        'vectorizer': train_vectorizer,\n",
        "        'docs_vectorized': train_docs,\n",
        "        'dataset_path': TRAINING_FILE\n",
        "    },\n",
        "    DatasetType.VAL: {\n",
        "        'questions': val_questions,\n",
        "        'question_encodings': val_questions_encodings,\n",
        "        'paragraphs': val_paragraphs,\n",
        "        'paragraph_encodings': val_paragraphs_encodings,\n",
        "        'vectorizer': val_vectorizer,\n",
        "        'docs_vectorized': val_docs,\n",
        "        'dataset_path': VALIDATION_FILE\n",
        "    },\n",
        "    DatasetType.TEST: {\n",
        "        'questions': test_questions,\n",
        "        'question_encodings': test_questions_encodings,\n",
        "        'paragraphs': test_paragraphs,\n",
        "        'paragraph_encodings': test_paragraphs_encodings,\n",
        "        'vectorizer': test_vectorizer,\n",
        "        'docs_vectorized': test_docs,\n",
        "        'dataset_path': TEST_FILE\n",
        "    },\n",
        "    DatasetType.TEST_WITH_TRAIN_VECT: {\n",
        "        'questions': test_questions,\n",
        "        'question_encodings': test_questions_encodings,\n",
        "        'paragraphs': test_paragraphs,\n",
        "        'paragraph_encodings': test_paragraphs_encodings,\n",
        "        'vectorizer': train_vectorizer,\n",
        "        'docs_vectorized': test_docs_with_train_vectorizer,\n",
        "        'dataset_path': TEST_FILE\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "eXUrwLUGMLoT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions will handle all of the scores mixing options defined previously."
      ],
      "metadata": {
        "id": "vzcdH4Q0za5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_sum_func(dpr_scores, tf_idf_scores, op='normalize', w_dpr=0, w_tfidf=0):\n",
        "    '''\n",
        "    A general functions for handling all weighted-sum-based mixing operations between scores.\n",
        "    '''\n",
        "    if op == 'normalize':\n",
        "        dpr_scores = dpr_scores/np.max(dpr_scores)\n",
        "        tf_idf_scores = tf_idf_scores/np.max(tf_idf_scores)\n",
        "    elif op == 'standardize':\n",
        "        dpr_scores = (dpr_scores-np.mean(dpr_scores))/np.std(dpr_scores)\n",
        "        tf_idf_scores = (tf_idf_scores-np.mean(tf_idf_scores))/np.std(tf_idf_scores)\n",
        "    return np.argsort(dpr_scores*w_dpr + tf_idf_scores*w_tfidf)\n",
        "\n",
        "def max_func(dpr_scores, tf_idf_scores):\n",
        "    '''\n",
        "    A general functions for handling all max-based mixing operations between scores.\n",
        "    '''\n",
        "    dpr_scores = dpr_scores/np.max(dpr_scores)\n",
        "    tf_idf_scores = tf_idf_scores/np.max(tf_idf_scores)\n",
        "    return np.argsort([max(d, t) for d, t in zip(dpr_scores, tf_idf_scores)])\n",
        "\n",
        "def get_best_paragraph_for_question_using_mix_type(dataset_info, i, mix_type:MixingType, h=0.15):    \n",
        "    '''\n",
        "    A general function that is able to handle all mixing types transparently.\n",
        "    It computes both DPR and TfIdf scores and sets up the parameters for mixing them\n",
        "    according to the `mix_type` argument.\n",
        "    `h` is the weighted sum hyperparameter which controls the weight of the TfIdf scores\n",
        "    with respect to the DPR scores.\n",
        "    '''\n",
        "    # Compute DPR scores\n",
        "    sample_q_repr = dataset_info['question_encodings'][i]\n",
        "    dpr_scores = np.dot(sample_q_repr, dataset_info['paragraph_encodings'].T)\n",
        "    # Compute TfIdf score\n",
        "    question_text = dataset_info['questions'][i]['qas']['question']\n",
        "    vect_question = dataset_info['vectorizer'].transform([question_text])\n",
        "    tfidf_scores = np.asarray(np.dot(dataset_info['docs_vectorized'], vect_question.T).todense()).flatten()\n",
        "    # Handle types\n",
        "    if mix_type is not MixingType.MAX:\n",
        "        op = 'normalize' if mix_type is not MixingType.WEIGHTED_SUM else 'standardize'\n",
        "        if mix_type is MixingType.DPR_ONLY:\n",
        "            w_dpr, w_tfidf = 1, 0\n",
        "        elif mix_type is MixingType.SUM:\n",
        "            w_dpr, w_tfidf = 1, 1\n",
        "        elif mix_type is MixingType.TF_IDF_ONLY:\n",
        "            w_dpr, w_tfidf = 0, 1\n",
        "        elif mix_type is MixingType.WEIGHTED_SUM:\n",
        "            w_dpr, w_tfidf = 1-h, h\n",
        "        return weighted_sum_func(dpr_scores, tfidf_scores, op, w_dpr, w_tfidf)[-1]\n",
        "    else:\n",
        "        return max_func(dpr_scores, tfidf_scores)[-1]"
      ],
      "metadata": {
        "id": "3Wj5x6uPL4S8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define the prediction function and start the evaluations:"
      ],
      "metadata": {
        "id": "T8VZW6ftXD8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_predictions(best_weights_path:str, \n",
        "                         path_to_predictions_json:str,\n",
        "                         config:Config,\n",
        "                         mixing_type:MixingType=MixingType.WEIGHTED_SUM,\n",
        "                         weighted_sum_h:float=0.15,\n",
        "                         dataset_type:DatasetType=DatasetType.TEST,\n",
        "                         hidden_state_list:List[int]=[3,4,5,6],\n",
        "                         bert=False):\n",
        "\n",
        "    # Deal with dataset type\n",
        "    print(\"Collecting the requested dataset and vectorizer...\")\n",
        "    if dataset_type in datasets_info:\n",
        "        dataset = datasets_info[dataset_type]\n",
        "    else:\n",
        "        raise NotImplementedError(\"That dataset type does not exist. \"\n",
        "            \"Change the dataset_type argument into one in the class DatasetType\")\n",
        "        \n",
        "    # We pre-compute the predicted paragraph for each question in the set.\n",
        "    print(\"Obtaining best paragraph for questions...\")\n",
        "    predicted_paragraphs = [dataset['paragraphs'][get_best_paragraph_for_question_using_mix_type(\n",
        "                                dataset, i, mixing_type, weighted_sum_h)]\n",
        "                            for i in tqdm(range(len(dataset['questions'])))]\n",
        "\n",
        "    print(\"Creating model and dataset...\")\n",
        "    config = Config(bert=bert)\n",
        "    # Process questions\n",
        "    tf_dataset = create_dataset_using_tf_idf_vectorizer(dataset['questions'], predicted_paragraphs, config)\n",
        "    print(\"Number of samples: \", len(tf_dataset))\n",
        "    tf_dataset = tf_dataset.batch(config.BATCH_SIZE)\n",
        "\n",
        "    # Generate the original dataset that contains the original context and token-char mapping\n",
        "    original_dataset = create_original_dataset_with_tf_idf(dataset['questions'], predicted_paragraphs, config)\n",
        "    original_dataset = original_dataset.batch(config.BATCH_SIZE)\n",
        "\n",
        "    # Load model with the best obtained weights from the old project\n",
        "    model = config.create_standard_model(hidden_state_list=hidden_state_list)\n",
        "    model.load_weights(best_weights_path)\n",
        "\n",
        "    # Predict the answers to the questions in the dataset\n",
        "    print(\"Computing predictions...\")\n",
        "    predictions = utils.compute_predictions(tf_dataset, original_dataset, model)\n",
        "    print(f\"Done! Saving predictions at {path_to_predictions_json} and running evaluation script...\")\n",
        "\n",
        "    # Create a prediction file formatted like the one that is expected\n",
        "    with open(path_to_predictions_json, 'w') as f:\n",
        "        json.dump(predictions, f)"
      ],
      "metadata": {
        "id": "zlCXjyaaDgnv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "sZ8yW3sxYpfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the path of the test dataset since almost all evaluations will use it."
      ],
      "metadata": {
        "id": "7kIUbJeOzsuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_DATASET_PATH = datasets_info[DatasetType.TEST]['dataset_path']"
      ],
      "metadata": {
        "id": "eyxrsDbuwAJz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert"
      ],
      "metadata": {
        "id": "ctJZ4Rsg_bos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-Idf-only evaluation"
      ],
      "metadata": {
        "id": "tGmLvUUzYujt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_only_test_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.TF_IDF_ONLY)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JJYY65pXllA",
        "outputId": "28675429-4eca-475f-a73c-f04d25c226c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:38<00:00, 275.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:28<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/tf_idf_only_test_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 38.666035950804165,\n",
            "  \"f1\": 49.26541133366401,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 38.666035950804165,\n",
            "  \"HasAns_f1\": 49.26541133366401,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPR-only evaluation"
      ],
      "metadata": {
        "id": "sVVHqPQuY1U4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/dpr_only_test_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.DPR_ONLY)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "5GeebFozYXMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf13976-bd57-4aed-d6d8-0a3eb0302cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:32<00:00, 322.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:29<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/dpr_only_test_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 30.57710501419111,\n",
            "  \"f1\": 40.149694209963585,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 30.57710501419111,\n",
            "  \"HasAns_f1\": 40.149694209963585,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "ZP2mdDB7Y3vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/sum_dpr_tf_idf_test_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.SUM)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "hfFgngjMY7Nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f90e6cb-67ed-4eb4-cd63-bc2e3ca4c1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:30<00:00, 351.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:31<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/sum_dpr_tf_idf_test_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 41.79754020813623,\n",
            "  \"f1\": 53.031748094147424,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 41.79754020813623,\n",
            "  \"HasAns_f1\": 53.031748094147424,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "IpozD9P7Y7uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/max_dpr_tf_idf_test_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.MAX)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "eYKGKW0LY9Xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca73f4ca-d60b-423a-9eef-e0798b6361c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:53<00:00, 198.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:30<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/max_dpr_tf_idf_test_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 35.00473036896878,\n",
            "  \"f1\": 45.18324802125428,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 35.00473036896878,\n",
            "  \"HasAns_f1\": 45.18324802125428,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "5o0krQQtY92J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/weighted_sum_dpr_tf_idf_test_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.WEIGHTED_SUM)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "di2fHpMCY_Cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b9971a-8c03-4068-a336-576f68892403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:34<00:00, 303.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:32<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/weighted_sum_dpr_tf_idf_test_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 47.19016083254494,\n",
            "  \"f1\": 59.49892924719032,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 47.19016083254494,\n",
            "  \"HasAns_f1\": 59.49892924719032,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert with paragraph encodings created from train vectorizer"
      ],
      "metadata": {
        "id": "bGpaa1vfBAQ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-Idf-only evaluation"
      ],
      "metadata": {
        "id": "7IDIjNaVBgMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_only_test_with_train_vect_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.TF_IDF_ONLY, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXcjU-p4BNIi",
        "outputId": "90ecc40a-3f41-40e6-c987-1096520c4f34"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:39<00:00, 268.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:30<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/tf_idf_only_test_with_train_vect_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 32.26111636707663,\n",
            "  \"f1\": 41.418149073848724,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 32.26111636707663,\n",
            "  \"HasAns_f1\": 41.418149073848724,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "kSiQnc50Bn7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/sum_dpr_tf_idf_test_with_train_vect_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.SUM, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e159694f-2d08-47f1-8985-d995ab0e4c4a",
        "id": "AsCU0NqoBn7a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:38<00:00, 276.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:32<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/sum_dpr_tf_idf_test_with_train_vect_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 36.08325449385052,\n",
            "  \"f1\": 46.04216513774338,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 36.08325449385052,\n",
            "  \"HasAns_f1\": 46.04216513774338,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "yRxP_4lCBn7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/max_dpr_tf_idf_test_with_train_vect_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.MAX, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f968e5-474a-43df-976b-c202e01e62b2",
        "id": "eBa8NcgTBn7c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:59<00:00, 176.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:29<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/max_dpr_tf_idf_test_with_train_vect_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 31.371807000946074,\n",
            "  \"f1\": 40.81043227113104,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 31.371807000946074,\n",
            "  \"HasAns_f1\": 40.81043227113104,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "_JkLGfk-Bn7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/weighted_sum_dpr_tf_idf_test_with_train_vect_predictions.json'\n",
        "compute_predictions(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.WEIGHTED_SUM, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba383be-86be-4b29-b7f5-48ff9b6a45a2",
        "id": "8VM5QrPhBn7f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:40<00:00, 261.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:29<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/weighted_sum_dpr_tf_idf_test_with_train_vect_predictions.json and running evaluation script...\n",
            "------------------------------------------------------------------------------------\n",
            "Scores:\n",
            "{\n",
            "  \"exact\": 44.03973509933775,\n",
            "  \"f1\": 55.732409824423996,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 44.03973509933775,\n",
            "  \"HasAns_f1\": 55.732409824423996,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert"
      ],
      "metadata": {
        "id": "mdaw4kLM_tnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_BEST_WEIGHTS_PATH = \"/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/normal_BERT_100_tpu_h5_cval/training_BERT_tpu_last.h5\""
      ],
      "metadata": {
        "id": "_2iP3ZqcAvjs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-Idf-only evaluation"
      ],
      "metadata": {
        "id": "ZSdMwIXF_xSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_only_test_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.TF_IDF_ONLY, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "nMa7kKth_xSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DPR-only evaluation"
      ],
      "metadata": {
        "id": "T0nHNKRB_xSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/dpr_only_test_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.DPR_ONLY, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "ixeoPh08_xSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "9N1eItAg_xSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/sum_dpr_tf_idf_test_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.SUM, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "omds8Pus_xSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "J45p57tO_xSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/max_dpr_tf_idf_test_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.MAX, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "moPtcgHc_xSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "ZUo_aedr_xSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/weighted_sum_dpr_tf_idf_test_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.WEIGHTED_SUM, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "LE9rdBIS_xSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert with paragraph encodings created from train vectorizer"
      ],
      "metadata": {
        "id": "OY-Fl61IB6jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tf-Idf-only evaluation"
      ],
      "metadata": {
        "id": "KsHM0NCyB6jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_only_test_with_train_vect_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.TF_IDF_ONLY, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "TYbmGQa6B6jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "lwskzy9OB6jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/sum_dpr_tf_idf_test_with_train_vect_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.SUM, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "lhPhT0vlB6jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Max between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "qKyApKpVB6jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/max_dpr_tf_idf_test_with_train_vect_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.MAX, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "stb11R5WB6jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted sum between DPR and Tf-Idf scores evaluation"
      ],
      "metadata": {
        "id": "PcCc-a5pB6jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_PREDICTIONS_JSON = '../data/results/weighted_sum_dpr_tf_idf_test_with_train_vect_bert_predictions.json'\n",
        "compute_predictions(BERT_BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, mixing_type=MixingType.WEIGHTED_SUM, \n",
        "                    dataset_type=DatasetType.TEST_WITH_TRAIN_VECT, bert=True)\n",
        "print(\"------------------------------------------------------------------------------------\")\n",
        "print(\"Scores:\")\n",
        "!python eval/evaluate.py $TEST_DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ],
      "metadata": {
        "id": "8D9xzCCgB6jU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}