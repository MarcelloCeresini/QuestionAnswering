{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_O8H6y5b5yZT"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g84UMFvU5yZW"
      },
      "source": [
        "# Tf-Idf retrieval baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GLWZShI5yZY"
      },
      "source": [
        "In this notebook, we implement a simple baseline for paragraph retrieval using Tf-Idf weighted sparse representations of documents and query questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wSMzmIlZ5yZZ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from config import Config\n",
        "config = Config()\n",
        "import utils\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "random.seed(config.RANDOM_SEED)\n",
        "tf.random.set_seed(config.RANDOM_SEED)\n",
        "\n",
        "from typing import List, Dict\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_thUdCdU5yZZ"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')\n",
        "\n",
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuChVKjW5yZa"
      },
      "source": [
        "First of all, we separate questions from their paragraphs. We assign a `context_id` to each question and paragraph:\n",
        "\n",
        "- For questions, the context ID is a tuple `(paragraph_id, question_id_in_paragraph)`\n",
        "- For paragraphs, the context ID is a simple index corresponding to the first value in the tuple of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Kzmnxv5yZa",
        "outputId": "a6ef863c-12c1-4c32-a06d-4bf1f851d7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training questions: 65064\n",
            "Number of training paragraphs: 13975\n",
            "\n",
            "Number of val questions: 22535\n",
            "Number of val paragraphs: 4921\n",
            "\n",
            "Number of test questions: 10570\n",
            "Number of test paragraphs: 2067\n"
          ]
        }
      ],
      "source": [
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)\n",
        "\n",
        "print(f\"Number of training questions: {len(train_questions)}\")\n",
        "print(f\"Number of training paragraphs: {len(train_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of val questions: {len(val_questions)}\")\n",
        "print(f\"Number of val paragraphs: {len(val_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of test questions: {len(test_questions)}\")\n",
        "print(f\"Number of test paragraphs: {len(test_paragraphs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFoe0wTg5yZc"
      },
      "source": [
        "We build a function to obtain paragraphs given the `context_id`s we built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GrYOzf4X5yZc"
      },
      "outputs": [],
      "source": [
        "def get_paragraph_from_question(qas, dataset):\n",
        "    i,j = qas['context_id']\n",
        "    return dataset[i]['paragraphs'][j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try it on a random question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNTJ9JHH5yZd",
        "outputId": "cf89fe8c-b7e0-4b54-c1a3-d7f0f5a9c1da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Which 1909 ballet used Chopin's music?\n",
            "\n",
            "Ground truth context: 'Chopin's music was used in the 1909 ballet Chopiniana, choreographed by Michel Fokine and orchestrated by Alexander Glazunov. Sergei Diaghilev commissioned additional orchestrations—from Stravinsky, Anatoly Lyadov, Sergei Taneyev and Nikolai Tcherepnin—for later productions, which used the title Les Sylphides.'\n"
          ]
        }
      ],
      "source": [
        "x = random.randint(0, len(train_questions)-1)\n",
        "print(f\"Question: {train_questions[x]['qas']['question']}\")\n",
        "print()\n",
        "print(f\"Ground truth context: '{get_paragraph_from_question(train_questions[x], train_paragraphs_and_questions)['context']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nomrUS_g5yZe"
      },
      "source": [
        "Now, given a question (query) we would like to obtain the paragraph that most probably contains the answer. Once we have a paragraph, we pass it into the BERT QA model we created for the standard project to obtain an answer. \n",
        "\n",
        "One way to do that is by using Tf-Idf on the large set of paragraphs. In reality we will use more complex methods and this should be considered a baseline. We will use a `TdIdfVectorizer` from Scikit Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "46T1i7J95yZf"
      },
      "outputs": [],
      "source": [
        "train_vectorizer = TfidfVectorizer(strip_accents='unicode', \n",
        "    lowercase=True, \n",
        "    max_df=0.8,     # Filter out common words that appear in more than 80% of the paragraphs\n",
        "    norm='l2')      # The vectorizer also l2 normalizes the vectors it produces, \n",
        "                    # so that the cosine similarity operation between vectors simply becomes a dot product.\n",
        "\n",
        "val_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "test_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "\n",
        "train_docs = train_vectorizer.fit_transform([train_paragraphs[i]['context'] for i in range(len(train_paragraphs))])\n",
        "val_docs = val_vectorizer.fit_transform([val_paragraphs[i]['context'] for i in range(len(val_paragraphs))])\n",
        "test_docs = test_vectorizer.fit_transform([test_paragraphs[i]['context'] for i in range(len(test_paragraphs))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBbhOcrr5yZg",
        "outputId": "63b7f2a0-cdc9-425c-fecc-16b795bbd1ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((13975, 65808), (4921, 36612), (2067, 22934))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_docs.shape, val_docs.shape, test_docs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t7Prb_F5yZg"
      },
      "source": [
        "Now, in order to compute scores between a query question and the set of document, we use **cosine similarity**.\n",
        "\n",
        "$$\n",
        "S_C (A,B) = \\frac{A \\cdot B}{\\lVert A\\rVert  \\lVert B\\rVert }\n",
        "$$\n",
        "\n",
        "Note: the `TfIdfVectorizer` we use already L2-normalizes all vectors it produces, so in the actual implementation we only compute a dot product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlbBSHE75yZh",
        "outputId": "f5576c5d-efc3-415f-e22e-4662bf8a3d75"
      },
      "outputs": [],
      "source": [
        "def score_documents(vectorizer, query, docs):\n",
        "    q = query['qas']['question']\n",
        "    q = vectorizer.transform([q]) # q will be a (sparse) matrix with dimensionality 1 x vocab_dim\n",
        "    # We can compute a vector of all dot products scores and transform it from dense matrix to numpy array like this:\n",
        "    return np.asarray(np.dot(docs, q.T).todense()).flatten()\n",
        "\n",
        "def top_5_for_question(paragraphs, vectorizer, query, docs):\n",
        "    scores = score_documents(vectorizer, query, docs)\n",
        "    sorted_scores = np.argsort(-scores) # Negated for descending order\n",
        "    return [paragraphs[i] for i in sorted_scores[:5]], scores[sorted_scores[:5]], sorted_scores[:5]\n",
        "\n",
        "def print_top_5(query, top_5_para, top_5_scores, top_5_indices):\n",
        "    print(f\"Top-5 paragraphs: {top_5_indices}\")\n",
        "    print(f\"Question: {query['qas']['question']}\")\n",
        "    print(f\"Paragraphs:\")\n",
        "    for i in range(5):\n",
        "        print(f\"{i} (score: {top_5_scores[i]:.2f}): {top_5_para[i]['context']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-5 paragraphs: [    0  6929  6937  6944 12250]\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Paragraphs:\n",
            "0 (score: 0.27): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\n",
            "1 (score: 0.26):  In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary \"continued a pure and unspotted virgin\", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.\n",
            "\n",
            "2 (score: 0.23): Mary's special position within God's purpose of salvation as \"God-bearer\" (Theotokos) is recognised in a number of ways by some Anglican Christians. All the member churches of the Anglican Communion affirm in the historic creeds that Jesus was born of the Virgin Mary, and celebrates the feast days of the Presentation of Christ in the Temple. This feast is called in older prayer books the Purification of the Blessed Virgin Mary on February 2. The Annunciation of our Lord to the Blessed Virgin on March 25 was from before the time of Bede until the 18th century New Year's Day in England. The Annunciation is called the \"Annunciation of our Lady\" in the 1662 Book of Common Prayer. Anglicans also celebrate in the Visitation of the Blessed Virgin on 31 May, though in some provinces the traditional date of July 2 is kept. The feast of the St. Mary the Virgin is observed on the traditional day of the Assumption, August 15. The Nativity of the Blessed Virgin is kept on September 8.\n",
            "\n",
            "3 (score: 0.22): The Protoevangelium of James, an extra-canonical book, has been the source of many Orthodox beliefs on Mary. The account of Mary's life presented includes her consecration as a virgin at the temple at age three. The High Priest Zachariah blessed Mary and informed her that God had magnified her name among many generations. Zachariah placed Mary on the third step of the altar, whereby God gave her grace. While in the temple, Mary was miraculously fed by an angel, until she was twelve years old. At that point an angel told Zachariah to betroth Mary to a widower in Israel, who would be indicated. This story provides the theme of many hymns for the Feast of Presentation of Mary, and icons of the feast depict the story. The Orthodox believe that Mary was instrumental in the growth of Christianity during the life of Jesus, and after his Crucifixion, and Orthodox Theologian Sergei Bulgakov wrote: \"The Virgin Mary is the center, invisible, but real, of the Apostolic Church.\"\n",
            "\n",
            "4 (score: 0.21): The presence of the Virgin Mary under the cross[Jn. 19:26-27] has in itself been the subject of Marian art, and well known Catholic symbolism such as the Miraculous Medal and Pope John Paul II's Coat of Arms bearing a Marian Cross. And a number of Marian devotions also involve the presence of the Virgin Mary in Calvary, e.g., Pope John Paul II stated that \"Mary was united to Jesus on the Cross\". Well known works of Christian art by masters such as Raphael (e.g., the Mond Crucifixion), and Caravaggio (e.g., his Entombment) depict the Virgin Mary as part of the crucifixion scene.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "QUERY = questions[0]\n",
        "top5_para, top5_scores, top5_indices = top_5_for_question(paragraphs, vectorizer, QUERY, docs)\n",
        "print_top_5(QUERY, top5_para, top5_scores, top5_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_PVSuvt5yZh"
      },
      "source": [
        "We can measure how many times this simple baseline retrieves the correct paragraph on the training and test sets (top-1 and top-5 accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bEQ3Jm8j5yZi",
        "outputId": "668a7791-7cbf-4688-e6c3-ff961041c14a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 87599/87599 [07:19<00:00, 199.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 1 score: 72.01%,\n",
            "Top 5 score: 87.82%\n",
            "Wall time: 7min 19s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(questions)\n",
        "\n",
        "for q in tqdm(questions):\n",
        "    top5_para, top5_scores, top5_indices = top_5_for_question(paragraphs, vectorizer, q, docs)\n",
        "    top5_context_ids = [top5_para[i]['context_id'] for i in range(len(top5_para))]\n",
        "    gt_context_id = q['context_id'][0]\n",
        "    if gt_context_id == top5_context_ids[0]:\n",
        "        count_top1 += 1\n",
        "    if gt_context_id in top5_context_ids:\n",
        "        count_top5 += 1\n",
        "\n",
        "top1_score = count_top1 / count_total * 100\n",
        "top5_score = count_top5 / count_total * 100\n",
        "\n",
        "print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)\n",
        "\n",
        "test_questions = [{\n",
        "        'qas': qas,\n",
        "        'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "    }\n",
        "    for i in range(len(test_paragraphs_and_questions['data']))\n",
        "    for j, para in enumerate(test_paragraphs_and_questions['data'][i]['paragraphs'])\n",
        "    for qas in para['qas']\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10570/10570 [00:52<00:00, 201.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 1 score: 0.18%,\n",
            "Top 5 score: 0.96%\n",
            "Wall time: 52.3 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(test_questions)\n",
        "\n",
        "for q in tqdm(test_questions):\n",
        "    top5_para, top5_scores, top5_indices = top_5_for_question(paragraphs, vectorizer, q, docs)\n",
        "    top5_context_ids = [top5_para[i]['context_id'] for i in range(len(top5_para))]\n",
        "    gt_context_id = q['context_id'][0]\n",
        "    if gt_context_id == top5_context_ids[0]:\n",
        "        count_top1 += 1\n",
        "    if gt_context_id in top5_context_ids:\n",
        "        count_top5 += 1\n",
        "\n",
        "top1_score = count_top1 / count_total * 100\n",
        "top5_score = count_top5 / count_total * 100\n",
        "\n",
        "print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'context_id': 0}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paragraphs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5pMXBe-5yZj"
      },
      "source": [
        "The results are quite good already, but we'll investigate whether using a dense representation (eg. vectors computed by Bert) can further improve these scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhJKwAFO5yZj"
      },
      "source": [
        "## Answer computation\n",
        "\n",
        "Here we check how good are the answers of our usual model which selects the first document retrieved by Tf-Idf. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPneohNI5yZj"
      },
      "source": [
        "Firstly, we define a new kind of dataset containing everything we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q6NBvZ4Q5yZk"
      },
      "outputs": [],
      "source": [
        "def tf_idf_dataset_generator(questions: List[Dict], \n",
        "                             predicted_paragraphs: List,\n",
        "                             config: Config,\n",
        "                             return_question_id:bool=False):\n",
        "    # Iterate over questions\n",
        "    for i, q in enumerate(questions):\n",
        "        # Use the vectorizer to obtain the best scoring paragraph from the question\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        # Then encode the input using Bert's tokenizer\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question text\n",
        "            paragraph['context'],               # Then the best scoring paragraph text\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to the max length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "            return_token_type_ids = config.bert,# Return if the token is from sentence \n",
        "                                                # 0 or sentence 1\n",
        "            return_attention_mask = True,       # Return if it's a pad token or not\n",
        "        )\n",
        "        if return_question_id:\n",
        "            yield dict(encoded_inputs), q['qas']['id']\n",
        "        else:\n",
        "            yield dict(encoded_inputs)\n",
        "\n",
        "\n",
        "def create_original_dataset_with_tf_idf(questions: List[Dict], \n",
        "                                        predicted_paragraphs: List,\n",
        "                                        config: Config):\n",
        "    features = []\n",
        "    for i, q in enumerate(questions):\n",
        "        inputs={}\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question\n",
        "            paragraph[\"context\"],               # Then the context\n",
        "\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to this length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "\n",
        "            return_token_type_ids = False,      # Return if the token is from sentence \n",
        "                                                # 0 or sentence 1\n",
        "            return_attention_mask = False,      # Return if it's a pad token or not\n",
        "\n",
        "            return_offsets_mapping = True       # Returns each token's first and last char \n",
        "                                                # positions in the original sentence\n",
        "                                                # (we will use it to match answers starting \n",
        "                                                # and ending points to tokens)\n",
        "        )\n",
        "        inputs[\"context\"] = paragraph[\"context\"]\n",
        "        inputs[\"offset_mapping\"] = encoded_inputs[\"offset_mapping\"]\n",
        "        features.append(inputs)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        pd.DataFrame.from_dict(features).to_dict(orient=\"list\"))\n",
        "\n",
        "\n",
        "def create_dataset_using_tf_idf_vectorizer( questions: List[Dict],\n",
        "                                            predicted_paragraphs: List,\n",
        "                                            config: Config  ) -> tf.data.Dataset:\n",
        "    # Create expected signature for the generator output\n",
        "    if config.bert:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
        "            'token_type_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    else:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    # The dataset contains the features and the question IDs (strings)\n",
        "    signature = (features, tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    # Instantiates a partial generator\n",
        "    data_gen = partial(tf_idf_dataset_generator, \n",
        "        questions, predicted_paragraphs, config, return_question_id=True)\n",
        "    # Creates the dataset with the computed signature\n",
        "    dataset = tf.data.Dataset.from_generator(data_gen,\n",
        "        output_signature=signature)\n",
        "    # Compute dataset length, to be used by tensorflow internals\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    # Return the dataset\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we make some changes to the prediction and evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jmYhDgr05yZo"
      },
      "outputs": [],
      "source": [
        "### Prediction and evaluation function ###\n",
        "def predict_and_evaluate(DATASET_PATH:str, \n",
        "                         BEST_WEIGHTS_PATH:str, \n",
        "                         PATH_TO_PREDICTIONS_JSON:str,\n",
        "                         hidden_state_list:List[int]=[3,4,5,6],\n",
        "                         bert=False):\n",
        "    data = utils.read_question_set(DATASET_PATH)\n",
        "    print(\"Gathering questions and paragraphs from dataset...\")\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context \n",
        "                                   # and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(data['data']))\n",
        "        for j, para in enumerate(data['data'][i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(data['data']))\n",
        "        for para in data['data'][i]['paragraphs']\n",
        "    ]\n",
        "    print(\"Fitting the vectorizer...\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        strip_accents='unicode',    # Text is normalized into unicode characters\n",
        "        lowercase=True, # Then, we transform all text to lowercase\n",
        "        max_df=0.8,     # Filter out common words that appear in more \n",
        "                        # than 80% of the paragraphs\n",
        "        norm='l2'       # The vectorizer also l2 normalizes the vectors it produces, \n",
        "                        # so that the cosine similarity operation between \n",
        "                        # vectors simply becomes a dot product.\n",
        "    )      \n",
        "    docs = vectorizer.fit_transform(\n",
        "        [paragraphs[i]['context'] for i in range(len(paragraphs))] # Transform the paragraphs and fit the vectorizer\n",
        "    )\n",
        "    # To be sure that the matching question -> paragraph is always the same\n",
        "    # we pre-compute it in this generator \n",
        "    print(\"Obtaining best paragraph for questions...\")\n",
        "    predicted_paragraph = [top_5_for_question(paragraphs, vectorizer, q, docs)[0][0] for q in tqdm(questions)]\n",
        "    print(\"Creating model and dataset...\")\n",
        "    config = Config(bert=bert)\n",
        "    # Process questions\n",
        "    dataset = create_dataset_using_tf_idf_vectorizer(questions, predicted_paragraph, config)\n",
        "    print(\"Number of samples: \", len(dataset))\n",
        "    # Generate the original dataset that contains the original context and token-char mapping\n",
        "    original_dataset = create_original_dataset_with_tf_idf(questions, predicted_paragraph, config)\n",
        "    original_dataset = original_dataset.batch(config.BATCH_SIZE)\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    # Load model\n",
        "    model = config.create_standard_model(hidden_state_list=hidden_state_list)\n",
        "    model.load_weights(BEST_WEIGHTS_PATH)\n",
        "    print(\"Computing predictions...\")\n",
        "    # Predict the answers to the questions in the dataset\n",
        "    predictions = utils.compute_predictions(dataset, original_dataset, model)\n",
        "    print(f\"Done! Saving predictions at {PATH_TO_PREDICTIONS_JSON} and running evaluation script...\")\n",
        "    # Create a prediction file formatted like the one that is expected\n",
        "    with open(PATH_TO_PREDICTIONS_JSON, 'w') as f:\n",
        "        json.dump(predictions, f)\n",
        "    \n",
        "    !python eval/evaluate.py $DATASET_PATH $PATH_TO_PREDICTIONS_JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we run the test on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "lWt9EWUiQdnb"
      },
      "outputs": [],
      "source": [
        "BEST_WEIGHTS_PATH = \"./checkpoints/normal.h5\"\n",
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_predictions_tpu_normal.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c_oeEGD5yZr",
        "outputId": "966d781e-a018-4088-a2ed-d4b5f6d727d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gathering questions and paragraphs from dataset...\n",
            "Fitting the vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10570/10570 [00:10<00:00, 991.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 661/661 [02:31<00:00,  4.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! Saving predictions at ../data/results/tf_idf_predictions_tpu_normal.json and running evaluation script...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"exact\": 38.675496688741724,\n",
            "  \"f1\": 49.271948260308754,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 38.675496688741724,\n",
            "  \"HasAns_f1\": 49.271948260308754,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "predict_and_evaluate(TEST_FILE, BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_idf_retrieval_baseline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "68bcab510961ee730b63c1a3ea5fc4f15a05e7cdf00d3442986724f8b958dcdf"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
