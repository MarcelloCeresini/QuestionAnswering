{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O8H6y5b5yZT"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g84UMFvU5yZW"
      },
      "source": [
        "# Tf-Idf retrieval baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5idEIXrZhwB"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GLWZShI5yZY"
      },
      "source": [
        "In this notebook, we implement a simple baseline for paragraph retrieval using Tf-Idf weighted sparse representations of documents and query questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSMzmIlZ5yZZ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from config import Config\n",
        "config = Config()\n",
        "import utils\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "np.random.seed(config.RANDOM_SEED)\n",
        "random.seed(config.RANDOM_SEED)\n",
        "tf.random.set_seed(config.RANDOM_SEED)\n",
        "\n",
        "from typing import List, Dict\n",
        "#os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSfC7aXZhwE"
      },
      "source": [
        "Load the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_thUdCdU5yZZ"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')\n",
        "\n",
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuChVKjW5yZa"
      },
      "source": [
        "First of all, we separate questions from their paragraphs. We assign a `context_id` to each question and paragraph:\n",
        "\n",
        "- For questions, the context ID is a tuple `(paragraph_id, question_id_in_paragraph)`\n",
        "- For paragraphs, the context ID is a simple index corresponding to the first value in the tuple of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-Kzmnxv5yZa",
        "outputId": "e5944959-00f7-4d92-ae4e-ddca2aa41999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training questions: 65064\n",
            "Number of training paragraphs: 13975\n",
            "\n",
            "Number of val questions: 22535\n",
            "Number of val paragraphs: 4921\n",
            "\n",
            "Number of test questions: 10570\n",
            "Number of test paragraphs: 2067\n"
          ]
        }
      ],
      "source": [
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)\n",
        "\n",
        "print(f\"Number of training questions: {len(train_questions)}\")\n",
        "print(f\"Number of training paragraphs: {len(train_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of val questions: {len(val_questions)}\")\n",
        "print(f\"Number of val paragraphs: {len(val_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of test questions: {len(test_questions)}\")\n",
        "print(f\"Number of test paragraphs: {len(test_paragraphs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFoe0wTg5yZc"
      },
      "source": [
        "We build a function to obtain paragraphs given the `context_id`s we built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GrYOzf4X5yZc"
      },
      "outputs": [],
      "source": [
        "def get_paragraph_from_question(qas, dataset):\n",
        "    i,j = qas['context_id']\n",
        "    return dataset[i]['paragraphs'][j]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gva5b_nQZhwM"
      },
      "source": [
        "Let's try it on a random question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNTJ9JHH5yZd",
        "outputId": "54d2a7e8-2b8d-4a83-c1d9-c8eb9cfa247d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What historical event illustrated that dividing sovereignty was not possible?\n",
            "\n",
            "Ground truth context: 'Until recently, in the absence of prior agreement on a clear and precise definition, the concept was thought to mean (as a shorthand) 'a division of sovereignty between two levels of government'. New research, however, argues that this cannot be correct, as dividing sovereignty - when this concept is properly understood in its core meaning of the final and absolute source of political authority in a political community - is not possible. The descent of the United States into Civil War in the mid-nineteenth century, over disputes about unallocated competences concerning slavery and ultimately the right of secession, showed this. One or other level of government could be sovereign to decide such matters, but not both simultaneously. Therefore, it is now suggested that federalism is more appropriately conceived as 'a division of the powers flowing from sovereignty between two levels of government'. What differentiates the concept from other multi-level political forms is the characteristic of equality of standing between the two levels of government established. This clarified definition opens the way to identifying two distinct federal forms, where before only one was known, based upon whether sovereignty resides in the whole (in one people) or in the parts (in many peoples): the federal state (or federation) and the federal union of states (or federal union), respectively. Leading examples of the federal state include the United States, Germany, Canada, Switzerland, Australia and India. The leading example of the federal union of states is the European Union.'\n"
          ]
        }
      ],
      "source": [
        "x = random.randint(0, len(train_questions)-1)\n",
        "print(f\"Question: {train_questions[x]['qas']['question']}\")\n",
        "print()\n",
        "print(f\"Ground truth context: '{get_paragraph_from_question(train_questions[x], train_paragraphs_and_questions)['context']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKpoi6sCZhwO"
      },
      "source": [
        "## Vectorizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nomrUS_g5yZe"
      },
      "source": [
        "Now, given a question (query) we would like to obtain the paragraph that most probably contains the answer. Once we have a paragraph, we pass it into the BERT QA model we created for the standard project to obtain an answer. \n",
        "\n",
        "One way to do that is by using Tf-Idf on the large set of paragraphs. In reality we will use more complex methods and this should be considered a baseline. We will use a `TdIdfVectorizer` from Scikit Learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "46T1i7J95yZf"
      },
      "outputs": [],
      "source": [
        "train_vectorizer = TfidfVectorizer(strip_accents='unicode', \n",
        "    lowercase=True, \n",
        "    max_df=0.8,     # Filter out common words that appear in more than 80% of the paragraphs\n",
        "    norm='l2')      # The vectorizer also l2 normalizes the vectors it produces, \n",
        "                    # so that the cosine similarity operation between vectors simply becomes a dot product.\n",
        "\n",
        "val_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "test_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "\n",
        "train_docs = train_vectorizer.fit_transform([train_paragraphs[i]['context'] for i in range(len(train_paragraphs))])\n",
        "val_docs = val_vectorizer.fit_transform([val_paragraphs[i]['context'] for i in range(len(val_paragraphs))])\n",
        "test_docs = test_vectorizer.fit_transform([test_paragraphs[i]['context'] for i in range(len(test_paragraphs))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBbhOcrr5yZg",
        "outputId": "e6d1f4f0-a56d-42d0-9af7-61f3d265001e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13975, 65808), (4921, 36612), (2067, 22934))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_docs.shape, val_docs.shape, test_docs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-mVyLoKZhwR"
      },
      "source": [
        "- The training set is made of 13975 paragraphs and was tokenized into 65808 tokens, weighted with Tf-Idf.\n",
        "- The validation set is made of 4921 paragraphs and was tokenized into 36612 tokens.\n",
        "- The validation set is made of 2067 paragraphs and was tokenized into 22934 tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t7Prb_F5yZg"
      },
      "source": [
        "A way to compute **scores** between a query question and the set of documents, we use the **cosine similarity**.\n",
        "\n",
        "$$\n",
        "S_C (A,B) = \\frac{A \\cdot B}{\\lVert A\\rVert  \\lVert B\\rVert }\n",
        "$$\n",
        "\n",
        "With the `TfIdfVectorizer` we already L2-normalized all produced vectors, so in the actual implementation we only compute a dot product, which is slightly faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mlbBSHE75yZh"
      },
      "outputs": [],
      "source": [
        "def score_documents(vectorizer, query, docs):\n",
        "    q = query['qas']['question']\n",
        "    q = vectorizer.transform([q]) # q will be a (sparse) matrix with dimensionality 1 x vocab_dim\n",
        "    # We can compute a vector of all dot products scores and transform it from dense matrix to numpy array like this:\n",
        "    return np.asarray(np.dot(docs, q.T).todense()).flatten()\n",
        "\n",
        "def top_5_for_question(paragraphs, vectorizer, query, docs):\n",
        "    scores = score_documents(vectorizer, query, docs)\n",
        "    sorted_scores = np.argsort(-scores) # Negated scores for descending order\n",
        "    return [paragraphs[i] for i in sorted_scores[:5]], scores[sorted_scores[:5]], sorted_scores[:5]\n",
        "\n",
        "def print_top_5(query, top_5_para, top_5_scores, top_5_indices):\n",
        "    print(f\"Top-5 paragraphs: {top_5_indices}\")\n",
        "    print(f\"Question: {query['qas']['question']}\")\n",
        "    print(f\"Paragraphs:\")\n",
        "    for i in range(5):\n",
        "        print(f\"{i} (score: {top_5_scores[i]:.2f}): {top_5_para[i]['context']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "shkcnfNjZhwS",
        "outputId": "7b61fb4f-9720-401f-86b7-b2a99aba1d08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-5 paragraphs: [   0 5037 5045 5052 8619]\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Paragraphs:\n",
            "0 (score: 0.26): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\n",
            "1 (score: 0.26):  In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary \"continued a pure and unspotted virgin\", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.\n",
            "\n",
            "2 (score: 0.23): Mary's special position within God's purpose of salvation as \"God-bearer\" (Theotokos) is recognised in a number of ways by some Anglican Christians. All the member churches of the Anglican Communion affirm in the historic creeds that Jesus was born of the Virgin Mary, and celebrates the feast days of the Presentation of Christ in the Temple. This feast is called in older prayer books the Purification of the Blessed Virgin Mary on February 2. The Annunciation of our Lord to the Blessed Virgin on March 25 was from before the time of Bede until the 18th century New Year's Day in England. The Annunciation is called the \"Annunciation of our Lady\" in the 1662 Book of Common Prayer. Anglicans also celebrate in the Visitation of the Blessed Virgin on 31 May, though in some provinces the traditional date of July 2 is kept. The feast of the St. Mary the Virgin is observed on the traditional day of the Assumption, August 15. The Nativity of the Blessed Virgin is kept on September 8.\n",
            "\n",
            "3 (score: 0.22): The Protoevangelium of James, an extra-canonical book, has been the source of many Orthodox beliefs on Mary. The account of Mary's life presented includes her consecration as a virgin at the temple at age three. The High Priest Zachariah blessed Mary and informed her that God had magnified her name among many generations. Zachariah placed Mary on the third step of the altar, whereby God gave her grace. While in the temple, Mary was miraculously fed by an angel, until she was twelve years old. At that point an angel told Zachariah to betroth Mary to a widower in Israel, who would be indicated. This story provides the theme of many hymns for the Feast of Presentation of Mary, and icons of the feast depict the story. The Orthodox believe that Mary was instrumental in the growth of Christianity during the life of Jesus, and after his Crucifixion, and Orthodox Theologian Sergei Bulgakov wrote: \"The Virgin Mary is the center, invisible, but real, of the Apostolic Church.\"\n",
            "\n",
            "4 (score: 0.20): The presence of the Virgin Mary under the cross[Jn. 19:26-27] has in itself been the subject of Marian art, and well known Catholic symbolism such as the Miraculous Medal and Pope John Paul II's Coat of Arms bearing a Marian Cross. And a number of Marian devotions also involve the presence of the Virgin Mary in Calvary, e.g., Pope John Paul II stated that \"Mary was united to Jesus on the Cross\". Well known works of Christian art by masters such as Raphael (e.g., the Mond Crucifixion), and Caravaggio (e.g., his Entombment) depict the Virgin Mary as part of the crucifixion scene.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "QUERY = train_questions[0]\n",
        "top5_para, top5_scores, top5_indices = top_5_for_question(train_paragraphs, train_vectorizer, QUERY, train_docs)\n",
        "print_top_5(QUERY, top5_para, top5_scores, top5_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g6FAw-_ZhwT"
      },
      "source": [
        "In this case, the prediction was correct. Also, the other paragraphs seem quite relevant too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiDA_EgfZhwT"
      },
      "source": [
        "## Paragraph Retrieval evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_PVSuvt5yZh"
      },
      "source": [
        "We can measure how many times this simple baseline retrieves the correct paragraph on the training and test sets computing the top-1 and top-5 accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2C2v7VqZhwU"
      },
      "source": [
        "### Training set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bEQ3Jm8j5yZi",
        "outputId": "d563963b-e937-4518-f6ca-f2ceea3e7c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65064/65064 [06:25<00:00, 168.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 1 score: 75.41%,\n",
            "Top 5 score: 90.15%\n",
            "CPU times: user 6min 18s, sys: 4.38 s, total: 6min 22s\n",
            "Wall time: 6min 25s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(train_questions)\n",
        "\n",
        "for q in tqdm(train_questions):\n",
        "    top5_para, top5_scores, top5_indices = top_5_for_question(train_paragraphs, train_vectorizer, q, train_docs)\n",
        "    top5_context_ids = [top5_para[i]['context_id'] for i in range(len(top5_para))]\n",
        "    gt_context_id = q['context_id'][0]\n",
        "    if gt_context_id == top5_context_ids[0]:\n",
        "        count_top1 += 1\n",
        "    if gt_context_id in top5_context_ids:\n",
        "        count_top5 += 1\n",
        "\n",
        "top1_score = count_top1 / count_total * 100\n",
        "top5_score = count_top5 / count_total * 100\n",
        "\n",
        "print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMrLv_HjZhwU"
      },
      "source": [
        "### Test set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "caSPPogaZhwV",
        "outputId": "324c190a-4d5e-4017-f5bf-9a7fe5ae276e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:18<00:00, 577.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 1 score: 89.04%,\n",
            "Top 5 score: 97.69%\n",
            "CPU times: user 18.3 s, sys: 142 ms, total: 18.4 s\n",
            "Wall time: 18.3 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(test_questions)\n",
        "\n",
        "for q in tqdm(test_questions):\n",
        "    top5_para, top5_scores, top5_indices = top_5_for_question(test_paragraphs, test_vectorizer, q, test_docs)\n",
        "    top5_context_ids = [top5_para[i]['context_id'] for i in range(len(top5_para))]\n",
        "    gt_context_id = q['context_id'][0]\n",
        "    if gt_context_id == top5_context_ids[0]:\n",
        "        count_top1 += 1\n",
        "    if gt_context_id in top5_context_ids:\n",
        "        count_top5 += 1\n",
        "\n",
        "top1_score = count_top1 / count_total * 100\n",
        "top5_score = count_top5 / count_total * 100\n",
        "\n",
        "print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5pMXBe-5yZj"
      },
      "source": [
        "The results are quite good already, but we'll investigate whether using a dense representation (eg. vectors computed by Bert) can further improve these scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhJKwAFO5yZj"
      },
      "source": [
        "## Question Answering evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGqnUdyTZhwV"
      },
      "source": [
        "We can plug this simple baseline into the pipeline for using the Bert model for Question Answering we built for the standard project: in this way we can evaluate the actual Question Answering performances of this method.\n",
        "\n",
        "To do that, we need to modify the dataset generator a little bit, because it needs to compute on-the-fly the associated paragraphs for each query question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q6NBvZ4Q5yZk"
      },
      "outputs": [],
      "source": [
        "# New dataset generator using the Bert or DistilBert tokenizer to create input encodings for the model\n",
        "# using the original questions and the PREDICTED paragraphs instead of the ground truth ones\n",
        "def tf_idf_dataset_generator(questions: List[Dict],       # Can be training or test questions\n",
        "                             predicted_paragraphs: List,  # This generator takes in input the List of predicted paragraphs for each question.\n",
        "                             config: Config,\n",
        "                             return_question_id:bool=False):\n",
        "    # Iterate over questions\n",
        "    for i, q in enumerate(questions):\n",
        "        # We use the paragraph obtained by the vectorizer to compute the best scoring paragraph for the question\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        # Then encode the input as usual using Bert's tokenizer\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question text\n",
        "            paragraph['context'],               # Then the best scoring paragraph text\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to the max length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "            return_token_type_ids = config.bert,# Return if the token is from sentence \n",
        "                                                # 0 or sentence 1\n",
        "            return_attention_mask = True,       # Return if it's a pad token or not\n",
        "        )\n",
        "        if return_question_id:\n",
        "            yield dict(encoded_inputs), q['qas']['id']\n",
        "        else:\n",
        "            yield dict(encoded_inputs)\n",
        "    \n",
        "# Here instead we generate the \"original\" dataset containing only the context of the predicted\n",
        "# paragraph and the offset mappings of the tokens.\n",
        "def create_original_dataset_with_tf_idf(questions: List[Dict], \n",
        "                                        predicted_paragraphs: List,\n",
        "                                        config: Config):\n",
        "    features = []\n",
        "    for i, q in enumerate(questions):\n",
        "        inputs={}\n",
        "        # The paragraph is collected from those that were pre-predicted\n",
        "        paragraph = predicted_paragraphs[i]\n",
        "        encoded_inputs = config.tokenizer(\n",
        "            q['qas'][\"question\"],               # First we pass the question\n",
        "            paragraph[\"context\"],               # Then the context\n",
        "\n",
        "            max_length = config.INPUT_LEN,      # We want to pad and truncate to this length\n",
        "            truncation = True,\n",
        "            padding = 'max_length',             # Pads all sequences to 512.\n",
        "\n",
        "            return_token_type_ids = False,      # Return if the token is from sentence \n",
        "                                                # 0 or sentence 1\n",
        "            return_attention_mask = False,      # Return if it's a pad token or not\n",
        "\n",
        "            return_offsets_mapping = True       # Returns each token's first and last char \n",
        "                                                # positions in the original sentence\n",
        "                                                # (we will use it to match answers starting \n",
        "                                                # and ending points to tokens)\n",
        "        )\n",
        "        # We fill the inputs dictionary\n",
        "        inputs[\"context\"] = paragraph[\"context\"]\n",
        "        inputs[\"offset_mapping\"] = encoded_inputs[\"offset_mapping\"]\n",
        "        features.append(inputs)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        pd.DataFrame.from_dict(features).to_dict(orient=\"list\"))\n",
        "\n",
        "\n",
        "# This function creates the actual dataset using the generator.\n",
        "def create_dataset_using_tf_idf_vectorizer( questions: List[Dict],\n",
        "                                            predicted_paragraphs: List,\n",
        "                                            config: Config  ) -> tf.data.Dataset:\n",
        "    # Create expected signature for the generator output\n",
        "    if config.bert:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32),\n",
        "            'token_type_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    else:\n",
        "        features = {\n",
        "            'input_ids': tf.TensorSpec(shape=(512,), dtype=tf.int32), \n",
        "            'attention_mask': tf.TensorSpec(shape=(512,), dtype=tf.int32)\n",
        "        }\n",
        "    # The dataset contains the features and the question IDs (strings)\n",
        "    signature = (features, tf.TensorSpec(shape=(), dtype=tf.string))\n",
        "    # Instantiates a partial generator\n",
        "    data_gen = partial(tf_idf_dataset_generator, \n",
        "        questions, predicted_paragraphs, config, return_question_id=True)\n",
        "    # Creates the dataset with the computed signature\n",
        "    dataset = tf.data.Dataset.from_generator(data_gen,\n",
        "        output_signature=signature)\n",
        "    # Compute dataset length, to be used by tensorflow internals\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    # Return the dataset\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBgLN7mwZhwX"
      },
      "source": [
        "Then, we make some changes to the prediction and evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jmYhDgr05yZo"
      },
      "outputs": [],
      "source": [
        "### Prediction and evaluation function ###\n",
        "def predict_and_evaluate(best_weights_path:str, \n",
        "                         path_to_predictions_json:str,\n",
        "                         config:Config,\n",
        "                         dataset_type:str='test',               # One of 'train', 'val', 'test'\n",
        "                         hidden_state_list:List[int]=[3,4,5,6],\n",
        "                         bert=False):\n",
        "    print(\"Collecting the requested dataset and vectorizer...\")\n",
        "    if dataset_type == 'train':\n",
        "        questions = train_questions\n",
        "        paragraphs = train_paragraphs\n",
        "        vectorizer = train_vectorizer\n",
        "        docs = train_docs\n",
        "        # dataset_path = \n",
        "    elif dataset_type == 'val':\n",
        "        questions = val_questions\n",
        "        paragraphs = val_paragraphs\n",
        "        vectorizer = val_vectorizer\n",
        "        docs = val_docs\n",
        "        dataset_path = os.path.join(config.ROOT_PATH, 'data', 'validation_set.json')\n",
        "    elif dataset_type == 'test':\n",
        "        questions = test_questions\n",
        "        paragraphs = test_paragraphs\n",
        "        vectorizer = test_vectorizer\n",
        "        docs = test_docs\n",
        "        dataset_path = os.path.join(config.ROOT_PATH, 'data', 'dev_set.json')\n",
        "    else:\n",
        "        raise NotImplementedError(\"That dataset type does not exist. Change the dataset_type argument into one of ['train', 'val', 'test']\")\n",
        "    # We pre-compute the predicted paragraph for each question in the set.\n",
        "    print(\"Obtaining best paragraph for questions...\")\n",
        "    predicted_paragraphs = [top_5_for_question(paragraphs, vectorizer, q, docs)[0][0] for q in tqdm(questions)]\n",
        "    print(\"Creating model and dataset...\")\n",
        "    config = Config(bert=bert)\n",
        "    # Process questions\n",
        "    dataset = create_dataset_using_tf_idf_vectorizer(questions, predicted_paragraphs, config)\n",
        "    print(\"Number of samples: \", len(dataset))\n",
        "    dataset = dataset.batch(config.BATCH_SIZE)\n",
        "    # Generate the original dataset that contains the original context and token-char mapping\n",
        "    original_dataset = create_original_dataset_with_tf_idf(questions, predicted_paragraphs, config)\n",
        "    original_dataset = original_dataset.batch(config.BATCH_SIZE)\n",
        "    # Load model with the best obtained weights from the old project\n",
        "    model = config.create_standard_model(hidden_state_list=hidden_state_list)\n",
        "    model.load_weights(best_weights_path)\n",
        "    print(\"Computing predictions...\")\n",
        "    # Predict the answers to the questions in the dataset\n",
        "    predictions = utils.compute_predictions(dataset, original_dataset, model)\n",
        "    print(f\"Done! Saving predictions at {path_to_predictions_json} and running evaluation script...\")\n",
        "    # Create a prediction file formatted like the one that is expected\n",
        "    with open(path_to_predictions_json, 'w') as f:\n",
        "        json.dump(predictions, f)\n",
        "    \n",
        "    !python eval/evaluate.py $dataset_path $path_to_predictions_json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWOUcHxGZhwY"
      },
      "source": [
        "Finally, we run an evaluation on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lWt9EWUiQdnb"
      },
      "outputs": [],
      "source": [
        "BEST_WEIGHTS_PATH = \"./checkpoints/normal.h5\" if not IN_COLAB else '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/normal_100_tpu_h5_cval/normal.h5'\n",
        "PATH_TO_PREDICTIONS_JSON = '../data/results/tf_idf_test_predictions_normal.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c_oeEGD5yZr",
        "outputId": "d4508ae6-9296-46b1-8f6b-5b694ba8da5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting the requested dataset and vectorizer...\n",
            "Obtaining best paragraph for questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10570/10570 [00:19<00:00, 540.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model and dataset...\n",
            "Number of samples:  10570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661/661 [04:26<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Saving predictions at ../data/results/tf_idf_test_predictions_normal.json and running evaluation script...\n",
            "{\n",
            "  \"exact\": 38.666035950804165,\n",
            "  \"f1\": 49.26541133366401,\n",
            "  \"total\": 10570,\n",
            "  \"HasAns_exact\": 38.666035950804165,\n",
            "  \"HasAns_f1\": 49.26541133366401,\n",
            "  \"HasAns_total\": 10570\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "predict_and_evaluate(BEST_WEIGHTS_PATH, PATH_TO_PREDICTIONS_JSON, config, dataset_type='test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_idf_retrieval_baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 ('NLP')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "18a89a7fdf3e3a68641885182c683d74918d7d0e2f844b3d808d8c30cfd9264b"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}