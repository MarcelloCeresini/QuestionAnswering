{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcelloCeresini/QuestionAnswering/blob/dpr_v2_lower_dim/src/dense_passage_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8oZkaIoI1esj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8223e2cc-135c-46c9-860f-503c8a04eacc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 45.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=1b2ae4c41f9e633bc707bda3598e2774fd5e9a15497c264c3019cbc49ffa5af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
            "Cloning into 'QuestionAnswering'...\n",
            "warning: redirecting to https://github.com/MarcelloCeresini/QuestionAnswering.git/\n",
            "remote: Enumerating objects: 945, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 945 (delta 76), reused 34 (delta 29), pack-reused 830\u001b[K\n",
            "Receiving objects: 100% (945/945), 28.98 MiB | 15.69 MiB/s, done.\n",
            "Resolving deltas: 100% (573/573), done.\n",
            "/content/QuestionAnswering/src\n"
          ]
        }
      ],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXDnbCt1ess"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tIdZRSl1esu"
      },
      "source": [
        "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
        "\n",
        "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
        "\n",
        "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
        "\n",
        "In the Tf-Idf example, the retriever was simply a function that returned the top 5 scores obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
        "\n",
        "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
        "\n",
        "At run-time, another Dense Encoder is used $E_Q$ which maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
        "\n",
        "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
        "\n",
        "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
        "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
        "\n",
        "The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI. At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
        "\n",
        "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
        "\n",
        "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
        "\n",
        "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
        "- Random: a negative is any random passage in the corpus\n",
        "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
        "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
        "\n",
        "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pais where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1cbEuy1es4"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm45AN-fDEiv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8CBcRCvCCpzg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import List, Union, Dict, Tuple\n",
        "from transformers import BertTokenizerFast, DistilBertTokenizerFast, \\\n",
        "                         TFBertModel, TFDistilBertModel\n",
        "import utils\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "BERT_DIMENSIONALITY = 240 # otherwise it becomes impossible to store the BERT representations of the paragraphs in our system\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Ih1ZqSDCmW"
      },
      "source": [
        "### TPU check\n",
        "The training could be made faster if we use the cloud GPUs offered by Google on Google Colab. Since TPUs require manual intialization and other oddities, we check multiple times throughout the notebook what kind of hardware we are running the code on.\n",
        "using_TPU = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iyt5e89MCm4X",
        "outputId": "9e12a8cb-abc0-4fed-8759-b0106abc244e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.144.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.31.144.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "if using_TPU:\n",
        "    try: \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
        "        using_TPU = False\n",
        "else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    checkpoint_dir = '/content/drive/My Drive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "    datasets_dir = '/content/drive/My Drive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
        "else:\n",
        "    # Create the folder where we'll save the weights of the model\n",
        "    checkpoint_dir = os.path.join(\"data\", \"training_dpr\")\n",
        "    datasets_dir = os.path.join(\"data\", \"training_dpr\", \"dataset\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(datasets_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "jDjUKQxol1lS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1640bf9-a7af-4daf-b708-1cf87d8d57ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Allowed operations"
      ],
      "metadata": {
        "id": "vVHmkEeC2kuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OVERWRITE_DATASETS = False      # Best to be run on GPU, takes about half an hour.\n",
        "                                # It saves a new copy of the pre-tokenized\n",
        "                                # question-paragraph pair on Google Drive.\n",
        "                                # The copy should then be uploaded to GCloud into\n",
        "                                # the bucket allocated for the project to be used by\n",
        "                                # TPUs.\n",
        "\n",
        "DO_TRAINING = True              # Best to be run on TPU, takes about 2 hours.\n",
        "                                # It trains the model and saves a copy of the initial\n",
        "                                # and last weights on Google Drive. When run on GPU\n",
        "                                # It also saves checkpoints and Tensorboard data,\n",
        "                                # but it takes over 4 hours to do a single epoch,\n",
        "                                # so it's impossible to run it on Colab.\n",
        "\n",
        "OVERRIDE_REPRESENTATIONS = False # Best to be run on GPU, it takes about 30 minutes.\n",
        "                                 # It produces representations of all paragraphs\n",
        "                                 # using the trained model_p as a large NumPy array\n",
        "                                 # that is stored on Google Drive at the end.\n",
        "\n",
        "RUN_ACCURACY_ANALYSIS = False   # Best to be run on GPU, takes about an hour\n",
        "                                # It analyzes the paragraph retrieval capability\n",
        "                                # of the model with top-1 and top-5 accuracy.\n",
        "\n",
        "DO_QA_TRAINING = False          # Best to be run on TPU, takes more than 2 hours.\n",
        "                                # See DO_TRAINING."
      ],
      "metadata": {
        "id": "QSfXDW2g2sAr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv3n12tDIAo"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8piWcupDOPR"
      },
      "source": [
        "We define all the paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "On3I2PUgCwNF"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLC6xwfKDSAN"
      },
      "source": [
        "We collect the training and validation questions into different lists. Paragraphs are collected together into a single, unified pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1zdDCsLZDMkm"
      },
      "outputs": [],
      "source": [
        "train_dict = utils.read_question_set(TRAINING_FILE)\n",
        "val_dict = utils.read_question_set(VALIDATION_FILE)\n",
        "\n",
        "def get_questions_and_paragraphs_from_dataset(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset['data']))\n",
        "        for j, para in enumerate(dataset['data'][i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': (i,j)\n",
        "        }\n",
        "        for i in range(len(dataset['data']))\n",
        "        for j, para in enumerate(dataset['data'][i]['paragraphs'])\n",
        "    ]\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        questions[i]['global_id'] = i\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "\n",
        "def make_paragraph_pool(train_p, val_p):\n",
        "    paragraphs = train_p + val_p\n",
        "    for i in range(len(paragraphs)):\n",
        "        paragraphs[i]['global_id'] = i\n",
        "    for i in range(len(train_p)):\n",
        "        paragraphs[i]['from_set'] = 'train' \n",
        "    for i in range(len(train_p), len(train_p) + len(val_p)):\n",
        "        paragraphs[i]['from_set'] = 'val' \n",
        "    return paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs_from_dataset(train_dict)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs_from_dataset(val_dict)\n",
        "paragraphs = make_paragraph_pool(train_paragraphs, val_paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvGAqUBvDbIg"
      },
      "source": [
        "We create the two different DistilBert models for encoding and test them on a random question/paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1XI6YUBZDit8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "d922006c7beb461db6ca5f09837f100c",
            "c046fdc704844307a24bf63a90106a39",
            "dd4b7a42411c4468bd3a286359d87898",
            "718c8209da524e299d7f7217efeebae1",
            "d9a7f773c79841e09b23a70f3dce5f3d",
            "2acb3dc386044c9196ec6b2169607883",
            "7cdb6d43aef34b4d8637315681fd87fa",
            "2050a44993f24997920d540e1fe20c29",
            "4767ff8f74b046d6b596718b5c3cbaf9",
            "ff5782c53edb41719efe7be4c9c9163f",
            "af011dde1eb14a27aee289ce846230b0",
            "251ca56eb0a34797a4cdbc03cd69a4d5",
            "731db3a2ea1c46d38da3877b3e06380e",
            "bab2ea94301d4877ad320124f2417390",
            "6b7105f611db4731a002027f515dd3de",
            "cfa23da7945145d08fe253e4504d68b8",
            "7e7f79c0f3044452b1c7914d18b4a0ad",
            "6a59c67f1e7a49bdb04de7696e371cb3",
            "dd83641538d24610969ab498a7d2fa9b",
            "ca80801d21504a6da8a81189ef352ab5",
            "89a3f8ab7a784f82848622295c35d103",
            "6b5184d0bced4fb0bb6c683a15e8c72a",
            "97811a5510e04d6f80f78349af33104a",
            "09f2b409bafd4b1ea541d4eb6c9eb90a",
            "fb0b949573c24eb7898efba6547375d6",
            "abf69e4d64c74e06b940fe876cacb620",
            "8a560e0afb894e86ae50d0073d807a39",
            "b9bb21fd93654d5e8347b78a71b6240e",
            "de90f7a1d9304dcc968031e2577cc5f6",
            "7842392242d747c28ff250c1f480ea81",
            "cb6fc8935a1a4d0b9c96b8e53fd4def1",
            "adc1fe162d494cab9093e11515003370",
            "7c4de7dd902a4eb28583185a2ead2087",
            "7d09df23fc714d8db8b9bbf0e16bb8dd",
            "1dee67fe311042ba93397b589496a249",
            "ada87120c122422db11b1dca822a148a",
            "a5b7e24fad334b489e0f94026c8b6124",
            "9fe366dff9ea48d4b75636ed0a046007",
            "46fbd4324ea541dcbbd8d7099c6a2855",
            "e62ef7d4513f4360b802602bdd549a38",
            "0924f87b11a746ddb47ab7c04ff6a00d",
            "e77f0e4ff90643c6b81309f28c1e736f",
            "b52231374aa240c6a5f007d70bd7a854",
            "b94a57eb641d476890c126cc34fa2549",
            "52fd40dc559647508ae234f56eb7c2b0",
            "f6a8d1a6b30a44a189c22b6512a124e8",
            "90346f08361947f9b3d9a02e57699f71",
            "cd90708906dd44e6813b8d247047696d",
            "38f584f7b21d4632a58fe8a76fdbe8fb",
            "dbc605143d284f609db16f7b5b91e7f7",
            "141bbc8726de412393f276e42cddc98a",
            "a6acaf5533ad44a797f2441a223598ee",
            "0eec18d9434245fdba77dbcc33bda84d",
            "5aa715c564a84e7281afdc822bf04d94",
            "0f7a56c2a52c48c6b4a045f087833201"
          ]
        },
        "outputId": "6ef7a82a-7afb-4950-a97d-1706b574cf33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d922006c7beb461db6ca5f09837f100c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "251ca56eb0a34797a4cdbc03cd69a4d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97811a5510e04d6f80f78349af33104a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d09df23fc714d8db8b9bbf0e16bb8dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/347M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52fd40dc559647508ae234f56eb7c2b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "model_q, model_p = TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                   TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the ideal maximum length of a sequence of tokens?"
      ],
      "metadata": {
        "id": "QFP6H-ok1jyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "for paragraph in tqdm(paragraphs):\n",
        "    emb = tokenizer_distilbert(paragraph['context'], return_tensors='np')\n",
        "    tokens = emb['attention_mask'].shape[1]\n",
        "    lens.append(tokens)"
      ],
      "metadata": {
        "id": "RFfkIhW-1i8P",
        "outputId": "e61deaf2-29b7-4703-e4ab-c71798273b8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 501/23817 [00:00<00:13, 1697.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 23817/23817 [00:13<00:00, 1762.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total n. of paragraphs: {len(paragraphs)}\")\n",
        "for i in range(350, 500, 10):\n",
        "    print(f\"Only {np.sum(np.asarray(lens) > i)} are truncated when using {i} max tokens ({np.sum(np.asarray(lens) > i) / len(paragraphs)*100}%)\")"
      ],
      "metadata": {
        "id": "k3AYesH33TWp",
        "outputId": "8aedcbcb-7330-48c6-d071-98eeb5b888c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total n. of paragraphs: 23817\n",
            "Only 306 are truncated when using 350 max tokens (1.284796573875803%)\n",
            "Only 264 are truncated when using 360 max tokens (1.1084519460889282%)\n",
            "Only 228 are truncated when using 370 max tokens (0.9572994079858924%)\n",
            "Only 201 are truncated when using 380 max tokens (0.8439350044086157%)\n",
            "Only 170 are truncated when using 390 max tokens (0.7137758743754461%)\n",
            "Only 144 are truncated when using 400 max tokens (0.6046101524121426%)\n",
            "Only 120 are truncated when using 410 max tokens (0.5038417936767855%)\n",
            "Only 104 are truncated when using 420 max tokens (0.4366628878532141%)\n",
            "Only 86 are truncated when using 430 max tokens (0.36108661880169624%)\n",
            "Only 75 are truncated when using 440 max tokens (0.31490112104799095%)\n",
            "Only 63 are truncated when using 450 max tokens (0.26451694168031237%)\n",
            "Only 52 are truncated when using 460 max tokens (0.21833144392660706%)\n",
            "Only 42 are truncated when using 470 max tokens (0.17634462778687493%)\n",
            "Only 37 are truncated when using 480 max tokens (0.15535121971700885%)\n",
            "Only 30 are truncated when using 490 max tokens (0.12596044841919637%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose to truncate sequences at 400 tokens to reduce memory consumption.\n",
        "\n",
        "Furthermore, we reduce the dimensionality of DistilBert's output to 240 (instead of the default of 768)."
      ],
      "metadata": {
        "id": "t80p88uc5493"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReducedDistilBertModel(keras.Model):\n",
        "    def __init__(self, distilbert_model):\n",
        "        super(ReducedDistilBertModel, self).__init__()\n",
        "        self.distilbert_model = distilbert_model\n",
        "        self.reduction_layer = keras.layers.Dense(BERT_DIMENSIONALITY, \n",
        "                                                  activation='gelu')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        hidden_state = self.distilbert_model(inputs).last_hidden_state\n",
        "        # We introduce a dense layer that simply reduces the dimensionality of distilbert\n",
        "        return self.reduction_layer(hidden_state)\n",
        "\n",
        "model_q = ReducedDistilBertModel(model_q)\n",
        "model_p = ReducedDistilBertModel(model_p)"
      ],
      "metadata": {
        "id": "8GSRnaQpJOqB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EcCbC-Vi1es5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b262b290-a745-4f2f-d57a-f3a9c052cf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on a simple question. \n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Representation dimensionality: (240,)\n"
          ]
        }
      ],
      "source": [
        "test_question = train_questions[0]['qas']['question']\n",
        "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
        "inputs_test = tokenizer_distilbert(test_question, return_tensors=\"tf\")\n",
        "outputs = model_q(inputs_test)\n",
        "\n",
        "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
        "test_q_repr = outputs[0,0,:]\n",
        "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_lPiEZu1es9"
      },
      "source": [
        "# Encoders Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7qavEm61es-"
      },
      "source": [
        "First of all, we need to train our models. To do that, we need to create a dataset that feeds batches of questions and positive and negative paragraphs to a model, which is used to compute the representations, then the similarities and to correct the learnt distributions from the encoder models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCYkx8rf1es_"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynvz3Gw1etA"
      },
      "source": [
        "For the dataset, we use the `tf.data.Dataset` API. In particular, we first create the dataset in a `.proto` file which contains all the information in a byte format. The file is then uploaded on Google Cloud, where it can easily be accessed all future times. This is the only working way to have a large dataset accessible from the TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OaBWVf5k1etB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e13838-bfad-4138-daef-7ad2a9be2df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train_v3 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/train_v3.proto).\n",
            "Loading val_v3 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/val_v3.proto).\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8 if not using_TPU else 64\n",
        "MAX_SEQ_LEN = 400\n",
        "\n",
        "def get_paragraph_from_question(qas, dataset):\n",
        "    i,j = qas['context_id']\n",
        "    return dataset['data'][i]['paragraphs'][j]\n",
        "\n",
        "def get_paragraph_global_id_from_question(question, paragraphs):\n",
        "    for p in paragraphs:\n",
        "        if p['context_id'] == question['context_id']:\n",
        "            return p['global_id']\n",
        "\n",
        "def pre_tokenize_data(questions, dataset, tokenizer):\n",
        "    tokenized_questions = [\n",
        "        dict(tokenizer(questions[i]['qas']['question'], \n",
        "            max_length = MAX_SEQ_LEN, truncation = True, \n",
        "            padding = 'max_length'))\n",
        "    for i in tqdm(range(len(questions)))]\n",
        "    tokenized_paragraphs = [\n",
        "        dict(tokenizer(get_paragraph_from_question(\n",
        "                    questions[i], dataset\n",
        "                )['context'], max_length = MAX_SEQ_LEN, \n",
        "            truncation = True, padding = 'max_length',\n",
        "            return_offsets_mapping = True))\n",
        "    for i in tqdm(range(len(questions)))]\n",
        "    return tokenized_questions, tokenized_paragraphs\n",
        "\n",
        "def find_start_end_token_one_hot_encoded(\n",
        "    answers: Dict, \n",
        "    offsets: List[Tuple[int]]) -> Dict:\n",
        "    '''\n",
        "    This function returns the starting and ending token of the answer, \n",
        "    already one hot encoded and ready for binary crossentropy.\n",
        "    Inputs:\n",
        "        - answers: `List[Dict]` --> for each question, a list of answers.\n",
        "            Each answer contains:\n",
        "            - `answer_start`: the index of the starting character\n",
        "            - `text`: the text of the answer, that we exploit through the \n",
        "                number of chars that it contains\n",
        "        - offsets: `List[Tuple[int]]` --> the tokenizer from HuggingFace \n",
        "            transforms the paragraph into a sequence of tokens. \n",
        "            Offsets keeps track of the character start and end indexes for each token.\n",
        "   \n",
        "    Output:\n",
        "        - result: `Dict` --> each key contains only one array, the one-hot \n",
        "            encoded version of, respectively, the start and end token of \n",
        "            the answer in the sentence (question+context)\n",
        "    '''\n",
        "    result = {\n",
        "        \"out_S\": np.zeros(len(offsets), dtype=np.int32),\n",
        "        \"out_E\": np.zeros(len(offsets), dtype=np.int32)\n",
        "    } \n",
        "    for answer in answers:\n",
        "        starting_char = answer['answer_start']\n",
        "        answer_len = len(answer['text'])\n",
        "        # We skip the first token, [CLS], that has (0,0) as a tuple\n",
        "        for i in range(1, len(offsets)):\n",
        "            # Check if starting char is within the indexes\n",
        "            if (starting_char >= offsets[i][0]) and \\\n",
        "                (starting_char <= offsets[i][1]):\n",
        "                result[\"out_S\"][i] += 1\n",
        "            # If the ending char (starting + length -1) is in the interval, \n",
        "            # same as above.\n",
        "            if (starting_char + answer_len - 1 >= offsets[i][0]) and \\\n",
        "                (starting_char + answer_len - 1 < offsets[i][1]):\n",
        "                result[\"out_E\"][i] += 1\n",
        "                break\n",
        "    return result\n",
        "\n",
        "def decode_fn(record_bytes):\n",
        "  example = tf.io.parse_single_example(\n",
        "      # Data\n",
        "      record_bytes,\n",
        "      # Schema\n",
        "      {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64)})\n",
        "  return {\n",
        "      \"questions\": {'input_ids': example['question__input_ids'],\n",
        "                    'attention_mask': example['question__attention_mask'],\n",
        "                    'index': example['question__index']},\n",
        "      \"answers\":   {'out_s': example['answer__out_s'],\n",
        "                    'out_e': example['answer__out_e']},\n",
        "      \"paragraphs\":{'input_ids': example['paragraph__input_ids'],\n",
        "                    'attention_mask': example['paragraph__attention_mask'],\n",
        "                    'tokens_s': example['paragraph__tokens_s'],\n",
        "                    'tokens_e': example['paragraph__tokens_e'],\n",
        "                    'index': example['paragraph__index']}\n",
        "  }\n",
        "\n",
        "def create_dataset_from_records(questions, paragraphs, dataset, tokenizer, \n",
        "                                fn, batch_size=BATCH_SIZE, training=True):\n",
        "    # Pre-tokenize and write dataset on disk\n",
        "    filename = f'{fn}_v3.proto'\n",
        "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
        "    gcs_filename = f'gs://volpepe-nlp-project-squad-datasets/{fn_type}.proto'\n",
        "    if not os.path.exists(filename) or OVERWRITE_DATASETS:\n",
        "        print(\"Pre-tokenizing data...\")\n",
        "        tok_questions, tok_paragraphs = pre_tokenize_data(questions, dataset, tokenizer)\n",
        "        assert len(tok_questions) == len(tok_paragraphs), \"Error while pre-tokenizing dataset\"\n",
        "        print(\"Preprocessing answers...\")\n",
        "        answer_tokens = [find_start_end_token_one_hot_encoded(\n",
        "            questions[i]['qas']['answers'], tok_paragraphs[i]['offset_mapping'])\n",
        "        for i in tqdm(range(len(questions)))]\n",
        "        print(\"Saving dataset on disk...\")\n",
        "        with tf.io.TFRecordWriter(filename) as file_writer:\n",
        "            for i in tqdm(range(len(tok_questions))):\n",
        "                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                    \"question__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=tok_questions[i][\"input_ids\"])),\n",
        "                    \"question__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=tok_questions[i][\"attention_mask\"])),\n",
        "                    \"question__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[i])),\n",
        "                    \"answer__out_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_S\"])),\n",
        "                    \"answer__out_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_E\"])),\n",
        "                    \"paragraph__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_paragraphs[i][\"input_ids\"])),\n",
        "                    \"paragraph__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_paragraphs[i][\"attention_mask\"])),\n",
        "                    \"paragraph__tokens_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[0] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
        "                    \"paragraph__tokens_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[1] for x in tok_paragraphs[i][\"offset_mapping\"]])), \n",
        "                    \"paragraph__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[get_paragraph_global_id_from_question(questions[i], paragraphs)]))\n",
        "                    })).SerializeToString()\n",
        "                file_writer.write(record_bytes)\n",
        "        print(\"Upload the dataset on Google Cloud and re-run the function\")\n",
        "        return None\n",
        "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
        "    # Return it as processed dataset\n",
        "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_fn)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(10000)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "### CREATE DATASETS ###\n",
        "# Valid for both TPU and GPU\n",
        "dataset_train = create_dataset_from_records(train_questions, paragraphs, train_dict, tokenizer_distilbert, \n",
        "                                            os.path.join(datasets_dir, 'train'))\n",
        "dataset_val = create_dataset_from_records(val_questions, paragraphs, val_dict, tokenizer_distilbert,\n",
        "                                            os.path.join(datasets_dir, 'val'), training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAnwbiFU1etF"
      },
      "source": [
        "## Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lwz19a1etG"
      },
      "source": [
        "First of all, we need a layer that takes as input the dictionary containing the tokenized questions and answers and returns their compact representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PjPUMwKk1etG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055fce23-b5cc-4967-a835-806b7b8b6416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape when in training mode: (64, 240), (64, 240)\n",
            "Output shape when in testing mode: (64, 240)\n",
            "Output shape when dealing with a single question: (1, 240)\n"
          ]
        }
      ],
      "source": [
        "class DenseEncoder(layers.Layer):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.model_q = model_q  # Dense encoder for questions\n",
        "        self.model_p = model_p  # Dense encoder for paragraphs\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "        qs = {\n",
        "            'input_ids': inputs['questions']['input_ids'],\n",
        "            'attention_mask': inputs['questions']['attention_mask']\n",
        "        }\n",
        "        q_repr = self.model_q(qs)[:,0,:]\n",
        "        if training:\n",
        "            # Input contains the questions and paragraphs encoding\n",
        "            ps = {\n",
        "                'input_ids': inputs['paragraphs']['input_ids'],\n",
        "                'attention_mask': inputs['paragraphs']['attention_mask']\n",
        "            }\n",
        "            p_repr = self.model_p(ps)[:,0,:]\n",
        "            return q_repr, p_repr\n",
        "        else:\n",
        "            return q_repr\n",
        "\n",
        "# Small test for the layer\n",
        "class TestDenseEncoderModel(keras.Model):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.enc(inputs, training=training)\n",
        "\n",
        "test_model = TestDenseEncoderModel(model_q, model_p)\n",
        "q_repr, p_repr = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=True)\n",
        "print(f\"Output shape when in training mode: {q_repr.shape}, {p_repr.shape}\")\n",
        "q_repr_2 = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=False)\n",
        "print(f\"Output shape when in testing mode: {q_repr_2.shape}\")\n",
        "q = tokenizer_distilbert(\n",
        "    train_questions[0]['qas']['question'], max_length = MAX_SEQ_LEN, \n",
        "    truncation = True, padding = 'max_length', return_tensors=\"tf\")\n",
        "q_repr_3 = test_model({'questions': q})\n",
        "print(f\"Output shape when dealing with a single question: {q_repr_3.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4r1MfHP1etI"
      },
      "source": [
        "Once we have the representations, we should compute the similarities, thus obtaining a a full mini-batch of positive-negative examples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZdG5bkWV2FU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bdb3c3-a605-476c-aed4-9292730e15ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Create the similarity matrix\n",
        "S = tf.tensordot(q_repr, tf.transpose(p_repr), axes=1)\n",
        "S.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEPy1ug1__n0"
      },
      "source": [
        "This similarity matrix has the following meaning:\n",
        "- Rows represent questions.\n",
        "- Each row contains the similarity that the respective question has with the 16 paragraphs (one of them is the positive one, the others are negative)\n",
        "\n",
        "In the paper, they refer to the loss as a *minimization of the negative log-likelihood of the positive passage*: what it really means is that we need to transform similarities to probabilities and use a categorical cross-entropy loss, where labels are the row index (which is also the column index in that row for the positive passage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J_aJezqWCKsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "645a7e08-77c5-43f9-8056-d27ce9f0c221"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5599976"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True\n",
        ")\n",
        "loss(y_true=tf.range(BATCH_SIZE), y_pred=S).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnidWQLZDwsf"
      },
      "source": [
        "The loss seems to be quite high for this batch. We can study it with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vdgc2-gQD1PP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "if not using_TPU: # Otherwise the batch size is HUGE\n",
        "    S_arr = S.numpy()\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQwdzlTfEsH4"
      },
      "source": [
        "Indeed, ideally the predictions should be on the diagonal. This means that the \"default\" space for this metric learning problem is not that good. We are ready to learn a new representation distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOLMK5mIMPc"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nme0koNHE76t"
      },
      "outputs": [],
      "source": [
        "class DeepQPEncoder(keras.Model):\n",
        "\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            # For training we return the similarity matrix\n",
        "            repr_q, repr_p = self.enc(inputs, training=training)\n",
        "            S = tf.tensordot(repr_q, tf.transpose(repr_p), axes=1)\n",
        "            return S\n",
        "        else:\n",
        "            # In other cases, we return the representation of the question(s)\n",
        "            repr_q = self.enc(inputs, training=training)            \n",
        "            return repr_q\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Obtain similarities\n",
        "            S = self(x, training=True)\n",
        "            # Obtain loss value\n",
        "            loss = self.compiled_loss(y, S)\n",
        "        # Construct gradients and apply them through the optimizer\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Update and return metrics (specifically the one for the loss value).\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x = data\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        S = self(x, training=True) # We are not really training, but we have to obtain S\n",
        "        self.compiled_loss(y, S)\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "def create_model(sample, freeze_layers_up_to=5):\n",
        "    print(\"Creating BERT models...\")\n",
        "    model_q, model_p =  TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                        TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Freeze layers \n",
        "    for i in range(freeze_layers_up_to): # layers 0 to variable are frozen, successive layers learn\n",
        "        model_q.distilbert.transformer.layer[i].trainable = False\n",
        "        model_p.distilbert.transformer.layer[i].trainable = False\n",
        "\n",
        "    model_q, model_p = ReducedDistilBertModel(model_q), ReducedDistilBertModel(model_p)\n",
        "    \n",
        "    print(\"Creating Deep Encoder...\")\n",
        "    model = DeepQPEncoder(model_q, model_p)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model and loss\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of data to build the model\n",
        "    model(sample)\n",
        "\n",
        "    # Return the model\n",
        "    print(\"Model created!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA_dwB6MIHaq"
      },
      "source": [
        "## Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdxw7F4IWVi"
      },
      "source": [
        "Define utility variables and saving paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s838Rj9GIUA9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "PATIENCE = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrJEqCQIZ3c"
      },
      "source": [
        "Before training, we check if we're using a TPU, in order to create the model within the scope of the strategy.\n",
        "\n",
        "Then, we train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wN6WxrL-IRjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2f8971-6ffe-46b0-a3be-6db22383d839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating BERT models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Deep Encoder...\n",
            "Compiling...\n",
            "Testing on some data...\n",
            "Model created!\n",
            "Epoch 1/100\n",
            "1369/1369 [==============================] - 392s 231ms/step - loss: 0.5823 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.2891 - val_sparse_categorical_accuracy: 0.8976\n",
            "Epoch 2/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.2295 - sparse_categorical_accuracy: 0.9183 - val_loss: 0.1975 - val_sparse_categorical_accuracy: 0.9289\n",
            "Epoch 3/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.1745 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.1595 - val_sparse_categorical_accuracy: 0.9406\n",
            "Epoch 4/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.1447 - sparse_categorical_accuracy: 0.9474 - val_loss: 0.1334 - val_sparse_categorical_accuracy: 0.9518\n",
            "Epoch 5/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.1225 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.1134 - val_sparse_categorical_accuracy: 0.9586\n",
            "Epoch 6/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.1088 - sparse_categorical_accuracy: 0.9604 - val_loss: 0.1043 - val_sparse_categorical_accuracy: 0.9598\n",
            "Epoch 7/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.1007 - sparse_categorical_accuracy: 0.9624 - val_loss: 0.0897 - val_sparse_categorical_accuracy: 0.9668\n",
            "Epoch 8/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.0871 - sparse_categorical_accuracy: 0.9678 - val_loss: 0.0839 - val_sparse_categorical_accuracy: 0.9684\n",
            "Epoch 9/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.0802 - sparse_categorical_accuracy: 0.9695 - val_loss: 0.0741 - val_sparse_categorical_accuracy: 0.9724\n",
            "Epoch 10/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.0744 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.0735 - val_sparse_categorical_accuracy: 0.9724\n",
            "Epoch 11/100\n",
            "1369/1369 [==============================] - 294s 214ms/step - loss: 0.0706 - sparse_categorical_accuracy: 0.9738 - val_loss: 0.0661 - val_sparse_categorical_accuracy: 0.9753\n",
            "Epoch 12/100\n",
            "1369/1369 [==============================] - 292s 214ms/step - loss: 0.0653 - sparse_categorical_accuracy: 0.9751 - val_loss: 0.0639 - val_sparse_categorical_accuracy: 0.9748\n",
            "Epoch 13/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.0601 - sparse_categorical_accuracy: 0.9773 - val_loss: 0.0590 - val_sparse_categorical_accuracy: 0.9786\n",
            "Epoch 14/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9784 - val_loss: 0.0583 - val_sparse_categorical_accuracy: 0.9778\n",
            "Epoch 15/100\n",
            "1369/1369 [==============================] - 292s 214ms/step - loss: 0.0519 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0533 - val_sparse_categorical_accuracy: 0.9794\n",
            "Epoch 16/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.0539 - val_sparse_categorical_accuracy: 0.9797\n",
            "Epoch 17/100\n",
            "1369/1369 [==============================] - 292s 213ms/step - loss: 0.0499 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.0495 - val_sparse_categorical_accuracy: 0.9810\n",
            "Epoch 18/100\n",
            "1369/1369 [==============================] - 291s 212ms/step - loss: 0.0459 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0500 - val_sparse_categorical_accuracy: 0.9806\n",
            "Epoch 19/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.0467 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0439 - val_sparse_categorical_accuracy: 0.9832\n",
            "Epoch 20/100\n",
            "1369/1369 [==============================] - 291s 212ms/step - loss: 0.0417 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.0445 - val_sparse_categorical_accuracy: 0.9828\n",
            "Epoch 21/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.0404 - sparse_categorical_accuracy: 0.9843 - val_loss: 0.0409 - val_sparse_categorical_accuracy: 0.9841\n",
            "Epoch 22/100\n",
            "1369/1369 [==============================] - 292s 214ms/step - loss: 0.0393 - sparse_categorical_accuracy: 0.9851 - val_loss: 0.0395 - val_sparse_categorical_accuracy: 0.9847\n",
            "Epoch 23/100\n",
            "1369/1369 [==============================] - 294s 214ms/step - loss: 0.0388 - sparse_categorical_accuracy: 0.9846 - val_loss: 0.0381 - val_sparse_categorical_accuracy: 0.9850\n",
            "Epoch 24/100\n",
            "1369/1369 [==============================] - 293s 214ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.0359 - val_sparse_categorical_accuracy: 0.9866\n",
            "Epoch 25/100\n",
            "1369/1369 [==============================] - 291s 212ms/step - loss: 0.0354 - sparse_categorical_accuracy: 0.9864 - val_loss: 0.0369 - val_sparse_categorical_accuracy: 0.9866\n",
            "Epoch 26/100\n",
            "1369/1369 [==============================] - 291s 213ms/step - loss: 0.0344 - sparse_categorical_accuracy: 0.9867 - val_loss: 0.0396 - val_sparse_categorical_accuracy: 0.9841\n",
            "Epoch 27/100\n",
            "1369/1369 [==============================] - 299s 219ms/step - loss: 0.0341 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0376 - val_sparse_categorical_accuracy: 0.9855\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)\n",
        "\n",
        "    # Workaraound for saving locally when using cloud TPUs\n",
        "    local_device_option = tf.train.CheckpointOptions(\n",
        "        experimental_io_device=\"/job:localhost\")\n",
        "else:\n",
        "    # GPUs and local systems don't need the above specifications. We simply\n",
        "    # create a pattern for the filename and let the callbacks deal with it.\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.ckpt\")\n",
        "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        \"training_dpr_reduced_dim\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)\n",
        "\n",
        "    if DO_TRAINING:\n",
        "        # ModelCheckpoint callback is only available when not using TPU\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath = checkpoint_path,\n",
        "            verbose=1,\n",
        "            save_weights_only = True,\n",
        "            save_best_only = False\n",
        "        )\n",
        "\n",
        "        # Same for tensorboard callback\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        )\n",
        "\n",
        "if DO_TRAINING:\n",
        "    # Early stopping can be used by both hardware\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    if using_TPU:\n",
        "        # Save first weights in a h5 file (it's the most stable way)\n",
        "        model.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_reduced_dim_tpu_0.h5'),  overwrite=True)\n",
        "    else:\n",
        "        # Save the first weights using the pattern from before\n",
        "        model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "    callbacks = [es_callback]\n",
        "    if not using_TPU:\n",
        "        # These callback imply saving stuff on local disk, which cannot be \n",
        "        # done automatically using TPUs.\n",
        "        # Therefore, they are only active when using GPUs and local systems\n",
        "        callbacks.extend([cp_callback, tensorboard_callback])\n",
        "\n",
        "    # We fit the model\n",
        "    history = model.fit(\n",
        "        dataset_train, \n",
        "        y=None,\n",
        "        validation_data=dataset_val,\n",
        "        epochs=EPOCHS, \n",
        "        callbacks=callbacks,\n",
        "        shuffle=True,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=0,\n",
        "        verbose=1 # Show progress bar\n",
        "    )\n",
        "\n",
        "    if using_TPU:\n",
        "        # Save last weights\n",
        "        model.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_reduced_dim_tpu_last.h5'), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained in 24 (+3 due to patience) epochs on a cloud TPU. It took about two hours and reached a validation accuracy of 98.66%."
      ],
      "metadata": {
        "id": "LCgA-4RbAZCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering with the DPR\n",
        "\n",
        "We have trained the two Bert (`bert_p`, the paragraphs encoder and `bert_q`, the questions encoder) models to produce embeddings that are as similar as possible for matching question-paragraph pairs. \n",
        "\n",
        "Thanks to our training, when we use `bert_q` to encode our question, we will now be sure that questions and paragraphs will both be encoded in the same space and have a high similarity between the question encoding and the matching paragraph's encoding.\n",
        "\n",
        "We can now use `bert_p` to encode all of our paragraphs a-priori using the same method we have used before (taking the 768-d encoding at the `[CLS]` token). These encodings will be stored in RAM. \n",
        "\n",
        "Then, we can define our final Question Answering model in this way:\n",
        "- It receives only a question's embedding as input.\n",
        "- It uses `bert_q` to create a representation of the question in the learnt 768-d space, that is in common with the paragraph representations.\n",
        "- We compute similarity scores between the representation of the question and all representations of paragraphs. Based on these scores, we select the top-k ($k=100$) paragraphs.\n",
        "- For each of the $k$ paragraphs, we must compute the probability of the paragraph being selected $P_{selected}(i)$, as well as the usual $P_{start, i}(s), P_{end, i}(t)$ for each of the $s$-th and $t$-th words of the $i$-th paragraph. To do that, we need the full encoding of the paragraph (the $512 \\times 768$ output of Bert), which will be denoted as $P_i$ in contrast to $\\hat{P}_i$ which is the 768-d encoding at the `[CLS]` token. We obtain the full encoding by passing the $k$ paragraphs through `bert_p`, which is set to non-trainable (otherwise the encoding of the `[CLS]` token would constantly change). \n",
        "- All probabilities are computed through dense layers:\n",
        "\\begin{gather}\n",
        "P_{start,i}(s) = softmax(P_i w_{start})_s\n",
        "\\\\\n",
        "P_{end,i}(t) = softmax(P_i w_{end})_t\n",
        "\\\\\n",
        "P_{selected}(i) = softmax(\\hat{P}^\\intercal w_{selected})_i\n",
        "\\\\\n",
        "\\end{gather}\n",
        "where $w_{start}$, $w_{end}$ and $w_{selected}$ are learnt vectors, while $\\hat{P} = [P_{1}^{[CLS]}, \\dots, P_k^{[CLS]}]$.\n",
        "- As final answer, we select the highest scoring start-end legal span from the highest-scoring paragraph.\n",
        "\n",
        "During training: For each question, we create a batch by sampling $m$ ($m=24$ in the paper) from the top-100 passages returned by the retrieval system (DPR, so by computing similarities with the pre-computed representations). The training objective is to maximize the marginal log-likelihood of all the correct answer spans in the positive passage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. In the paper, a batch size of 16 was used.\n"
      ],
      "metadata": {
        "id": "5DtqzKMW6scF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paragraphs representations"
      ],
      "metadata": {
        "id": "5InDU76v93rS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we load the weights of the trained model."
      ],
      "metadata": {
        "id": "fbp2lLe9iX-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the obtained weights\n",
        "model.load_weights(os.path.join(checkpoint_dir, 'training_normal_tpu_last.h5'))"
      ],
      "metadata": {
        "id": "ry1uahz-bh4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we allocate space for the matrices that will contain all paragraphs encoding."
      ],
      "metadata": {
        "id": "OVIemd7MibmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paragraphs_encodings = np.empty(\n",
        "    shape=(len(train_paragraphs), BERT_DIMENSIONALITY)\n",
        ")\n",
        "val_paragraphs_encodings = np.empty(\n",
        "    shape=(len(val_paragraphs), BERT_DIMENSIONALITY)\n",
        ")\n",
        "print(train_paragraphs_encodings.shape, val_paragraphs_encodings.shape)"
      ],
      "metadata": {
        "id": "TPpSZ9nSZWMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8152710-1c15-45fc-c9e2-bbc841220501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18896, 768) (4921, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.join(checkpoint_dir, 'train_paragraphs_encodings.txt')) or \\\n",
        "   OVERRIDE_REPRESENTATIONS:\n",
        "\n",
        "    print(\"Obtaining training representations\")\n",
        "    for i in tqdm(range(len(train_paragraphs))):\n",
        "        train_paragraphs_encodings[i] = model.enc.model_p(tokenizer_distilbert(\n",
        "            train_paragraphs[i]['context'], max_length = 512, \n",
        "            return_tensors='tf',\n",
        "            truncation = True, padding = 'max_length'\n",
        "        )).last_hidden_state[0,0,:]\n",
        "    \n",
        "    np.savetxt(os.path.join(checkpoint_dir, 'train_paragraphs_encodings.txt'), \n",
        "               train_paragraphs_encodings, delimiter=',')\n",
        "\n",
        "if not os.path.exists(os.path.join(checkpoint_dir, 'val_paragraphs_encodings.txt')) or \\\n",
        "   OVERRIDE_REPRESENTATIONS:\n",
        "\n",
        "    print(\"Obtaining validation representations\")\n",
        "    for i in tqdm(range(len(val_paragraphs))):\n",
        "        val_paragraphs_encodings[i] = model.enc.model_p(tokenizer_distilbert(\n",
        "            val_paragraphs[i]['context'], max_length = 512, \n",
        "            return_tensors='tf',\n",
        "            truncation = True, padding = 'max_length'\n",
        "        )).last_hidden_state[0,0,:]\n",
        "\n",
        "    np.savetxt(os.path.join(checkpoint_dir, 'val_paragraphs_encodings.txt'),\n",
        "               val_paragraphs_encodings, delimiter=',')"
      ],
      "metadata": {
        "id": "bU8pBjgvbZS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_paragraphs_encoding = np.loadtxt(os.path.join(checkpoint_dir, \n",
        "                                                    'train_paragraphs_encodings.txt'), \n",
        "                                       delimiter=',', dtype=np.float32)\n",
        "val_paragraphs_encoding = np.loadtxt(os.path.join(checkpoint_dir, \n",
        "                                                  'val_paragraphs_encodings.txt'), \n",
        "                                     delimiter=',', dtype=np.float32)\n",
        "\n",
        "paragraph_encodings = np.concatenate([\n",
        "                                      train_paragraphs_encoding, \n",
        "                                      val_paragraphs_encoding], axis=0)"
      ],
      "metadata": {
        "id": "jPRAKDc-9GQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_encodings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACzsT13jiowd",
        "outputId": "77dae5aa-0d63-4c2a-cac0-847eb61b0c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23817, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, we don't have \"training\" and \"validation\" paragraphs, but a large set containing all of the available paragraphs."
      ],
      "metadata": {
        "id": "nRjPOuWJPMT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative test"
      ],
      "metadata": {
        "id": "ReN18CaggRW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select some random questions and see whether the selected paragraph is the correct one."
      ],
      "metadata": {
        "id": "qQ4-VRXZgXlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = random.choice(train_questions)\n",
        "print(f\"Question: {sample_question['qas']['question']}\")\n",
        "print(f\"Ground truth paragraph: {get_paragraph_from_question(sample_question, train_dict)['context']}\")\n",
        "print(f\"Ground truth index: {get_paragraph_global_id_from_question(sample_question, paragraphs)}\")\n",
        "sample_q_repr = model.enc.model_q(tokenizer_distilbert(\n",
        "    sample_question['qas']['question'], max_length = 512, \n",
        "    return_tensors='tf', truncation = True, padding = 'max_length'\n",
        ")).last_hidden_state[:,0,:]\n",
        "print(f\"Question representation shape: {sample_q_repr.shape}\")\n",
        "scores = tf.tensordot(sample_q_repr, paragraph_encodings.T, axes=1)\n",
        "best_par_index = tf.argsort(scores, axis=1, direction='DESCENDING')[0, :5].numpy()\n",
        "print(f\"Top-5 best matching paragraphs have indexes {best_par_index}\")\n",
        "print(f\"Top-5 best matching paragraphs:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i})\\t{paragraphs[best_par_index[i]]['context']}\")"
      ],
      "metadata": {
        "id": "S1NISpx7gs4I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f4c777f-8b7e-4b4d-cc0c-bdb11c1ffc1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What poet wrote a long poem describing Roman religious holidays?\n",
            "Ground truth paragraph: The meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation — a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.\n",
            "Ground truth index: 18137\n",
            "Question representation shape: (1, 768)\n",
            "Top-5 best matching paragraphs have indexes [18137 18135 10372 23647 18136]\n",
            "Top-5 best matching paragraphs:\n",
            "0)\tThe meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation — a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.\n",
            "1)\tRoman calendars show roughly forty annual religious festivals. Some lasted several days, others a single day or less: sacred days (dies fasti) outnumbered \"non-sacred\" days (dies nefasti). A comparison of surviving Roman religious calendars suggests that official festivals were organized according to broad seasonal groups that allowed for different local traditions. Some of the most ancient and popular festivals incorporated ludi (\"games,\" such as chariot races and theatrical performances), with examples including those held at Palestrina in honour of Fortuna Primigenia during Compitalia, and the Ludi Romani in honour of Liber. Other festivals may have required only the presence and rites of their priests and acolytes, or particular groups, such as women at the Bona Dea rites.\n",
            "2)\tWhile forming an integral part of the Christian calendar, particularly in Catholic regions, many Carnival traditions resemble those antedating Christianity. Italian Carnival is sometimes thought to be derived from the ancient Roman festivals of Saturnalia and Bacchanalia. The Saturnalia, in turn, may be based on the Greek Dionysia and Oriental festivals. For the start of the Roman Saturnalia, on December 17 authorities chose an enemy of the Roman people to represent the Lord of Misrule in each community. These men and women were forced to indulge in food and physical pleasures throughout the week, horribly murdered on December 25th: \"destroying the forces of darkness\".\n",
            "3)\tWhile forming an integral part of the Christian calendar, particularly in Catholic regions, many Carnival traditions resemble those antedating Christianity. Italian Carnival is sometimes thought to be derived from the ancient Roman festivals of Saturnalia and Bacchanalia. The Saturnalia, in turn, may be based on the Greek Dionysia and Oriental festivals. For the start of the Roman Saturnalia, on December 17 authorities chose an enemy of the Roman people to represent the Lord of Misrule in each community. These men and women were forced to indulge in food and physical pleasures throughout the week, horribly murdered on December 25th: \"destroying the forces of darkness\".\n",
            "4)\tOther public festivals were not required by the calendar, but occasioned by events. The triumph of a Roman general was celebrated as the fulfillment of religious vows, though these tended to be overshadowed by the political and social significance of the event. During the late Republic, the political elite competed to outdo each other in public display, and the ludi attendant on a triumph were expanded to include gladiator contests. Under the Principate, all such spectacular displays came under Imperial control: the most lavish were subsidised by emperors, and lesser events were provided by magistrates as a sacred duty and privilege of office. Additional festivals and games celebrated Imperial accessions and anniversaries. Others, such as the traditional Republican Secular Games to mark a new era (saeculum), became imperially funded to maintain traditional values and a common Roman identity. That the spectacles retained something of their sacral aura even in late antiquity is indicated by the admonitions of the Church Fathers that Christians should not take part.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually, the correct paragraphs show up in the Top-5 list."
      ],
      "metadata": {
        "id": "bOPMaSyb5CfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative test"
      ],
      "metadata": {
        "id": "cWM3pdRAwBlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can measure the top-1/top-5 accuracy with the same used in the tf-idf baseline."
      ],
      "metadata": {
        "id": "lJXrFcjnwFaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(val_questions)\n",
        "\n",
        "if RUN_ACCURACY_ANALYSIS:\n",
        "    def top_5_for_question(question):\n",
        "        sample_q_repr = model.enc.model_q(question).last_hidden_state[:,0,:]\n",
        "        scores = tf.tensordot(sample_q_repr, paragraphs.T, axes=1)\n",
        "        top5_indices = tf.argsort(scores, axis=1, direction='DESCENDING')[0, :5].numpy()\n",
        "        top5_scores = tf.sort(scores, axis=1, direction='DESCENDING')[0, :5].numpy()\n",
        "        top5_para = [paragraphs[i] for i in top5_indices]\n",
        "        return top5_para, top5_scores, top5_indices     \n",
        "\n",
        "    for i,q in enumerate(tqdm(val_questions)):\n",
        "        top5_para, top5_scores, top5_indices = top_5_for_question(\n",
        "            tokenizer_distilbert(\n",
        "                q['qas']['question'], max_length = 512, \n",
        "                return_tensors='tf', truncation = True, padding = 'max_length'\n",
        "            )\n",
        "        )\n",
        "        top5_context_ids = [top5_para[i]['context_id'] for i in range(len(top5_para))]\n",
        "        gt_context_id = q['context_id'][0]\n",
        "        if gt_context_id == top5_context_ids[0]:\n",
        "            count_top1 += 1\n",
        "        if gt_context_id in top5_context_ids:\n",
        "            count_top5 += 1\n",
        "\n",
        "    top1_score = count_top1 / count_total * 100\n",
        "    top5_score = count_top5 / count_total * 100\n",
        "\n",
        "    print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
      ],
      "metadata": {
        "id": "MShv2E92wXJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Redo\n",
        "\n",
        "Top 1 score: 86.49%\n",
        "\n",
        "Top 5 score: 96.58%\n",
        "\n",
        "This is a clear improvement with respect to the Tf-Idf baseline (72.01%/87.82%), but it's also quite slower. \n",
        "- It may look as an unfair comparison, because the Tf-Idf baseline was tested on all questions, but in fact it's more realistic because it's done on a subset of data that the model has not been trained on."
      ],
      "metadata": {
        "id": "RwctHyoXCrP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering model"
      ],
      "metadata": {
        "id": "zPG2DHje99it"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pre-tokenize the paragraphs so that we have easy access to them inside the model."
      ],
      "metadata": {
        "id": "bGzumB9nAVYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretokenized_paragraphs = {\n",
        "    'input_ids': [],\n",
        "    'attention_mask': [],\n",
        "    'offset_mapping': []\n",
        "}\n",
        "\n",
        "for i in tqdm(range(len(paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        paragraphs[i]['context'], max_length = 512, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['offset_mapping'].append(token_p['offset_mapping'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDK42osrAlsX",
        "outputId": "e38eaf7c-d1d9-4a2c-d383-6e25e063eaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23817/23817 [00:19<00:00, 1253.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we create the Dense Passage Retriever model which contains the logic to deal with finding the highest scoring paragraph. It's by design non-trainable, since it assumes that `model_q` has already been trained."
      ],
      "metadata": {
        "id": "VukXJ1tECXl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DensePassageRetriever(keras.Model):\n",
        "    '''\n",
        "    This model retrieves the best available passages given a question\n",
        "    and a set of paragraphs (encodings). The model is non-trainable,\n",
        "    as it assumes that the deep question encoder (model_q) has already been\n",
        "    trained.\n",
        "    '''\n",
        "    def __init__(self, model_q, m=100):\n",
        "        super().__init__(trainable=False)\n",
        "        self.model_q = model_q\n",
        "        self.m = m\n",
        "\n",
        "    def call(self, inputs):\n",
        "        '''\n",
        "        Assumes that inputs is a dictionary containing (at least):\n",
        "        - `input_ids`: the input ids of the questions obtained from the tokenizer\n",
        "        - `attention_mask`: the attention mask of the questions obtained from the tokenizer\n",
        "\n",
        "        Returns the (batch_size x num_parag) matrix of scores and the (batch_size x m) top_m indexes (according\n",
        "        to parameter `m` which defaults to 100)\n",
        "        '''\n",
        "        # 1) Obtain search encoding of question\n",
        "        qs = {\n",
        "            'input_ids': inputs['questions']['input_ids'],\n",
        "            'attention_mask': inputs['questions']['attention_mask']\n",
        "        }\n",
        "        q_repr = self.model_q(qs).last_hidden_state[:,0,:]                                   # batch_size x encoding_dim\n",
        "        # 2) Selection of paragraphs using search encoding\n",
        "        scores = tf.tensordot(q_repr, tf.transpose(paragraph_encodings), axes=1)             # batch_size x num_parag\n",
        "        topm_indexes = tf.argsort(scores, axis=1, direction='DESCENDING')[:, :self.m]        # batch_size x m\n",
        "        return scores, topm_indexes\n"
      ],
      "metadata": {
        "id": "w5mgm-pbCnLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define a function to create the model. The model should accept the top indices given by the DPR and return the collected paragraphs, their start, end and selection probabilities, as well as the representations according to `model_p`."
      ],
      "metadata": {
        "id": "qOcyq9GtN7i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_LEN = 512     # Maximum length of the token sequence\n",
        "M = 3               # Number of paragraphs to be collected by the DPR (24 in the paper)\n",
        "\n",
        "class CollectParagraphsLayer(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer that is used to collect the paragraphs according to the indices\n",
        "    returned by the DPR.\n",
        "    '''\n",
        "    def __init__(self, model_p):\n",
        "        super(CollectParagraphsLayer, self).__init__(trainable=False)\n",
        "        self.model_p = model_p\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Collect paragraphs. Inputs is the batch of paragraph indexes to gather\n",
        "        paragraphs = {\n",
        "            'input_ids': tf.squeeze(tf.gather(pretokenized_paragraphs['input_ids'], inputs), axis=2),\n",
        "            'attention_mask': tf.squeeze(tf.gather(pretokenized_paragraphs['attention_mask'], inputs), axis=2),\n",
        "            'offset_mapping': tf.squeeze(tf.gather(pretokenized_paragraphs['offset_mapping'], inputs), axis=2),\n",
        "            'indexes': inputs\n",
        "        }\n",
        "\n",
        "        # Iterate over batch to collect paragraphs representations\n",
        "        paragraphs_full_encodings = tf.cast(tf.expand_dims(self.model_p({\n",
        "                'input_ids': tf.gather(paragraphs['input_ids'], 0),\n",
        "                'attention_mask': tf.gather(paragraphs['attention_mask'], 0)\n",
        "            }).last_hidden_state, axis=0), dtype=tf.float32)\n",
        "        for i in tf.range(1, tf.shape(paragraphs['input_ids'])[0]):\n",
        "            tf.autograph.experimental.set_loop_options(\n",
        "                parallel_iterations=1,\n",
        "                swap_memory=True,\n",
        "                maximum_iterations=tf.shape(paragraphs['input_ids'])[0],\n",
        "                shape_invariants=[(paragraphs_full_encodings, tf.TensorShape([None, M, 512, 768]))]\n",
        "            )\n",
        "            hidden_state = self.model_p({\n",
        "                'input_ids': tf.gather(paragraphs['input_ids'], i),\n",
        "                'attention_mask': tf.gather(paragraphs['attention_mask'], i)\n",
        "            }).last_hidden_state\n",
        "            paragraphs_full_encodings = tf.concat([paragraphs_full_encodings, \n",
        "                                                  tf.cast(tf.expand_dims(hidden_state, axis=0), tf.float32)], \n",
        "                                                    axis=0)\n",
        "        \n",
        "        # Transform to tensors\n",
        "        paragraphs_search_encodings = paragraphs_full_encodings[:,:,0,:]\n",
        "\n",
        "        # # Compute full encoding (cannot pre-encode because it takes too much space) (also, for test mode)\n",
        "        # def hidden_states_fn(i, prev_hs, prev_hs_u):\n",
        "        #     # Body of the iteration: compute hidden state of element in batch\n",
        "        #     hidden_state = self.model_p({\n",
        "        #         'input_ids': tf.gather(paragraphs['input_ids'], i),\n",
        "        #         'attention_mask': tf.gather(paragraphs['attention_mask'], i)\n",
        "        #     }).last_hidden_state\n",
        "        #     return  tf.add(i, 1), tf.concat([prev_hs, tf.expand_dims(hidden_state, axis=0)], axis=0), \\\n",
        "        #             tf.concat([prev_hs_u, tf.expand_dims(hidden_state[:,0,:], axis=0)], axis=0)\n",
        "\n",
        "        # # Initial hidden state is that of element 0 of the batch\n",
        "        # init_hidden_state =  self.model_p({\n",
        "        #     'input_ids': tf.gather(paragraphs['input_ids'], 0),\n",
        "        #     'attention_mask': tf.gather(paragraphs['attention_mask'], 0)\n",
        "        # }).last_hidden_state\n",
        "        # # Define the loop variables and conditions\n",
        "        # loop_vars = (tf.constant(1), tf.expand_dims(init_hidden_state, axis=0), \n",
        "        #             tf.expand_dims(init_hidden_state[:,0,:], axis=0))\n",
        "        # cond = lambda i, a, b: tf.less(i, tf.shape(paragraphs['input_ids'])[0])\n",
        "        # body = hidden_states_fn\n",
        "        # # Run the while loop\n",
        "        # _, paragraphs_full_encodings, paragraphs_search_encodings = \\\n",
        "        #     tf.while_loop(cond, body, loop_vars, shape_invariants=(loop_vars[0].get_shape(), \n",
        "        #                                                         tf.TensorShape([None, M, 512, 768]),\n",
        "        #                                                         tf.TensorShape([None, M, 768])),\n",
        "        #                   swap_memory=True, maximum_iterations=tf.shape(paragraphs['input_ids'])[0])\n",
        "\n",
        "        return paragraphs, paragraphs_full_encodings, paragraphs_search_encodings\n",
        "\n",
        "class BestScoringCollector(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer to collect the start and end probabilities from the best scoring\n",
        "    paragraph\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BestScoringCollector, self).__init__(trainable=False, **kwargs)\n",
        "\n",
        "    def call(self, probs_s, probs_e, probs_sel):\n",
        "        # Selection of best scoring paragraphs\n",
        "        best_scoring_paragraphs = tf.squeeze(tf.argmax(probs_sel, axis=1, output_type=tf.int32))\n",
        "        # Selection of related start-end probabilities\n",
        "        probs_s = tf.squeeze(tf.gather(probs_s, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        probs_e = tf.squeeze(tf.gather(probs_e, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        return probs_s, probs_e\n",
        "\n",
        "def create_QA_model(model_p, m = M):\n",
        "    # Receives in input the top indexes received by the DPR\n",
        "    topm_indexes = keras.Input(shape=(M, ), dtype='int32', name=\"topm_indexes\")\n",
        "\n",
        "    paragraphs, paragraphs_full_encodings, paragraphs_search_encodings = \\\n",
        "        CollectParagraphsLayer(model_p)(topm_indexes)\n",
        "\n",
        "    # Compute probabilities for the start token\n",
        "    out_S = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"start_token_logits\")(paragraphs_full_encodings)\n",
        "    out_S = keras.layers.Reshape((M, 512))(out_S)\n",
        "    out_S = keras.layers.Softmax(name=\"start_probs\", axis=1, dtype='float32')(out_S)\n",
        "\n",
        "    # The same is done for the end tokens.\n",
        "    out_E = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"end_token_logits\")(paragraphs_full_encodings)\n",
        "    out_E = keras.layers.Reshape((M, 512))(out_E)\n",
        "    out_E = keras.layers.Softmax(name=\"end_probs\", axis=1, dtype='float32')(out_E)\n",
        "\n",
        "    # Also, we compute paragraph selection probabilities\n",
        "    out_SEL = keras.layers.Dense(1, name=\"selection_logits\")(paragraphs_search_encodings)\n",
        "    out_SEL = keras.layers.Flatten('channels_first')(out_SEL)\n",
        "    out_SEL = keras.layers.Softmax(name=\"selection_probs\", dtype='float32')(out_SEL)\n",
        "\n",
        "    out_S, out_E = BestScoringCollector(name='best_scoring_collector')(out_S, out_E, out_SEL)\n",
        "\n",
        "    # We return the keras model\n",
        "    model = keras.Model(\n",
        "        inputs=[topm_indexes],\n",
        "        outputs = [paragraphs, paragraphs_full_encodings, out_S, out_E, out_SEL]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "R_v3r3DcBeKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We analyze the model."
      ],
      "metadata": {
        "id": "JR-Jr45AObut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa = create_QA_model(model.enc.model_q, model.enc.model_p)"
      ],
      "metadata": {
        "id": "Cvu8NLQGtSbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_qa.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkdbnPt_v-G8",
        "outputId": "e34d586c-5a2e-482d-b48a-d4b7b06c513f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " topm_indexes (InputLayer)      [(None, 3)]          0           []                               \n",
            "                                                                                                  \n",
            " collect_paragraphs_layer (Coll  ({'input_ids': (Non  66362880   ['topm_indexes[0][0]']           \n",
            " ectParagraphsLayer)            e, 3, 512),                                                       \n",
            "                                 'attention_mask':                                                \n",
            "                                (None, 3, 512),                                                   \n",
            "                                 'offset_mapping':                                                \n",
            "                                (None, 3, 512, 2),                                                \n",
            "                                 'indexes': (None,                                                \n",
            "                                3)},                                                              \n",
            "                                 (None, 3, 512, 768                                               \n",
            "                                ),                                                                \n",
            "                                 (None, 3, 768))                                                  \n",
            "                                                                                                  \n",
            " start_token_logits (TimeDistri  (None, 3, 512, 1)   769         ['collect_paragraphs_layer[0][4]'\n",
            " buted)                                                          ]                                \n",
            "                                                                                                  \n",
            " end_token_logits (TimeDistribu  (None, 3, 512, 1)   769         ['collect_paragraphs_layer[0][4]'\n",
            " ted)                                                            ]                                \n",
            "                                                                                                  \n",
            " selection_logits (Dense)       (None, 3, 1)         769         ['collect_paragraphs_layer[0][5]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 3, 512)       0           ['start_token_logits[0][0]']     \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 3, 512)       0           ['end_token_logits[0][0]']       \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 3)            0           ['selection_logits[0][0]']       \n",
            "                                                                                                  \n",
            " start_probs (Softmax)          (None, 3, 512)       0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " end_probs (Softmax)            (None, 3, 512)       0           ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " selection_probs (Softmax)      (None, 3)            0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " best_scoring_collector (BestSc  (None, None)        0           ['start_probs[0][0]',            \n",
            " oringCollector)                                                  'end_probs[0][0]',              \n",
            "                                                                  'selection_probs[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,365,187\n",
            "Trainable params: 2,307\n",
            "Non-trainable params: 66,362,880\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outputs are of the expected shape. We also use an utility to see the model visually."
      ],
      "metadata": {
        "id": "m1Etf4vYO5OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model_qa, \"multi_input_and_output_model.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "q8KH_VJSL8VN",
        "outputId": "bed812e2-58be-4f4b-ac9e-4ca1257e1764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "To train the model we need to:\n",
        "\n",
        "- Compile it defining the losses and optimizer.\n",
        "- Create a ground truth batch that we use for comparing the model's output."
      ],
      "metadata": {
        "id": "WNhAi3lsPUOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define custom training and testing functions."
      ],
      "metadata": {
        "id": "leUruObriIep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics (create under the strategy scope if using TPU)\n",
        "if using_TPU:\n",
        "    with strategy.scope():\n",
        "        loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
        "        end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
        "        sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
        "else:\n",
        "    loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "    start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
        "    end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
        "    sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
        "\n",
        "class QuestionAnsweringModel(keras.Model):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super(QuestionAnsweringModel, self).__init__()\n",
        "        self.dpr_model = DensePassageRetriever(model_q, m=M)\n",
        "        self.model_qa = create_QA_model(model_q, model_p)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.model_qa(inputs)\n",
        "\n",
        "    @tf.function\n",
        "    def tf_shuffle_on_columns(self, value):\n",
        "        '''\n",
        "        Utility function that shuffles a tensor randomly on each of its rows.\n",
        "        '''\n",
        "        # Create a tensor of random numbers, argsort it and use them as indices to gather\n",
        "        # values from the original tensor\n",
        "        return tf.gather(value, tf.argsort(tf.random.uniform(tf.shape(value))), batch_dims=1)\n",
        "\n",
        "    @tf.function\n",
        "    def obtain_training_info(self, indexes, topm_indexes):\n",
        "        '''\n",
        "        Obtains a batch of data and the ground truth mask to be used while training the model\n",
        "        '''\n",
        "        # Collect ground truth indexes\n",
        "        gt_paragraphs = tf.expand_dims(tf.cast(indexes, tf.int32), -1)\n",
        "        # A training sample is formed by the positive and m-1 negative examples\n",
        "        # obtained from the top-100 for each of the questions in the batch.\n",
        "        # We create a data batch by sampling m-1 examples from the masked 100 paragraphs\n",
        "        negative_masks = tf.math.not_equal(topm_indexes, gt_paragraphs)\n",
        "        # To keep the graph working with the correct sizes, we create a tensor of negatives \n",
        "        # by random shuffling the large tensor of topm and taking the first train_m elements.\n",
        "        # The positive examples are replaced by randomly sampling from the same tensor.\n",
        "        # It could happen that the positive example is replaced by itself, or that a \n",
        "        # negative sample appears twice in the batch, but it's a non-deterministic\n",
        "        # process.\n",
        "        negatives = self.tf_shuffle_on_columns(tf.where(\n",
        "            negative_masks, topm_indexes, self.tf_shuffle_on_columns(topm_indexes))\n",
        "        )[:,:M-1]\n",
        "        # We concatenate the positive paragraph index to the selected negatives and shuffle\n",
        "        # so that the positive is not always the last element\n",
        "        data_batch = self.tf_shuffle_on_columns(\n",
        "            tf.concat([negatives, gt_paragraphs], axis=1))\n",
        "        # When we have a data batch, we create the ground truth mask, which represents the position\n",
        "        # of the positive sample in the data batch in a one-hot encoded fashion.\n",
        "        gt_mask = tf.cast(data_batch == gt_paragraphs, tf.int32)\n",
        "        return data_batch, gt_mask\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Get top indexes from the DPR\n",
        "        _, topm_indexes = self.dpr_model(data)\n",
        "        # Obtain the training batch and the ground truth mask\n",
        "        data_batch, gt_mask = self.obtain_training_info(\n",
        "            data['paragraphs']['index'], topm_indexes)\n",
        "        # Open the gradient tape, obtain predictions and compute the loss\n",
        "        with tf.GradientTape() as tape:\n",
        "            _, _, out_S, out_E, out_SEL = self(data_batch, training=True)\n",
        "            loss_start = self.compiled_loss(data['answers']['out_s'], out_S)\n",
        "            loss_end = self.compiled_loss(data['answers']['out_e'], out_E)\n",
        "            loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "            loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        # Compute the gradients and apply them on the variables\n",
        "        grads = tape.gradient(loss_value, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        # Update the metrics\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        start_acc_metric.update_state(data['answers']['out_s'], out_S)\n",
        "        end_acc_metric.update_state(data['answers']['out_e'], out_E)\n",
        "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        return {\"loss\": loss_tracker.result(), \n",
        "                \"start_accuracy\": start_acc_metric.result(),\n",
        "                \"end_accuracy\": end_acc_metric.result(),\n",
        "                \"sel_accuracy\": sel_acc_metric.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Get top indexes from the DPR\n",
        "        _, topm_indexes = self.dpr_model(data)\n",
        "        # Obtain the training batch and the ground truth mask to compute metrics\n",
        "        data_batch, gt_mask = self.obtain_training_info(\n",
        "            data['paragraphs']['index'], topm_indexes)\n",
        "        # Compute predictions\n",
        "        _, _, out_S, out_E, out_SEL = self(data_batch, training=False)\n",
        "        # Compute the loss to update its metric\n",
        "        loss_start = self.compiled_loss(data['answers']['out_s'], out_S)\n",
        "        loss_end = self.compiled_loss(data['answers']['out_e'], out_E)\n",
        "        loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "        loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        # Updates the metrics\n",
        "        start_acc_metric.update_state(data['answers']['out_s'], out_S)\n",
        "        end_acc_metric.update_state(data['answers']['out_e'], out_E)\n",
        "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        return {\"loss\": loss_tracker.result(), \n",
        "                \"start_accuracy\": start_acc_metric.result(),\n",
        "                \"end_accuracy\": end_acc_metric.result(),\n",
        "                \"sel_accuracy\": sel_acc_metric.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        return [loss_tracker, start_acc_metric, end_acc_metric, sel_acc_metric]\n",
        "\n",
        "def create_trainable_QA_model(model_q, model_p):\n",
        "    print(\"Creating Question Answering model...\")\n",
        "    model = QuestionAnsweringModel(model_q, model_p)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model (metrics are defined into the model)\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss = keras.losses.CategoricalCrossentropy()\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of test data to build the model\n",
        "    inputs = next(dataset_train.take(1).as_numpy_iterator())\n",
        "    _, topm_indexes = model.dpr_model(inputs)\n",
        "    paragraphs, best_scoring_paragraphs, probs_sel, probs_s, probs_e = model(topm_indexes)\n",
        "    \n",
        "    print(\"Output shapes:\")\n",
        "    print(f\"\\tparagraphs: {paragraphs.keys()}\")\n",
        "    print(f\"\\tbest_scoring_paragraphs: {best_scoring_paragraphs.shape}\")\n",
        "    print(f\"\\tprobs_sel: {probs_sel.shape}\")\n",
        "    print(f\"\\tprobs_s: {probs_s.shape}\")\n",
        "    print(f\"\\tprobs_e: {probs_e.shape}\")\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3iWkkQK8zhv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        model_QA = create_trainable_QA_model(model.enc.model_q, model.enc.model_p)\n",
        "\n",
        "    # Workaraound for saving locally when using cloud TPUs\n",
        "    local_device_option = tf.train.CheckpointOptions(\n",
        "        experimental_io_device=\"/job:localhost\")\n",
        "else:\n",
        "    # GPUs and local systems don't need the above specifications. We simply\n",
        "    # create a pattern for the filename and let the callbacks deal with it.\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.ckpt\")\n",
        "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        \"training_qa_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    model_QA = create_trainable_QA_model(model.enc.model_q, model.enc.model_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RLqioFsIon",
        "outputId": "dd938727-727a-4ef2-f2c3-6babf79550db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Question Answering model...\n",
            "Compiling...\n",
            "Testing on some data...\n",
            "Output shapes:\n",
            "\tparagraphs: dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'indexes'])\n",
            "\tbest_scoring_paragraphs: (8, 3, 512, 768)\n",
            "\tprobs_sel: (8, 512)\n",
            "\tprobs_s: (8, 512)\n",
            "\tprobs_e: (8, 3)\n",
            "Model: \"question_answering_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_passage_retriever (De  multiple                 66362880  \n",
            " nsePassageRetriever)                                            \n",
            "                                                                 \n",
            " model_1 (Functional)        [{'input_ids': (None, 3,  66365187  \n",
            "                              512),                              \n",
            "                              'attention_mask': (None            \n",
            "                             , 3, 512),                          \n",
            "                              'offset_mapping': (None            \n",
            "                             , 3, 512, 2),                       \n",
            "                              'indexes': (None, 3)},             \n",
            "                              (None, 3, 512, 768),               \n",
            "                              None, None, (None, 3)]             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,365,187\n",
            "Trainable params: 2,307\n",
            "Non-trainable params: 66,362,880\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model:"
      ],
      "metadata": {
        "id": "ffh1nGX0I9FT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "QA_BATCH_SIZE = 4 if not using_TPU else 32\n",
        "\n",
        "dataset_train = create_dataset_from_records(train_questions, paragraphs, train_dict, tokenizer_distilbert, \n",
        "                                            os.path.join(datasets_dir, 'train'), batch_size=QA_BATCH_SIZE)\n",
        "dataset_val = create_dataset_from_records(val_questions, paragraphs, val_dict, tokenizer_distilbert,\n",
        "                                            os.path.join(datasets_dir, 'val'), training=False, batch_size=QA_BATCH_SIZE)\n",
        "\n",
        "if DO_QA_TRAINING:\n",
        "    # Early stopping can be used by both hardware\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    callbacks = [es_callback]\n",
        "    if not using_TPU:\n",
        "        # ModelCheckpoint callback is only available when not using TPU\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath = checkpoint_path,\n",
        "            verbose=1,\n",
        "            save_weights_only = True,\n",
        "            save_best_only = False\n",
        "        )\n",
        "        # Same for tensorboard callback\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        )\n",
        "        # Save the first weights using the pattern from before\n",
        "        model_QA.save_weights(checkpoint_path.format(epoch=0))\n",
        "        # These callback imply saving stuff on local disk, which cannot be \n",
        "        # done automatically using TPUs.\n",
        "        # Therefore, they are only active when using GPUs and local systems\n",
        "        callbacks.extend([cp_callback, tensorboard_callback])\n",
        "    else:\n",
        "        # Save first weights in a h5 file (it's the most stable way)\n",
        "        model_QA.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_qa_tpu_0.h5'), overwrite=True)        \n",
        "\n",
        "    # We fit the model\n",
        "    history = model_QA.fit(\n",
        "        dataset_train, \n",
        "        y=None,\n",
        "        validation_data=dataset_val,\n",
        "        epochs=EPOCHS, \n",
        "        callbacks=callbacks,\n",
        "        shuffle=True,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=0,\n",
        "        verbose=1 # Show progress bar\n",
        "    )\n",
        "\n",
        "    if using_TPU:\n",
        "        # Save last weights\n",
        "        model_qa.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_qa_tpu_last.h5'), overwrite=True)"
      ],
      "metadata": {
        "id": "UmsAi9v63eia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtaining an answer"
      ],
      "metadata": {
        "id": "-I79iOHjGbE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def start_end_token_from_probabilities(\n",
        "    pstartv: np.array, \n",
        "    pendv: np.array, \n",
        "    dim:int=512) -> List[List[int]]:\n",
        "    '''\n",
        "    Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
        "    '''\n",
        "    idxs = []\n",
        "    for i in range(pstartv.shape[0]):\n",
        "        # For each element in the batch, transform the vectors into matrices\n",
        "        # by repeating them dim times:\n",
        "        # - Vectors of starting probabilities are stacked on the columns\n",
        "        pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
        "        # - Vectors of ending probabilities are repeated on the rows\n",
        "        pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
        "        # Once we have the two matrices, we sum them (element-wise operation)\n",
        "        # to obtain the scores of each combination\n",
        "        sums = pstart + pend\n",
        "        # We only care about the scores in the upper triangular part of the matrix\n",
        "        # (where the ending index is greater than the starting index)\n",
        "        # therefore we zero out the diagonal and the lower triangular area\n",
        "        sums = np.triu(sums, k=1)\n",
        "        # The most probable set of tokens is the one with highest score in the\n",
        "        # remaining matrix. Through argmax we obtain its position.\n",
        "        val = np.argmax(sums)\n",
        "        # Since the starting probabilities are repeated on the columns, each element\n",
        "        # is identified by the row. Ending probabilities are instead repeated on rows,\n",
        "        # so each element is identified by the column.\n",
        "        row = val // dim\n",
        "        col = val - dim*row\n",
        "        idxs.append([row,col])\n",
        "    return idxs"
      ],
      "metadata": {
        "id": "CWUaL844hyf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_start_end = start_end_token_from_probabilities(probs_s, probs_e)\n",
        "print(answers_start_end)"
      ],
      "metadata": {
        "id": "8CUVf45Cj0oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can obtain the answers to the questions we have given the network."
      ],
      "metadata": {
        "id": "xVfgDsb_GHIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_indices = tf.squeeze(tf.gather(paragraphs['indexes'], tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "best_offsets = tf.squeeze(tf.gather(pretokenized_val_paragraphs['offset_mapping'], best_indices))\n",
        "best_offsets.shape, best_indices.shape"
      ],
      "metadata": {
        "id": "CcpI3LVz2vKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_start_end = [(best_offsets[i][answers_start_end[i][0]][0].numpy(),\n",
        "                   best_offsets[i][answers_start_end[i][1]][1].numpy())\n",
        "                 for i in range(BATCH_SIZE)]\n",
        "char_start_end"
      ],
      "metadata": {
        "id": "FuWtARzUITEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correction for answers arriving to the end of the sequence\n",
        "for i in range(BATCH_SIZE):\n",
        "    c = char_start_end[i]\n",
        "    if c[1] >= c[0]:\n",
        "        char_start_end[i] = (c[0], c[1])\n",
        "    else:\n",
        "        char_start_end[i] = (c[0], 1000000)"
      ],
      "metadata": {
        "id": "ktvdpz52fxOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, p in enumerate(best_indices.numpy()):\n",
        "    print(val_paragraphs[p]['context'][char_start_end[i][0]:char_start_end[i][1]])"
      ],
      "metadata": {
        "id": "6edjxYTw4k-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, answers are extremely bad because we need to train the Dense layers selecting the start and end tokens, as well as the paragraph selector."
      ],
      "metadata": {
        "id": "V_B5U-_tKR6v"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "dense_passage_retriever.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "8d3a38b9baf6bccb52d534d3795fadb5d3190627f4e5187a36a9129f48a6e143"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "orig_nbformat": 4,
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d922006c7beb461db6ca5f09837f100c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c046fdc704844307a24bf63a90106a39",
              "IPY_MODEL_dd4b7a42411c4468bd3a286359d87898",
              "IPY_MODEL_718c8209da524e299d7f7217efeebae1"
            ],
            "layout": "IPY_MODEL_d9a7f773c79841e09b23a70f3dce5f3d"
          }
        },
        "c046fdc704844307a24bf63a90106a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2acb3dc386044c9196ec6b2169607883",
            "placeholder": "​",
            "style": "IPY_MODEL_7cdb6d43aef34b4d8637315681fd87fa",
            "value": "Downloading: 100%"
          }
        },
        "dd4b7a42411c4468bd3a286359d87898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2050a44993f24997920d540e1fe20c29",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4767ff8f74b046d6b596718b5c3cbaf9",
            "value": 28
          }
        },
        "718c8209da524e299d7f7217efeebae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff5782c53edb41719efe7be4c9c9163f",
            "placeholder": "​",
            "style": "IPY_MODEL_af011dde1eb14a27aee289ce846230b0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 664B/s]"
          }
        },
        "d9a7f773c79841e09b23a70f3dce5f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acb3dc386044c9196ec6b2169607883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdb6d43aef34b4d8637315681fd87fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2050a44993f24997920d540e1fe20c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4767ff8f74b046d6b596718b5c3cbaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff5782c53edb41719efe7be4c9c9163f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af011dde1eb14a27aee289ce846230b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "251ca56eb0a34797a4cdbc03cd69a4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_731db3a2ea1c46d38da3877b3e06380e",
              "IPY_MODEL_bab2ea94301d4877ad320124f2417390",
              "IPY_MODEL_6b7105f611db4731a002027f515dd3de"
            ],
            "layout": "IPY_MODEL_cfa23da7945145d08fe253e4504d68b8"
          }
        },
        "731db3a2ea1c46d38da3877b3e06380e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e7f79c0f3044452b1c7914d18b4a0ad",
            "placeholder": "​",
            "style": "IPY_MODEL_6a59c67f1e7a49bdb04de7696e371cb3",
            "value": "Downloading: 100%"
          }
        },
        "bab2ea94301d4877ad320124f2417390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd83641538d24610969ab498a7d2fa9b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca80801d21504a6da8a81189ef352ab5",
            "value": 231508
          }
        },
        "6b7105f611db4731a002027f515dd3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89a3f8ab7a784f82848622295c35d103",
            "placeholder": "​",
            "style": "IPY_MODEL_6b5184d0bced4fb0bb6c683a15e8c72a",
            "value": " 226k/226k [00:00&lt;00:00, 1.02MB/s]"
          }
        },
        "cfa23da7945145d08fe253e4504d68b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e7f79c0f3044452b1c7914d18b4a0ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a59c67f1e7a49bdb04de7696e371cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd83641538d24610969ab498a7d2fa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca80801d21504a6da8a81189ef352ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89a3f8ab7a784f82848622295c35d103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5184d0bced4fb0bb6c683a15e8c72a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97811a5510e04d6f80f78349af33104a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09f2b409bafd4b1ea541d4eb6c9eb90a",
              "IPY_MODEL_fb0b949573c24eb7898efba6547375d6",
              "IPY_MODEL_abf69e4d64c74e06b940fe876cacb620"
            ],
            "layout": "IPY_MODEL_8a560e0afb894e86ae50d0073d807a39"
          }
        },
        "09f2b409bafd4b1ea541d4eb6c9eb90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9bb21fd93654d5e8347b78a71b6240e",
            "placeholder": "​",
            "style": "IPY_MODEL_de90f7a1d9304dcc968031e2577cc5f6",
            "value": "Downloading: 100%"
          }
        },
        "fb0b949573c24eb7898efba6547375d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7842392242d747c28ff250c1f480ea81",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb6fc8935a1a4d0b9c96b8e53fd4def1",
            "value": 466062
          }
        },
        "abf69e4d64c74e06b940fe876cacb620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc1fe162d494cab9093e11515003370",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4de7dd902a4eb28583185a2ead2087",
            "value": " 455k/455k [00:00&lt;00:00, 1.47MB/s]"
          }
        },
        "8a560e0afb894e86ae50d0073d807a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9bb21fd93654d5e8347b78a71b6240e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de90f7a1d9304dcc968031e2577cc5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7842392242d747c28ff250c1f480ea81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb6fc8935a1a4d0b9c96b8e53fd4def1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adc1fe162d494cab9093e11515003370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4de7dd902a4eb28583185a2ead2087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d09df23fc714d8db8b9bbf0e16bb8dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dee67fe311042ba93397b589496a249",
              "IPY_MODEL_ada87120c122422db11b1dca822a148a",
              "IPY_MODEL_a5b7e24fad334b489e0f94026c8b6124"
            ],
            "layout": "IPY_MODEL_9fe366dff9ea48d4b75636ed0a046007"
          }
        },
        "1dee67fe311042ba93397b589496a249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46fbd4324ea541dcbbd8d7099c6a2855",
            "placeholder": "​",
            "style": "IPY_MODEL_e62ef7d4513f4360b802602bdd549a38",
            "value": "Downloading: 100%"
          }
        },
        "ada87120c122422db11b1dca822a148a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0924f87b11a746ddb47ab7c04ff6a00d",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e77f0e4ff90643c6b81309f28c1e736f",
            "value": 483
          }
        },
        "a5b7e24fad334b489e0f94026c8b6124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b52231374aa240c6a5f007d70bd7a854",
            "placeholder": "​",
            "style": "IPY_MODEL_b94a57eb641d476890c126cc34fa2549",
            "value": " 483/483 [00:00&lt;00:00, 8.16kB/s]"
          }
        },
        "9fe366dff9ea48d4b75636ed0a046007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46fbd4324ea541dcbbd8d7099c6a2855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62ef7d4513f4360b802602bdd549a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0924f87b11a746ddb47ab7c04ff6a00d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e77f0e4ff90643c6b81309f28c1e736f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b52231374aa240c6a5f007d70bd7a854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b94a57eb641d476890c126cc34fa2549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52fd40dc559647508ae234f56eb7c2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6a8d1a6b30a44a189c22b6512a124e8",
              "IPY_MODEL_90346f08361947f9b3d9a02e57699f71",
              "IPY_MODEL_cd90708906dd44e6813b8d247047696d"
            ],
            "layout": "IPY_MODEL_38f584f7b21d4632a58fe8a76fdbe8fb"
          }
        },
        "f6a8d1a6b30a44a189c22b6512a124e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbc605143d284f609db16f7b5b91e7f7",
            "placeholder": "​",
            "style": "IPY_MODEL_141bbc8726de412393f276e42cddc98a",
            "value": "Downloading: 100%"
          }
        },
        "90346f08361947f9b3d9a02e57699f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6acaf5533ad44a797f2441a223598ee",
            "max": 363423424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0eec18d9434245fdba77dbcc33bda84d",
            "value": 363423424
          }
        },
        "cd90708906dd44e6813b8d247047696d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aa715c564a84e7281afdc822bf04d94",
            "placeholder": "​",
            "style": "IPY_MODEL_0f7a56c2a52c48c6b4a045f087833201",
            "value": " 347M/347M [00:07&lt;00:00, 51.4MB/s]"
          }
        },
        "38f584f7b21d4632a58fe8a76fdbe8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc605143d284f609db16f7b5b91e7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "141bbc8726de412393f276e42cddc98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6acaf5533ad44a797f2441a223598ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eec18d9434245fdba77dbcc33bda84d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aa715c564a84e7281afdc822bf04d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f7a56c2a52c48c6b4a045f087833201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}