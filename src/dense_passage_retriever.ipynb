{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'\n",
    "\n",
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !git clone https://www.github.com/{username}/{repository}.git\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd /content/QuestionAnswering/src\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
    "\n",
    "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
    "\n",
    "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
    "\n",
    "In the Tf-Idf example, the retriever was simply a function that returned the top 5 scores obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
    "\n",
    "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
    "\n",
    "At run-time, another Dense Encoder is used $E_Q$ which maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
    "\n",
    "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
    "\n",
    "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
    "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
    "\n",
    "The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI. At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
    "\n",
    "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
    "\n",
    "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
    "\n",
    "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
    "- Random: a negative is any random passage in the corpus\n",
    "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
    "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
    "\n",
    "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pais where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, TFBertModel\n",
    "import utils\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"Configuration!\")\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Opening dataset and collecting questions and paragraphs...\")\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
    "paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)\n",
    "\n",
    "questions = [{\n",
    "        'qas': qas,\n",
    "        'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
    "    }\n",
    "    for i in range(len(paragraphs_and_questions['data']))\n",
    "    for j, para in enumerate(paragraphs_and_questions['data'][i]['paragraphs'])\n",
    "    for qas in para['qas']\n",
    "]\n",
    "\n",
    "paragraphs = [{\n",
    "        'context': para['context'],\n",
    "        'context_id': i\n",
    "    }\n",
    "    for i in range(len(paragraphs_and_questions['data']))\n",
    "    for para in paragraphs_and_questions['data'][i]['paragraphs']\n",
    "]\n",
    "\n",
    "print(\"Instantiating the two models and tokenizer...\")\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_q, model_p = TFBertModel.from_pretrained('bert-base-uncased'), TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test_question = questions[0]['qas']['question']\n",
    "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
    "inputs_test = tokenizer_bert(test_question, return_tensors=\"tf\")\n",
    "outputs = model_q(inputs_test)\n",
    "\n",
    "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "test_q_repr = last_hidden_states[0,0,:]\n",
    "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_from_question(qas, dataset):\n",
    "    i,j = qas['context_id']\n",
    "    return dataset['data'][i]['paragraphs'][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# As always, it's impossible to keep this whole thing on RAM\n",
    "# dataset = tf.data.Dataset.from_tensor_slices([{\n",
    "#     'question': tokenizer_bert(questions[i]['qas']['question'], \n",
    "#                                 return_tensors='tf', max_length = 512, \n",
    "#                                 truncation = True),\n",
    "#     'paragraph': tokenizer_bert(get_paragraph_from_question(\n",
    "#                                     questions[i], paragraphs_and_questions\n",
    "#                                 )['context'], \n",
    "#                                 return_tensors='tf', max_length = 512, \n",
    "#                                 truncation = True)\n",
    "# } for i in tqdm(range(len(questions)))]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d3a38b9baf6bccb52d534d3795fadb5d3190627f4e5187a36a9129f48a6e143"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
