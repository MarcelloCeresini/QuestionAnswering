{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oZkaIoI1esj",
        "outputId": "31c31ba8-be32-4668-f49c-86790b72aaa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "fatal: destination path 'QuestionAnswering' already exists and is not an empty directory.\n",
            "/content/QuestionAnswering/src\n"
          ]
        }
      ],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXDnbCt1ess"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tIdZRSl1esu"
      },
      "source": [
        "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
        "\n",
        "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
        "\n",
        "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
        "\n",
        "In the Tf-Idf example, the retriever was simply a function that returned the top 5 scores obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
        "\n",
        "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
        "\n",
        "At run-time, another Dense Encoder is used $E_Q$ which maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
        "\n",
        "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
        "\n",
        "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
        "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
        "\n",
        "The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI. At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
        "\n",
        "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
        "\n",
        "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
        "\n",
        "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
        "- Random: a negative is any random passage in the corpus\n",
        "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
        "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
        "\n",
        "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pais where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1cbEuy1es4"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm45AN-fDEiv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8CBcRCvCCpzg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, DistilBertTokenizer, \\\n",
        "                         TFBertModel, TFDistilBertModel\n",
        "import utils\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "BERT_DIMENSIONALITY = 768\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Ih1ZqSDCmW"
      },
      "source": [
        "### TPU check\n",
        "The training could be made faster if we use the cloud GPUs offered by Google on Google Colab. Since TPUs require manual intialization and other oddities, we check multiple times throughout the notebook what kind of hardware we are running the code on.\n",
        "using_TPU = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyt5e89MCm4X",
        "outputId": "5345c622-1c7b-4d18-b738-c99e64f18e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TPUs are not available, setting flag 'using_TPU' to False.\n"
          ]
        }
      ],
      "source": [
        "if using_TPU:\n",
        "    try: \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
        "        using_TPU = False\n",
        "else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv3n12tDIAo"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8piWcupDOPR"
      },
      "source": [
        "We define all the paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "On3I2PUgCwNF"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLC6xwfKDSAN"
      },
      "source": [
        "We collect the training and validation questions and paragraphs into different lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1zdDCsLZDMkm"
      },
      "outputs": [],
      "source": [
        "train_dict = utils.read_question_set(TRAINING_FILE)\n",
        "val_dict = utils.read_question_set(VALIDATION_FILE)\n",
        "\n",
        "def get_questions_and_paragraphs_from_dataset(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset['data']))\n",
        "        for j, para in enumerate(dataset['data'][i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset['data']))\n",
        "        for para in dataset['data'][i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs_from_dataset(train_dict)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs_from_dataset(val_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvGAqUBvDbIg"
      },
      "source": [
        "We create the two different DistilBert models for encoding and test them on a random question/paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XI6YUBZDit8",
        "outputId": "36467936-a9cf-4c42-b5b8-01bccbf785d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model_q, model_p = TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                   TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcCbC-Vi1es5",
        "outputId": "427d1db5-2ac1-4465-db34-77db8815ae3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing on a simple question. \n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Representation dimensionality: (768,)\n"
          ]
        }
      ],
      "source": [
        "test_question = train_questions[0]['qas']['question']\n",
        "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
        "inputs_test = tokenizer_distilbert(test_question, return_tensors=\"tf\")\n",
        "outputs = model_q(inputs_test)\n",
        "\n",
        "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
        "last_hidden_states = outputs.last_hidden_state\n",
        "test_q_repr = last_hidden_states[0,0,:]\n",
        "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_lPiEZu1es9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7qavEm61es-"
      },
      "source": [
        "First of all, we need to train our models. To do that, we need to create a dataset that feeds batches of questions and positive and negative paragraphs to a model, which is used to compute the representations, then the similarities and to correct the learnt distributions from the encoder models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCYkx8rf1es_"
      },
      "source": [
        "## Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynvz3Gw1etA"
      },
      "source": [
        "For the dataset, we use the `keras.utils.Sequence` object, which is a high-level multi-processing-ready data generator that we can use to generate full batches consisting of the tokenized question, as well as positive and negative paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OaBWVf5k1etB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import List, Union\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_paragraph_from_question(qas, dataset):\n",
        "    i,j = qas['context_id']\n",
        "    return dataset['data'][i]['paragraphs'][j]\n",
        "\n",
        "class TrainingDataset(Sequence):\n",
        "\n",
        "    def __init__(self, questions, dataset, tokenizer, batch_size=BATCH_SIZE):\n",
        "        self.questions = questions\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.questions) / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, _):\n",
        "        # Each batch is made of batch_size questions. Each question\n",
        "        # is paired with a positive and batch_size-1 negatives.\n",
        "        batch = []\n",
        "        # Randomly choose a batch of questions\n",
        "        questions_in = np.random.choice(self.questions, size=self.batch_size, replace=False)\n",
        "        # Obtain the respective paragraphs\n",
        "        paragraphs_in = [get_paragraph_from_question(q, self.dataset)['context'] \n",
        "                                  for q in questions_in]\n",
        "        # Then we obtain the tokenizer representation of the paragraphs\n",
        "        paragraphs_in = [dict(self.tokenizer(paragraphs_in[i], \n",
        "                    return_tensors='tf', max_length = 512, \n",
        "                    truncation = True, padding = 'max_length'))\n",
        "                    for i in range(len(paragraphs_in))]\n",
        "        # Tokenize the questions\n",
        "        qs = [dict(self.tokenizer(questions_in[i]['qas']['question'], \n",
        "                return_tensors='tf', max_length = 512, \n",
        "                truncation = True, padding = 'max_length')) for i in range(self.batch_size)]\n",
        "        # Populate the batch    \n",
        "        batch = [\n",
        "            qs,                   # 0: The encoded questions\n",
        "            paragraphs_in,        # 1: The encoded positive paragraphs\n",
        "            # The negative paragraphs are the other paragraphs in the batch\n",
        "        ]\n",
        "        # Create labels\n",
        "        labels = np.arange(BATCH_SIZE)\n",
        "        \n",
        "        return (batch, labels) # Data in form (x, y)\n",
        "\n",
        "if not using_TPU:\n",
        "    dataset_train = TrainingDataset(train_questions, train_dict, tokenizer_distilbert)\n",
        "    dataset_val = TrainingDataset(val_questions, val_dict, tokenizer_distilbert)\n",
        "else:\n",
        "    pass\n",
        "    # TODO\n",
        "    # dataset_train = tf.data.Dataset.from_tensor_slices([([\n",
        "    #     dict(tokenizer_distilbert(train_questions[i]['qas']['question'], \n",
        "    #         return_tensors='tf', max_length = 512, \n",
        "    #         padding = 'max_length', truncation = True)),\n",
        "    #     dict(tokenizer_distilbert(get_paragraph_from_question(\n",
        "    #             train_questions[i], train_dict)['context'], \n",
        "    #         return_tensors='tf', max_length = 512, \n",
        "    #         padding = 'max_length', truncation = True))\n",
        "    # ], ) for i in tqdm(range(BATCH_SIZE))]).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAnwbiFU1etF"
      },
      "source": [
        "## Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lwz19a1etG"
      },
      "source": [
        "First of all, we need a layer that takes as input the dictionary containing the tokenized questions and answers and returns their compact representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjPUMwKk1etG",
        "outputId": "003d9e7c-490e-4c16-9e32-b8e2981019d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape when in training mode: (16, 768), (16, 768)\n",
            "Output shape when in testing mode: (16, 768)\n",
            "Output shape when dealing with a single question: (1, 768)\n"
          ]
        }
      ],
      "source": [
        "class DenseEncoder(layers.Layer):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.model_q = model_q  # Dense encoder for questions\n",
        "        self.model_p = model_p  # Dense encoder for paragraphs\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            # Input contains the questions and paragraphs encoding\n",
        "            qs, ps = inputs\n",
        "            # We obtain the representation of the question with the first model\n",
        "            q_repr = tf.reshape(tf.stack([self.model_q(qs[i]).last_hidden_state[:,0,:] \n",
        "                 for i in range(len(qs))]), (len(qs), BERT_DIMENSIONALITY))\n",
        "            # We also encode the paragraphs.\n",
        "            p_repr = tf.reshape(tf.stack([self.model_p(ps[i]).last_hidden_state[:,0,:] \n",
        "                 for i in range(len(ps))]), (len(ps), BERT_DIMENSIONALITY))\n",
        "            return q_repr, p_repr\n",
        "        else:\n",
        "            # We check if the input is a list [we have [qs, ps] or just qs]\n",
        "            qs = inputs[0] if isinstance(inputs, list) else inputs\n",
        "            # If qs is a list (a batch of encoded questions), process the whole batch\n",
        "            if isinstance(qs, list) and len(qs) > 1:\n",
        "                # We obtain the representation of the question with the first model\n",
        "                q_repr = tf.reshape(tf.stack([self.model_q(qs[i]).last_hidden_state[:,0,:] \n",
        "                    for i in range(len(qs))]), (len(qs), BERT_DIMENSIONALITY))\n",
        "            else:\n",
        "                # If qs is a single element (for testing), process it singularly\n",
        "                q_repr = model_q(qs).last_hidden_state[:,0,:]\n",
        "            return q_repr\n",
        "\n",
        "# Small test for the layer\n",
        "class TestDenseEncoderModel(keras.Model):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs[0]\n",
        "        return self.enc(x, training=training)\n",
        "\n",
        "test_model = TestDenseEncoderModel(model_q, model_p)\n",
        "q_repr, p_repr = test_model(dataset_train[0], training=True)\n",
        "print(f\"Output shape when in training mode: {q_repr.shape}, {p_repr.shape}\")\n",
        "q_repr_2 = test_model(dataset_train[0], training=False)\n",
        "print(f\"Output shape when in testing mode: {q_repr_2.shape}\")\n",
        "q = dataset_train[0][0][0][0]\n",
        "q_repr_3 = test_model((q,None), training=False)\n",
        "print(f\"Output shape when dealing with a single question: {q_repr_3.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4r1MfHP1etI"
      },
      "source": [
        "Once we have the representations, we should compute the similarities, thus obtaining a a full mini-batch of positive-negative examples. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdG5bkWV2FU5",
        "outputId": "e3ca87bd-9402-469e-8433-28354596e084"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([16, 16])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the similarity matrix\n",
        "S = tf.tensordot(q_repr, tf.transpose(p_repr), axes=1)\n",
        "S.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEPy1ug1__n0"
      },
      "source": [
        "This similarity matrix has the following meaning:\n",
        "- Rows represent questions.\n",
        "- Each row contains the similarity that the respective question has with the 16 paragraphs (one of them is the positive one, the others are negative)\n",
        "\n",
        "In the paper, they refer to the loss as a *minimization of the negative log-likelihood of the positive passage*: what it really means is that we need to transform similarities to probabilities and use a categorical cross-entropy loss, where labels are the row index (which is also the column index in that row for the positive passage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_aJezqWCKsw",
        "outputId": "9469abcb-4adc-49e3-a630-bf749e61eb49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.7374897"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True\n",
        ")\n",
        "loss(y_true=tf.range(BATCH_SIZE), y_pred=S).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnidWQLZDwsf"
      },
      "source": [
        "The loss seems to be quite high for this batch. We can study it with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Vdgc2-gQD1PP",
        "outputId": "076c65e8-edfe-410d-c866-1e2585febda9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5560c98fd0>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "S_arr = S.numpy()\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQwdzlTfEsH4"
      },
      "source": [
        "Indeed, ideally the predictions should be on the diagonal. This means that the \"default\" space for this metric learning problem is not that good. We are ready to learn a new representation distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOLMK5mIMPc"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nme0koNHE76t"
      },
      "outputs": [],
      "source": [
        "class DensePassageRetriever(keras.Model):\n",
        "\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        '''\n",
        "        `inputs` contains the \"x\" of the batch: the list of questions and paragraphs\n",
        "        '''\n",
        "        if training:\n",
        "            # For training we return the similarity matrix\n",
        "            repr_q, repr_p = self.enc(inputs, training=training)\n",
        "            S = tf.tensordot(repr_q, tf.transpose(repr_p), axes=1)\n",
        "            return S\n",
        "        else:\n",
        "            # In other cases, we return the representation of the question(s)\n",
        "            repr_q = self.enc(inputs, training=training)            \n",
        "            return repr_q\n",
        "\n",
        "    # def train_step(self, data):\n",
        "    #     x, y = data\n",
        "    #     with tf.GradientTape() as tape:\n",
        "    #         # Obtain similarities\n",
        "    #         S = self(x, training=True)\n",
        "    #         # Obtain loss value\n",
        "    #         loss = self.compiled_loss(y, S)\n",
        "    #     # Construct gradients and apply them through the optimizer\n",
        "    #     gradients = tape.gradient(loss, self.trainable_variables)\n",
        "    #     self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "    #     # Update and return metrics (specifically the one for the loss value).\n",
        "    #     self.compiled_metrics.update_state(y, S)\n",
        "    #     return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "def create_model(sample=dataset_train[0][0], freeze_layers_up_to=3):\n",
        "    print(\"Creating BERT models...\")\n",
        "    model_q, model_p =  TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                        TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Freeze layers \n",
        "    for i in range(freeze_layers_up_to): # layers 0 to variable are frozen, successive layers learn\n",
        "        model_q.distilbert.transformer.layer[i].trainable = False\n",
        "        model_p.distilbert.transformer.layer[i].trainable = False\n",
        "    \n",
        "    print(\"Creating Dense Passage Retriever...\")\n",
        "    model = DensePassageRetriever(model_q, model_p)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model and loss\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of data to build the model\n",
        "    model(sample)\n",
        "\n",
        "    # Return the model\n",
        "    print(\"Model created!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA_dwB6MIHaq"
      },
      "source": [
        "## Training procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdxw7F4IWVi"
      },
      "source": [
        "Define utility variables and saving paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "s838Rj9GIUA9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "PATIENCE = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlqWpbk5IV_f",
        "outputId": "72858dd6-a416-49ef-fe40-2072ee101b31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    checkpoint_dir = '/content/drive/My Drive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "else:\n",
        "    # Create the folder where we'll save the weights of the model\n",
        "    checkpoint_dir = os.path.join(ROOT_PATH, \"data\", \"training_dpr\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrJEqCQIZ3c"
      },
      "source": [
        "Check if we're using a TPU, in order to create the model within the scope of the strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN6WxrL-IRjj"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "\n",
        "    # Workaraound for saving locally when using cloud TPUs\n",
        "    local_device_option = tf.train.CheckpointOptions(\n",
        "        experimental_io_device=\"/job:localhost\")\n",
        "else:\n",
        "    # GPUs and local systems don't need the above specifications. We simply\n",
        "    # create a pattern for the filename and let the callbacks deal with it.\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.ckpt\")\n",
        "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        \"training_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    model = create_model()\n",
        "\n",
        "    # ModelCheckpoint callback is only available when not using TPU\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = checkpoint_path,\n",
        "        verbose=1,\n",
        "        save_weights_only = True,\n",
        "        save_best_only = False\n",
        "    )\n",
        "\n",
        "    # Same for tensorboard callback\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir,\n",
        "        histogram_freq=1\n",
        "    )\n",
        "\n",
        "# Early stopping can be used by both hardware\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience = PATIENCE,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "if using_TPU:\n",
        "    # Save first weights in a h5 file (it's the most stable way)\n",
        "    model.save_weights(os.path.join(\n",
        "        checkpoint_dir, 'training_normal_tpu_0.h5'),  overwrite=True)\n",
        "else:\n",
        "    # Save the first weights using the pattern from before\n",
        "    model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "callbacks = [es_callback]\n",
        "if not using_TPU:\n",
        "    # These callback imply saving stuff on local disk, which cannot be \n",
        "    # done automatically using TPUs.\n",
        "    # Therefore, they are only active when using GPUs and local systems\n",
        "    callbacks.extend([cp_callback, tensorboard_callback])\n",
        "\n",
        "# We fit the model\n",
        "history = model.fit(\n",
        "    dataset_train, \n",
        "    y=None,\n",
        "    validation_data=dataset_val,\n",
        "    epochs=EPOCHS, \n",
        "    callbacks=callbacks,\n",
        "    use_multiprocessing=True,\n",
        "    initial_epoch=0,\n",
        "    verbose=1 # Show progress bar\n",
        ")\n",
        "\n",
        "if using_TPU:\n",
        "    # Save last weights\n",
        "    model.save_weights(os.path.join(\n",
        "        checkpoint_dir, 'training_normal_tpu_last.h5'), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giBxscVS1etI"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsiV3d_d1etJ"
      },
      "source": [
        "We pre-compute the representations of the paragraphs. We save it on disk so that we don't need to re-compute it everytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceMlD8dy1etJ"
      },
      "outputs": [],
      "source": [
        "# paragraphs_representations = np.empty(\n",
        "#     shape=(len(paragraphs), BERT_DIMENSIONALITY)\n",
        "# )\n",
        "# paragraphs_representations.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue-ZdYeK1etK"
      },
      "outputs": [],
      "source": [
        "# for i, p in enumerate(tqdm(paragraphs)):\n",
        "#     paragraphs_representations[i] = model_p(tokenizer_bert(\n",
        "#         p['context'], return_tensors='tf', max_length = 512, \n",
        "#         truncation = True, padding = 'max_length'\n",
        "#     )).last_hidden_state[0,0,:]\n",
        "# np.savetxt('paragraphs_representations.txt', paragraphs_representations, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxBcEA8s1etK"
      },
      "outputs": [],
      "source": [
        "# paragraphs_representations = np.loadtxt('paragraphs_representations.txt', delimiter=',')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dense_passage_retriever.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "8d3a38b9baf6bccb52d534d3795fadb5d3190627f4e5187a36a9129f48a6e143"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
