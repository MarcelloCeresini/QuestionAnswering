{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'\n",
    "\n",
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !git clone https://www.github.com/{username}/{repository}.git\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd /content/QuestionAnswering/src\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
    "\n",
    "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
    "\n",
    "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
    "\n",
    "In the Tf-Idf example, the retriever was simply a function that returned the top 5 scores obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
    "\n",
    "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
    "\n",
    "At run-time, another Dense Encoder is used $E_Q$ which maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
    "\n",
    "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
    "\n",
    "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
    "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
    "\n",
    "The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI. At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
    "\n",
    "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
    "\n",
    "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
    "\n",
    "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
    "- Random: a negative is any random passage in the corpus\n",
    "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
    "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
    "\n",
    "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pais where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration!\n",
      "Opening dataset and collecting questions and paragraphs...\n",
      "Instantiating the two models and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on a simple question. \n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Representation dimensionality: (768,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, DistilBertTokenizer, TFBertModel\n",
    "import utils\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(\"Configuration!\")\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Opening dataset and collecting questions and paragraphs...\")\n",
    "\n",
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
    "paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)\n",
    "\n",
    "questions = [{\n",
    "        'qas': qas,\n",
    "        'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
    "    }\n",
    "    for i in range(len(paragraphs_and_questions['data']))\n",
    "    for j, para in enumerate(paragraphs_and_questions['data'][i]['paragraphs'])\n",
    "    for qas in para['qas']\n",
    "]\n",
    "\n",
    "paragraphs = [{\n",
    "        'context': para['context'],\n",
    "        'context_id': i\n",
    "    }\n",
    "    for i in range(len(paragraphs_and_questions['data']))\n",
    "    for para in paragraphs_and_questions['data'][i]['paragraphs']\n",
    "]\n",
    "\n",
    "print(\"Instantiating the two models and tokenizer...\")\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_q, model_p = TFBertModel.from_pretrained('bert-base-uncased'), TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "test_question = questions[0]['qas']['question']\n",
    "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
    "inputs_test = tokenizer_bert(test_question, return_tensors=\"tf\")\n",
    "outputs = model_q(inputs_test)\n",
    "\n",
    "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "test_q_repr = last_hidden_states[0,0,:]\n",
    "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def get_paragraph_from_question(qas, dataset):\n",
    "    i,j = qas['context_id']\n",
    "    return dataset['data'][i]['paragraphs'][j]\n",
    "\n",
    "# As always we need to use a generator because there's no chance this whole\n",
    "# thing can be stored in RAM if we're not using Colab\n",
    "def dataset_generator(questions, dataset, tokenizer):\n",
    "    for i in range(len(questions)):\n",
    "        yield {\n",
    "            'questions': dict(tokenizer(questions[i]['qas']['question'], \n",
    "                return_tensors='tf', max_length = 512, \n",
    "                truncation = True, padding = 'max_length')),\n",
    "            'paragraphs': dict(tokenizer(get_paragraph_from_question(\n",
    "                    questions[i], dataset\n",
    "                )['context'], \n",
    "                return_tensors='tf', max_length = 512, \n",
    "                truncation = True, padding = 'max_length'))\n",
    "        }\n",
    "\n",
    "features = {\n",
    "    'input_ids': tf.TensorSpec(shape=(1,512,), dtype=tf.int32), \n",
    "    'token_type_ids': tf.TensorSpec(shape=(1,512,), dtype=tf.int32),\n",
    "    'attention_mask': tf.TensorSpec(shape=(1,512,), dtype=tf.int32)\n",
    "}\n",
    "\n",
    "signature = {\n",
    "    'questions': features,\n",
    "    'paragraphs': features\n",
    "}\n",
    "\n",
    "data_gen = partial(dataset_generator, questions, \n",
    "    paragraphs_and_questions, tokenizer_bert)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(data_gen, output_signature=signature)\n",
    "dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"dense_encoder_4\" (type DenseEncoder).\n\n'TakeDataset' object is not subscriptable\n\nCall arguments received:\n  • inputs=<TakeDataset shapes: {questions: {input_ids: (None, 1, 512), token_type_ids: (None, 1, 512), attention_mask: (None, 1, 512)}, paragraphs: {input_ids: (None, 1, 512), token_type_ids: (None, 1, 512), attention_mask: (None, 1, 512)}}, types: {questions: {input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, paragraphs: {input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}}>\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20196/1041823924.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDenseEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0menc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20196/1041823924.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'questions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'paragraphs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# 1) The questions and paragraphs are passed into the BERT models to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"dense_encoder_4\" (type DenseEncoder).\n\n'TakeDataset' object is not subscriptable\n\nCall arguments received:\n  • inputs=<TakeDataset shapes: {questions: {input_ids: (None, 1, 512), token_type_ids: (None, 1, 512), attention_mask: (None, 1, 512)}, paragraphs: {input_ids: (None, 1, 512), token_type_ids: (None, 1, 512), attention_mask: (None, 1, 512)}}, types: {questions: {input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, paragraphs: {input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}}>\n  • training=False"
     ]
    }
   ],
   "source": [
    "class DenseEncoder(keras.Model):\n",
    "    def __init__(self, model_q, model_p):\n",
    "        super().__init__()\n",
    "        self.model_q = model_q  # Dense encoder for question\n",
    "        self.model_p = model_p  # Dense encoder for paragraph\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        q = inputs['questions']\n",
    "        p = inputs['paragraphs']\n",
    "        # 1) The questions and paragraphs are passed into the BERT models to \n",
    "        # obtain the representations\n",
    "        q_repr = self.model_q(q)[:,0,:] # We take the first tokens as representation of the question... \n",
    "        p_repr = self.model_p(p)[:,0,:] # ...and the paragraph\n",
    "        return q_repr, p_repr\n",
    "    \n",
    "enc = DenseEncoder(model_q, model_p)\n",
    "enc(dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': {'input_ids': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[  101,  2000,  3183, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  2054,  2003, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  1996, 13546, ...,     0,     0,     0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  101,  2054,  3633, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  2029,  3396, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  2129,  2116, ...,     0,     0,     0]]])>, 'token_type_ids': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]]])>, 'attention_mask': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]]])>}, 'paragraphs': {'input_ids': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[ 101, 6549, 2135, ...,    0,    0,    0]],\n",
      "\n",
      "       [[ 101, 6549, 2135, ...,    0,    0,    0]],\n",
      "\n",
      "       [[ 101, 6549, 2135, ...,    0,    0,    0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 101, 1996, 2118, ...,    0,    0,    0]],\n",
      "\n",
      "       [[ 101, 1996, 2118, ...,    0,    0,    0]],\n",
      "\n",
      "       [[ 101, 1996, 2267, ...,    0,    0,    0]]])>, 'token_type_ids': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]]])>, 'attention_mask': <tf.Tensor: shape=(16, 1, 512), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]]])>}}\n"
     ]
    }
   ],
   "source": [
    "for d in dataset.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d3a38b9baf6bccb52d534d3795fadb5d3190627f4e5187a36a9129f48a6e143"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
