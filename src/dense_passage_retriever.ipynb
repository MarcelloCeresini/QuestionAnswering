{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oZkaIoI1esj",
    "outputId": "a5c715ba-0106-45de-8312-4aa7a5c63411"
   },
   "outputs": [],
   "source": [
    "username = 'MarcelloCeresini'\n",
    "repository = 'QuestionAnswering'\n",
    "\n",
    "# COLAB ONLY CELLS\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !pip3 install transformers\n",
    "    !git clone https://www.github.com/{username}/{repository}.git\n",
    "    #from google.colab import drive\n",
    "    #drive.mount('/content/drive/')\n",
    "    %cd /content/QuestionAnswering/src\n",
    "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmXDnbCt1ess"
   },
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tIdZRSl1esu"
   },
   "source": [
    "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
    "\n",
    "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
    "\n",
    "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
    "\n",
    "In the Tf-Idf example, the retriever was simply a function that returned the top score obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
    "\n",
    "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
    "\n",
    "At run-time, another Dense Encoder is used $E_Q$ which maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
    "\n",
    "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
    "\n",
    "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
    "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
    "\n",
    "The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI. At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
    "\n",
    "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
    "\n",
    "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
    "\n",
    "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
    "- Random: a negative is any random passage in the corpus\n",
    "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
    "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
    "\n",
    "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pais where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7s1cbEuy1es4"
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm45AN-fDEiv"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8CBcRCvCCpzg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from typing import List, Union, Dict, Tuple\n",
    "from transformers import BertTokenizerFast, DistilBertTokenizerFast, \\\n",
    "                         TFBertModel, TFDistilBertModel\n",
    "import utils\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 4 if not using_TPU else 64\n",
    "MAX_SEQ_LEN = 512\n",
    "BERT_DIMENSIONALITY = 768\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7Ih1ZqSDCmW"
   },
   "source": [
    "### TPU check\n",
    "The training could be made faster if we use the cloud GPUs offered by Google on Google Colab. Since TPUs require manual intialization and other oddities, we check multiple times throughout the notebook what kind of hardware we are running the code on.\n",
    "using_TPU = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyt5e89MCm4X",
    "outputId": "0c7dd94f-d607-4b5f-d4ed-a988501d087c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "if using_TPU:\n",
    "    try: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        # This is the TPU initialization code that has to be at the beginning.\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "        strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    except:\n",
    "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
    "        using_TPU = False\n",
    "else:\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDjUKQxol1lS",
    "outputId": "60ee1441-4156-41fa-9f01-9b67381ad35b"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    checkpoint_dir = '/content/drive/My Drive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
    "    datasets_dir = '/content/drive/My Drive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
    "else:\n",
    "    # Create the folder where we'll save the weights of the model\n",
    "    checkpoint_dir = os.path.join(\"data\", \"training_dpr\")\n",
    "    datasets_dir = os.path.join(\"data\", \"training_dpr\", \"dataset\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(datasets_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVHmkEeC2kuf"
   },
   "source": [
    "## Allowed operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QSfXDW2g2sAr"
   },
   "outputs": [],
   "source": [
    "OVERWRITE_DATASETS = True      # Best to be run on GPU, takes about half an hour.\n",
    "                                # It saves a new copy of the pre-tokenized\n",
    "                                # question-paragraph pair on Google Drive.\n",
    "                                # The copy should then be uploaded to GCloud into\n",
    "                                # the bucket allocated for the project to be used by\n",
    "                                # TPUs.\n",
    "\n",
    "ANALYSE_TOKENIZATIONS = True    # Tokenises all paragraphs and showcases a simple analysis\n",
    "                                # for reducing the length of the sequence. Runs in less \n",
    "                                # than a few minutes on any architecture.\n",
    "\n",
    "DO_TRAINING = True              # Best to be run on TPU, takes about 2 hours.\n",
    "                                # It trains the model and saves a copy of the initial\n",
    "                                # and last weights on Google Drive. When run on GPU\n",
    "                                # It also saves checkpoints and Tensorboard data,\n",
    "                                # but it takes over 4 hours to do a single epoch,\n",
    "                                # so it's impossible to run it on Colab.\n",
    "\n",
    "OVERRIDE_REPRESENTATIONS = True  # Best to be run on GPU, it takes about 30 minutes.\n",
    "                                 # It produces representations of all paragraphs\n",
    "                                 # using the trained model_p as a large NumPy array\n",
    "                                 # that is stored on Google Drive at the end.\n",
    "\n",
    "RUN_ACCURACY_ANALYSIS = True    # Best to be run on GPU, takes about an hour\n",
    "                                # It analyzes the paragraph retrieval capability\n",
    "                                # of the model with top-1 and top-5 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAv3n12tDIAo"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8piWcupDOPR"
   },
   "source": [
    "We define all the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "On3I2PUgCwNF"
   },
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.dirname(os.getcwd())\n",
    "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
    "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
    "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLC6xwfKDSAN"
   },
   "source": [
    "We collect the training and validation questions into different lists. Paragraphs are collected together into a single, unified pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1zdDCsLZDMkm"
   },
   "outputs": [],
   "source": [
    "train_dict = utils.read_question_set(TRAINING_FILE)\n",
    "test_dict = utils.read_question_set(TEST_FILE)\n",
    "\n",
    "# Important note: the test set contains the same paragraphs as the train set,\n",
    "# but different questions.\n",
    "\n",
    "full_dict = {'data': train_dict['data'] + val_dict['data']}\n",
    "\n",
    "def get_questions_and_paragraphs_from_dataset(dataset, offset_context=0, offset_index_q=0):\n",
    "    questions = [{\n",
    "            'qas': qas,\n",
    "            'context_id': (i+offset_context,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
    "        }\n",
    "        for i in range(len(dataset['data']))\n",
    "        for j, para in enumerate(dataset['data'][i]['paragraphs'])\n",
    "        for qas in para['qas']\n",
    "    ]\n",
    "\n",
    "    paragraphs = [{\n",
    "            'context': para['context'],\n",
    "            'context_id': (i+offset_context,j)\n",
    "        }\n",
    "        for i in range(len(dataset['data']))\n",
    "        for j, para in enumerate(dataset['data'][i]['paragraphs'])\n",
    "    ]\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        questions[i]['global_id'] = i+offset_index_q\n",
    "\n",
    "    return questions, paragraphs\n",
    "\n",
    "def make_paragraph_pool(train_p, val_p):\n",
    "    paragraphs = train_p + val_p\n",
    "    for i in range(len(paragraphs)):\n",
    "        paragraphs[i]['global_id'] = i\n",
    "    for i in range(len(train_p)):\n",
    "        paragraphs[i]['from_set'] = 'train' \n",
    "    for i in range(len(train_p), len(train_p) + len(val_p)):\n",
    "        paragraphs[i]['from_set'] = 'val' \n",
    "    return paragraphs\n",
    "\n",
    "train_questions, train_paragraphs = get_questions_and_paragraphs_from_dataset(train_dict)\n",
    "val_questions, val_paragraphs = get_questions_and_paragraphs_from_dataset(val_dict, \n",
    "                                                                          offset_context=len(train_dict['data']), \n",
    "                                                                          offset_index_q=len(train_questions))\n",
    "paragraphs = make_paragraph_pool(train_paragraphs, val_paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvGAqUBvDbIg"
   },
   "source": [
    "We create the two different DistilBert models for encoding and test them on a random question/paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376,
     "referenced_widgets": [
      "7da2b839270845c3b3fb0928edcfc149",
      "c7d7090757264106b93615b3fd1574c9",
      "6d2dd941140143729bdd6fd878e2c376",
      "d4f1e3dda90c43a19d9b769668080e86",
      "397a6ec7a4704a0e9122ac3c0271fb28",
      "42e0ea86a4f74d508c48e1f7960966b7",
      "a9ad60c7e5274d41a5a25f81732a0695",
      "f0db667cb45f470cbf2525cfea9eb1ce",
      "54f45a351dee44089b44f9bc8ee5570d",
      "0d7fc2c9cf9749e287a5e07e59af7a4f",
      "2cfcd23bb87745fd90c77c37590134a1",
      "ca33ac54e7df4f92a06c2c72200aff4a",
      "83219e13854747f5894fe9d605223019",
      "22633e9d60084edd8fcca380116ddf93",
      "a6fbdc84a77440c589ad6f9352364a39",
      "25fa6fa95bd94b8fac2d6fb3391d9a74",
      "126e6520864a437581b7d97ab3a4e0af",
      "cc411be30cab4a9c9c9e02b8edb44e84",
      "8d994ebe2d654c1eb2e89a461f035821",
      "3c633582d8924d92a0601bb379508f62",
      "9a9380a1cf8a44ee85282fe02351731f",
      "8d84a2b731d0475ea3119245622ec1c2",
      "c8d3f72f7b314b2ba135f171e276de8b",
      "45e76c1b09a146fa9dca419c046f00b0",
      "f9e80c6147b74b9e8834aeb622935321",
      "ad6595455134492d82701f96e5dd7a9c",
      "94a8bd23a58344e3a42ede24f10374ae",
      "6ba10b7dca1640c4b263bbc8907da41e",
      "1dffd56db474486cb4c107bfd70c50d2",
      "390594b7577a486c9bf520cd5b2ab193",
      "661505b92910476da651a07b44fab528",
      "195a140c70e745778ace224541ce4141",
      "a96d71574bf740a79a3f88010f08471b",
      "7f647104453e4250869175070609be5a",
      "e137603ce4d14dc9b90be144b7b475a6",
      "d2c5370180ad4485a2252a2b85704de9",
      "422efbc57dd74cc08a438caac84bd601",
      "3c9954ce2c424f32a4b81bbca876dfc8",
      "af23f0e25a734b729b0ee583fac6d61f",
      "5f2526e897de4f31918bae07945fbf0b",
      "c31b0fd6b4264587afc364a901049efa",
      "9f1adc5fc1ea4b7d943638ee6e3815ef",
      "a8604cc168684198b265a57b1cc8b61a",
      "873c56217f3e426991f3313d9b5849c4",
      "65816d8027d14104920f6eed6a730f3e",
      "db7b192708a24e84bc5d1c3fa85d1153",
      "db5782d258ea46d39776f7a6fbb5203b",
      "5f58b3b46ca043358bba575ec1a54493",
      "90040941dbdc4bb7952a8741abd583e4",
      "501efdef1b2841d38b1c53de69186ed9",
      "43d39781d07a498aa1e3898a2f028526",
      "94aeffc8dea14ec5955b4ca24b65a7d7",
      "89c34fcb072f4809903711866f9dca4c",
      "4cbe1783e2e84d4fa7c2962e9f0c8b5a",
      "077aa58e5bda4982861cdce735c72b0d"
     ]
    },
    "id": "1XI6YUBZDit8",
    "outputId": "211ac9d9-c4f8-4dbb-8e06-98961435e524"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model_q, model_p = TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
    "                   TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFP6H-ok1jyN"
   },
   "source": [
    "What is the ideal maximum length of a sequence of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RFfkIhW-1i8P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                           | 213/18896 [00:00<00:08, 2108.97it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 18896/18896 [00:07<00:00, 2534.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of paragraphs: 18896\n",
      "Only 245 are truncated when using 350 max tokens (1.2965707027942421%)\n",
      "Only 210 are truncated when using 360 max tokens (1.111346316680779%)\n",
      "Only 180 are truncated when using 370 max tokens (0.9525825571549535%)\n",
      "Only 160 are truncated when using 380 max tokens (0.8467400508044031%)\n",
      "Only 135 are truncated when using 390 max tokens (0.714436917866215%)\n",
      "Only 114 are truncated when using 400 max tokens (0.6033022861981372%)\n",
      "Only 94 are truncated when using 410 max tokens (0.49745977984758677%)\n",
      "Only 81 are truncated when using 420 max tokens (0.428662150719729%)\n",
      "Only 66 are truncated when using 430 max tokens (0.34928027095681624%)\n",
      "Only 57 are truncated when using 440 max tokens (0.3016511430990686%)\n",
      "Only 49 are truncated when using 450 max tokens (0.25931414055884844%)\n",
      "Only 41 are truncated when using 460 max tokens (0.21697713801862828%)\n",
      "Only 35 are truncated when using 470 max tokens (0.18522438611346317%)\n",
      "Only 32 are truncated when using 480 max tokens (0.1693480101608806%)\n",
      "Only 26 are truncated when using 490 max tokens (0.1375952582557155%)\n"
     ]
    }
   ],
   "source": [
    "if ANALYSE_TOKENIZATIONS:\n",
    "    lens = []\n",
    "    for paragraph in tqdm(paragraphs):\n",
    "        emb = tokenizer_distilbert(paragraph['context'], return_tensors='np')\n",
    "        tokens = emb['attention_mask'].shape[1]\n",
    "        lens.append(tokens)\n",
    "    print(f\"Total n. of paragraphs: {len(paragraphs)}\")\n",
    "    for i in range(350, 500, 10):\n",
    "        print(f\"Only {np.sum(np.asarray(lens) > i)} are truncated when using {i} max tokens ({np.sum(np.asarray(lens) > i) / len(paragraphs)*100}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t80p88uc5493"
   },
   "source": [
    "We can choose to truncate sequences at 380 tokens to reduce memory consumption.\n",
    "\n",
    "Furthermore, we reduce the dimensionality of DistilBert's output to `BERT_DIMENSIONALITY` (instead of the default of 768)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8GSRnaQpJOqB"
   },
   "outputs": [],
   "source": [
    "class ReducedDistilBertModel(keras.Model):\n",
    "    def __init__(self, distilbert_model):\n",
    "        super(ReducedDistilBertModel, self).__init__()\n",
    "        self.distilbert_model = distilbert_model\n",
    "        self.reduction_layer = keras.layers.Dense(BERT_DIMENSIONALITY, \n",
    "                                                  activation='gelu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_state = self.distilbert_model(inputs).last_hidden_state\n",
    "        # We introduce a dense layer that simply reduces the dimensionality of distilbert\n",
    "        return self.reduction_layer(hidden_state)\n",
    "\n",
    "model_q = ReducedDistilBertModel(model_q)\n",
    "model_p = ReducedDistilBertModel(model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcCbC-Vi1es5",
    "outputId": "b474bc9f-ccf4-4a43-9a9b-7dd637224a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on a simple question. \n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Representation dimensionality: (768,)\n"
     ]
    }
   ],
   "source": [
    "test_question = train_questions[0]['qas']['question']\n",
    "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
    "inputs_test = tokenizer_distilbert(test_question, return_tensors=\"tf\")\n",
    "outputs = model_q(inputs_test)\n",
    "\n",
    "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
    "test_q_repr = outputs[0,0,:]\n",
    "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_lPiEZu1es9"
   },
   "source": [
    "# Encoders Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7qavEm61es-"
   },
   "source": [
    "First of all, we need to train our models. To do that, we need to create a dataset that feeds batches of questions and positive and negative paragraphs to a model, which is used to compute the representations, then the similarities and to correct the learnt distributions from the encoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCYkx8rf1es_"
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hynvz3Gw1etA"
   },
   "source": [
    "For the dataset, we use the `tf.data.Dataset` API. In particular, we first create the dataset in a `.proto` file which contains all the information in a byte format. The file is then uploaded on Google Cloud, where it can easily be accessed all future times. This is the only working way to have a large dataset accessible from the TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaBWVf5k1etB",
    "outputId": "419bf3ff-8d5a-4fbd-dc20-80cbf3c4f73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_v3_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/train_v3_768.proto).\n",
      "Loading val_v3_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/val_v3_768.proto).\n"
     ]
    }
   ],
   "source": [
    "def get_paragraph_from_question(qas, dataset):\n",
    "    i,j = qas['context_id']\n",
    "    return dataset['data'][i]['paragraphs'][j]\n",
    "\n",
    "def get_paragraph_global_id_from_question(question, paragraphs):\n",
    "    for p in paragraphs:\n",
    "        if p['context_id'] == question['context_id']:\n",
    "            return p['global_id']\n",
    "\n",
    "def pre_tokenize_data(questions, dataset, tokenizer):\n",
    "    tokenized_questions = [\n",
    "        dict(tokenizer(questions[i]['qas']['question'], \n",
    "            max_length = MAX_SEQ_LEN, truncation = True, \n",
    "            padding = 'max_length'))\n",
    "    for i in tqdm(range(len(questions)))]\n",
    "    tokenized_paragraphs = [\n",
    "        dict(tokenizer(get_paragraph_from_question(\n",
    "                    questions[i], dataset\n",
    "                )['context'], max_length = MAX_SEQ_LEN, \n",
    "            truncation = True, padding = 'max_length',\n",
    "            return_offsets_mapping = True))\n",
    "    for i in tqdm(range(len(questions)))]\n",
    "    return tokenized_questions, tokenized_paragraphs\n",
    "\n",
    "def find_start_end_token_one_hot_encoded(\n",
    "    answers: Dict, \n",
    "    offsets: List[Tuple[int]]) -> Dict:\n",
    "    '''\n",
    "    This function returns the starting and ending token of the answer, \n",
    "    already one hot encoded and ready for binary crossentropy.\n",
    "    Inputs:\n",
    "        - answers: `List[Dict]` --> for each question, a list of answers.\n",
    "            Each answer contains:\n",
    "            - `answer_start`: the index of the starting character\n",
    "            - `text`: the text of the answer, that we exploit through the \n",
    "                number of chars that it contains\n",
    "        - offsets: `List[Tuple[int]]` --> the tokenizer from HuggingFace \n",
    "            transforms the paragraph into a sequence of tokens. \n",
    "            Offsets keeps track of the character start and end indexes for each token.\n",
    "   \n",
    "    Output:\n",
    "        - result: `Dict` --> each key contains only one array, the one-hot \n",
    "            encoded version of, respectively, the start and end token of \n",
    "            the answer in the sentence (question+context)\n",
    "    '''\n",
    "    result = {\n",
    "        \"out_S\": np.zeros(len(offsets), dtype=np.int32),\n",
    "        \"out_E\": np.zeros(len(offsets), dtype=np.int32)\n",
    "    } \n",
    "    for answer in answers:\n",
    "        starting_char = answer['answer_start']\n",
    "        answer_len = len(answer['text'])\n",
    "        # We skip the first token, [CLS], that has (0,0) as a tuple\n",
    "        for i in range(1, len(offsets)):\n",
    "            # Check if starting char is within the indexes\n",
    "            if (starting_char >= offsets[i][0]) and \\\n",
    "                (starting_char <= offsets[i][1]):\n",
    "                result[\"out_S\"][i] += 1\n",
    "            # If the ending char (starting + length -1) is in the interval, \n",
    "            # same as above.\n",
    "            if (starting_char + answer_len - 1 >= offsets[i][0]) and \\\n",
    "                (starting_char + answer_len - 1 < offsets[i][1]):\n",
    "                result[\"out_E\"][i] += 1\n",
    "                break\n",
    "    return result\n",
    "\n",
    "def decode_fn(record_bytes):\n",
    "    example = tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "      # Schema\n",
    "      {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"question__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "       \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "       \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64)})\n",
    "    return {\n",
    "      \"questions\": {'input_ids': example['question__input_ids'],\n",
    "                    'attention_mask': example['question__attention_mask'],\n",
    "                    'index': example['question__index']},\n",
    "      \"answers\":   {'out_s': example['answer__out_s'],\n",
    "                    'out_e': example['answer__out_e']},\n",
    "      \"paragraphs\":{'input_ids': example['paragraph__input_ids'],\n",
    "                    'attention_mask': example['paragraph__attention_mask'],\n",
    "                    'tokens_s': example['paragraph__tokens_s'],\n",
    "                    'tokens_e': example['paragraph__tokens_e'],\n",
    "                    'index': example['paragraph__index']}\n",
    "    }\n",
    "\n",
    "def create_dataset_from_records(questions, paragraphs, dataset, tokenizer, \n",
    "                                fn, batch_size=BATCH_SIZE, training=True):\n",
    "    # Pre-tokenize and write dataset on disk\n",
    "    filename = f'{fn}_v3_{BERT_DIMENSIONALITY}.proto'\n",
    "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
    "    gcs_filename = f'gs://volpepe-nlp-project-squad-datasets/{fn_type}.proto'\n",
    "    if OVERWRITE_DATASETS:\n",
    "    #if not os.path.exists(filename) or OVERWRITE_DATASETS:\n",
    "        print(\"Pre-tokenizing data...\")\n",
    "        tok_questions, tok_paragraphs = pre_tokenize_data(questions, dataset, tokenizer)\n",
    "        assert len(tok_questions) == len(tok_paragraphs), \"Error while pre-tokenizing dataset\"\n",
    "        print(\"Preprocessing answers...\")\n",
    "        answer_tokens = [find_start_end_token_one_hot_encoded(\n",
    "            questions[i]['qas']['answers'], tok_paragraphs[i]['offset_mapping'])\n",
    "        for i in tqdm(range(len(questions)))]\n",
    "        print(\"Saving dataset on disk...\")\n",
    "        with tf.io.TFRecordWriter(filename) as file_writer:\n",
    "            for i in tqdm(range(len(tok_questions))):\n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    \"question__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=tok_questions[i][\"input_ids\"])),\n",
    "                    \"question__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=tok_questions[i][\"attention_mask\"])),\n",
    "                    \"question__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[i])),\n",
    "                    \"answer__out_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=answer_tokens[i][\"out_S\"])),\n",
    "                    \"answer__out_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=answer_tokens[i][\"out_E\"])),\n",
    "                    \"paragraph__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=tok_paragraphs[i][\"input_ids\"])),\n",
    "                    \"paragraph__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=tok_paragraphs[i][\"attention_mask\"])),\n",
    "                    \"paragraph__tokens_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=[x[0] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
    "                    \"paragraph__tokens_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=[x[1] for x in tok_paragraphs[i][\"offset_mapping\"]])), \n",
    "                    \"paragraph__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[get_paragraph_global_id_from_question(questions[i], paragraphs)]))\n",
    "                    })).SerializeToString()\n",
    "                file_writer.write(record_bytes)\n",
    "        print(\"Upload the dataset on Google Cloud and re-run the function\")\n",
    "        return None\n",
    "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
    "    # Return it as processed dataset\n",
    "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_fn)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "### CREATE DATASETS ###\n",
    "# Valid for both TPU and GPU\n",
    "dataset_train = create_dataset_from_records(train_questions, paragraphs, full_dict, tokenizer_distilbert, \n",
    "                                            os.path.join(datasets_dir, 'train'))\n",
    "dataset_val = create_dataset_from_records(val_questions, paragraphs, full_dict, tokenizer_distilbert,\n",
    "                                            os.path.join(datasets_dir, 'val'), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAnwbiFU1etF"
   },
   "source": [
    "## Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52lwz19a1etG"
   },
   "source": [
    "First of all, we need a layer that takes as input the dictionary containing the tokenized questions and answers and returns their compact representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjPUMwKk1etG",
    "outputId": "7c6b5cdc-86ca-4c5c-f5ff-ed62e882fc5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape when in training mode: (4, 768), (4, 768)\n",
      "Output shape when in testing mode: (4, 768)\n",
      "Output shape when dealing with a single question: (1, 768)\n"
     ]
    }
   ],
   "source": [
    "class DenseEncoder(layers.Layer):\n",
    "    def __init__(self, model_q, model_p):\n",
    "        super().__init__()\n",
    "        self.model_q = model_q  # Dense encoder for questions\n",
    "        self.model_p = model_p  # Dense encoder for paragraphs\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        qs = {\n",
    "            'input_ids': inputs['questions']['input_ids'],\n",
    "            'attention_mask': inputs['questions']['attention_mask']\n",
    "        }\n",
    "        q_repr = self.model_q(qs)[:,0,:]\n",
    "        if training:\n",
    "            # Input contains the questions and paragraphs encoding\n",
    "            ps = {\n",
    "                'input_ids': inputs['paragraphs']['input_ids'],\n",
    "                'attention_mask': inputs['paragraphs']['attention_mask']\n",
    "            }\n",
    "            p_repr = self.model_p(ps)[:,0,:]\n",
    "            return q_repr, p_repr\n",
    "        else:\n",
    "            return q_repr\n",
    "\n",
    "# Small test for the layer\n",
    "class TestDenseEncoderModel(keras.Model):\n",
    "    def __init__(self, model_q, model_p):\n",
    "        super().__init__()\n",
    "        self.enc = DenseEncoder(model_q, model_p)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        return self.enc(inputs, training=training)\n",
    "\n",
    "test_model = TestDenseEncoderModel(model_q, model_p)\n",
    "q_repr, p_repr = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=True)\n",
    "print(f\"Output shape when in training mode: {q_repr.shape}, {p_repr.shape}\")\n",
    "q_repr_2 = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=False)\n",
    "print(f\"Output shape when in testing mode: {q_repr_2.shape}\")\n",
    "q = tokenizer_distilbert(\n",
    "    train_questions[0]['qas']['question'], max_length = MAX_SEQ_LEN, \n",
    "    truncation = True, padding = 'max_length', return_tensors=\"tf\")\n",
    "q_repr_3 = test_model({'questions': q})\n",
    "print(f\"Output shape when dealing with a single question: {q_repr_3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4r1MfHP1etI"
   },
   "source": [
    "Once we have the representations, we should compute the similarities, thus obtaining a a full mini-batch of positive-negative examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdG5bkWV2FU5",
    "outputId": "f93e2c52-3ca9-4730-90e4-3cfe5df026d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the similarity matrix\n",
    "S = tf.tensordot(q_repr, tf.transpose(p_repr), axes=1)\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEPy1ug1__n0"
   },
   "source": [
    "This similarity matrix has the following meaning:\n",
    "- Rows represent questions.\n",
    "- Each row contains the similarity that the respective question has with the 16 paragraphs (one of them is the positive one, the others are negative)\n",
    "\n",
    "In the paper, they refer to the loss as a *minimization of the negative log-likelihood of the positive passage*: what it really means is that we need to transform similarities to probabilities and use a categorical cross-entropy loss, where labels are the row index (which is also the column index in that row for the positive passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_aJezqWCKsw",
    "outputId": "9d6313eb-c64b-4983-8c61-1dcbc070d842"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3011715"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True\n",
    ")\n",
    "loss(y_true=tf.range(BATCH_SIZE), y_pred=S).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnidWQLZDwsf"
   },
   "source": [
    "The loss seems to be quite high for this batch. We can study it with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Vdgc2-gQD1PP",
    "outputId": "365e2c60-a138-4315-d0be-01d8aa98b10d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "if not using_TPU: # Otherwise the batch size is HUGE\n",
    "    S_arr = S.numpy()\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQwdzlTfEsH4"
   },
   "source": [
    "Indeed, ideally the predictions should be on the diagonal. This means that the \"default\" space for this metric learning problem is not that good. We are ready to learn a new representation distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOOLMK5mIMPc"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nme0koNHE76t"
   },
   "outputs": [],
   "source": [
    "class DeepQPEncoder(keras.Model):\n",
    "\n",
    "    def __init__(self, model_q, model_p):\n",
    "        super().__init__()\n",
    "        self.enc = DenseEncoder(model_q, model_p)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if training:\n",
    "            # For training we return the similarity matrix\n",
    "            repr_q, repr_p = self.enc(inputs, training=training)\n",
    "            S = tf.tensordot(repr_q, tf.transpose(repr_p), axes=1)\n",
    "            return S\n",
    "        else:\n",
    "            # In other cases, we return the representation of the question(s)\n",
    "            repr_q = self.enc(inputs, training=training)            \n",
    "            return repr_q\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x = data\n",
    "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Obtain similarities\n",
    "            S = self(x, training=True)\n",
    "            # Obtain loss value\n",
    "            loss = self.compiled_loss(y, S)\n",
    "        # Construct gradients and apply them through the optimizer\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Update and return metrics (specifically the one for the loss value).\n",
    "        self.compiled_metrics.update_state(y, S)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x = data\n",
    "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
    "        S = self(x, training=True) # We are not really training, but we have to obtain S\n",
    "        self.compiled_loss(y, S)\n",
    "        self.compiled_metrics.update_state(y, S)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "def create_model(sample, freeze_layers_up_to=5):\n",
    "    print(\"Creating BERT models...\")\n",
    "    model_q, model_p =  TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
    "                        TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    # Freeze layers \n",
    "    for i in range(freeze_layers_up_to): # layers 0 to variable are frozen, successive layers learn\n",
    "        model_q.distilbert.transformer.layer[i].trainable = False\n",
    "        model_p.distilbert.transformer.layer[i].trainable = False\n",
    "\n",
    "    model_q, model_p = ReducedDistilBertModel(model_q), ReducedDistilBertModel(model_p)\n",
    "\n",
    "    \n",
    "    print(\"Creating Deep Encoder...\")\n",
    "    model = DeepQPEncoder(model_q, model_p)\n",
    "\n",
    "    print(\"Compiling...\")\n",
    "    # Compile the model and loss\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=3e-6),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "    print(\"Testing on some data...\")\n",
    "    # Pass one batch of data to build the model\n",
    "    model(sample)\n",
    "\n",
    "    # Return the model\n",
    "    print(\"Model created!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA_dwB6MIHaq"
   },
   "source": [
    "## Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfdxw7F4IWVi"
   },
   "source": [
    "Define utility variables and saving paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s838Rj9GIUA9"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOrJEqCQIZ3c"
   },
   "source": [
    "Before training, we check if we're using a TPU, in order to create the model within the scope of the strategy.\n",
    "\n",
    "Then, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_O5zDcaTEtxV",
    "outputId": "093aef95-148f-4e71-a3cf-2b3e468f7147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BERT models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Deep Encoder...\n",
      "Compiling...\n",
      "Testing on some data...\n",
      "Model created!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "if using_TPU:\n",
    "    # TPU requires to create the model within the scope of the distributed strategy\n",
    "    # we're using.\n",
    "    with strategy.scope():\n",
    "        model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
    "                             freeze_layers_up_to=3)\n",
    "\n",
    "    # Workaraound for saving locally when using cloud TPUs\n",
    "    local_device_option = tf.train.CheckpointOptions(\n",
    "        experimental_io_device=\"/job:localhost\")\n",
    "else:\n",
    "    # GPUs and local systems don't need the above specifications. We simply\n",
    "    # create a pattern for the filename and let the callbacks deal with it.\n",
    "    model_name = f'model_{BERT_DIMENSIONALITY}'\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, model_name + \"_cp-{epoch:04d}.ckpt\")\n",
    "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
    "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
    "        f\"training_dpr_{BERT_DIMENSIONALITY}\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    \n",
    "    model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
    "                             freeze_layers_up_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN6WxrL-IRjj",
    "outputId": "1c2f8971-6ffe-46b0-a3be-6db22383d839",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DO_TRAINING:\n",
    "    if not using_TPU:\n",
    "        # ModelCheckpoint callback is only available when not using TPU\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_weights_only = True,\n",
    "            save_best_only = True\n",
    "        )\n",
    "\n",
    "        # Same for tensorboard callback\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "\n",
    "    # Early stopping can be used by both hardware\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = PATIENCE,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    callbacks = [es_callback]\n",
    "    if not using_TPU:\n",
    "        # These callback imply saving stuff on local disk, which cannot be \n",
    "        # done automatically using TPUs.\n",
    "        # Therefore, they are only active when using GPUs and local systems\n",
    "        callbacks.extend([cp_callback, tensorboard_callback])\n",
    "\n",
    "    # We fit the model\n",
    "    history = model.fit(\n",
    "        dataset_train, \n",
    "        y=None,\n",
    "        validation_data=dataset_val,\n",
    "        epochs=EPOCHS, \n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        initial_epoch=0,\n",
    "        verbose=1 # Show progress bar\n",
    "    )\n",
    "\n",
    "    if using_TPU:\n",
    "        # Save last weights\n",
    "        model.save_weights(os.path.join(\n",
    "            checkpoint_dir, f'training_{BERT_DIMENSIONALITY}_tpu_last.h5'), overwrite=True)\n",
    "    else:\n",
    "        # Final save\n",
    "        model.save_weights(os.path.join(checkpoint_dir, f'final_dpr_{BERT_DIMENSIONALITY}.ckpt'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCgA-4RbAZCh"
   },
   "source": [
    "The model was trained in 7 (+3 due to patience) epochs on a local GPU. It took about 14 hours and reached a validation accuracy of 99.3%. We can make a simple test with the same data of before to see if the embedding space is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pkTmbBS0CmZ4"
   },
   "outputs": [],
   "source": [
    "if DO_TRAINING:\n",
    "    S = model(next(dataset_val.take(1).as_numpy_iterator()), training=True)\n",
    "    S_arr = S.numpy()\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DtqzKMW6scF"
   },
   "source": [
    "# Question Answering with the DPR\n",
    "\n",
    "We have trained the two Bert (`bert_p`, the paragraphs encoder and `bert_q`, the questions encoder) models to produce embeddings that are as similar as possible for matching question-paragraph pairs. \n",
    "\n",
    "Thanks to our training, when we use `bert_q` to encode our question, we will now be sure that questions and paragraphs will both be encoded in the same space and have a high similarity between the question encoding and the matching paragraph's encoding.\n",
    "\n",
    "We can now use `bert_p` to encode all of our paragraphs a-priori using the same method we have used before (taking the 768-d encoding at the `[CLS]` token). These encodings will be stored in RAM. \n",
    "\n",
    "Then, we can define our final Question Answering model in this way:\n",
    "- It receives only a question's embedding as input.\n",
    "- It uses `bert_q` to create a representation of the question in the learnt 768-d space, that is in common with the paragraph representations.\n",
    "- We compute similarity scores between the representation of the question and all representations of paragraphs. Based on these scores, we select the top-k ($k=100$) paragraphs.\n",
    "- For each of the $k$ paragraphs, we must compute the probability of the paragraph being selected $P_{selected}(i)$, as well as the usual $P_{start, i}(s), P_{end, i}(t)$ for each of the $s$-th and $t$-th words of the $i$-th paragraph. To do that, we need the full encoding of the paragraph (the $512 \\times 768$ output of Bert), which will be denoted as $P_i$ in contrast to $\\hat{P}_i$ which is the 768-d encoding at the `[CLS]` token. We obtain the full encoding by passing the $k$ paragraphs through `bert_p`, which is set to non-trainable (otherwise the encoding of the `[CLS]` token would constantly change). \n",
    "- All probabilities are computed through dense layers:\n",
    "\\begin{gather}\n",
    "P_{start,i}(s) = softmax(P_i w_{start})_s\n",
    "\\\\\n",
    "P_{end,i}(t) = softmax(P_i w_{end})_t\n",
    "\\\\\n",
    "P_{selected}(i) = softmax(\\hat{P}^\\intercal w_{selected})_i\n",
    "\\\\\n",
    "\\end{gather}\n",
    "where $w_{start}$, $w_{end}$ and $w_{selected}$ are learnt vectors, while $\\hat{P} = [P_{1}^{[CLS]}, \\dots, P_k^{[CLS]}]$.\n",
    "- As final answer, we select the highest scoring start-end legal span from the highest-scoring paragraph.\n",
    "\n",
    "During training: For each question, we create a batch by sampling $m$ ($m=24$ in the paper) from the top-100 passages returned by the retrieval system (DPR, so by computing similarities with the pre-computed representations). The training objective is to maximize the marginal log-likelihood of all the correct answer spans in the positive passage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. In the paper, a batch size of 16 was used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5InDU76v93rS"
   },
   "source": [
    "### Paragraphs representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbp2lLe9iX-u"
   },
   "source": [
    "First of all, we load the weights of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ry1uahz-bh4_",
    "outputId": "f5fa3ed2-bc63-4d29-88d4-d029fdfe2022",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e32c3399c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the obtained weights\n",
    "model.load_weights(os.path.join(checkpoint_dir, f'final_dpr_{BERT_DIMENSIONALITY}.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the encodings in memory but using the memmap mode because the files are larger than the available RAM. Basically, we are only loading a map of the file on disk in memory and for complex reads we follow this map and read from the disk.\n",
    "\n",
    "This makes it possible to access the array, even though it will be slower than having the representations on RAM. But in this way we can also enlarge the dimensionality if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPpSZ9nSZWMz",
    "outputId": "9f4e0139-f939-4d44-ae94-9ffaf2c22980"
   },
   "outputs": [],
   "source": [
    "if OVERRIDE_REPRESENTATIONS:\n",
    "    # About 12 GB array\n",
    "    paragraph_encodings = np.memmap(os.path.join(checkpoint_dir, f'paragraphs_encodings_{BERT_DIMENSIONALITY}.npy'),\n",
    "                            dtype='float32', mode='w+', shape=(len(paragraphs), MAX_SEQ_LEN, BERT_DIMENSIONALITY))\n",
    "\n",
    "    print(\"Obtaining training representations\")\n",
    "    for i in tqdm(range(len(paragraphs))):\n",
    "        paragraph_encodings[i] = model.enc.model_p(tokenizer_distilbert(\n",
    "            paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
    "            return_tensors='tf', truncation = True, padding = 'max_length'\n",
    "        ))[0,:,:].numpy().astype(np.float32)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            # Partial save every 200 writes\n",
    "            paragraph_encodings.flush()\n",
    "\n",
    "    paragraph_encodings.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bU8pBjgvbZS0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18896, 400, 768), dtype('float32'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs_encoding = np.memmap(os.path.join(checkpoint_dir, f'paragraphs_encodings_{BERT_DIMENSIONALITY}.npy'),\n",
    "                            dtype='float32', mode='r', shape=(len(paragraphs), MAX_SEQ_LEN, BERT_DIMENSIONALITY))\n",
    "paragraphs_encoding.shape, paragraphs_encoding.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReN18CaggRW5"
   },
   "source": [
    "### Qualitative test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ4-VRXZgXlb"
   },
   "source": [
    "We select some random questions and see whether the selected paragraph is the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1NISpx7gs4I",
    "outputId": "0e1e536a-070e-4333-880b-0c3688de5b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Along with India, what other nation was to determine the how much the US would have to compensate China?\n",
      "Ground truth paragraph: In the resulting Battle of Pusan Perimeter (August–September 1950), the U.S. Army withstood KPA attacks meant to capture the city at the Naktong Bulge, P'ohang-dong, and Taegu. The United States Air Force (USAF) interrupted KPA logistics with 40 daily ground support sorties that destroyed 32 bridges, halting most daytime road and rail traffic. KPA forces were forced to hide in tunnels by day and move only at night. To deny matériel to the KPA, the USAF destroyed logistics depots, petroleum refineries, and harbors, while the U.S. Navy air forces attacked transport hubs. Consequently, the over-extended KPA could not be supplied throughout the south. On 27 August, 67th Fighter Squadron aircraft mistakenly attacked facilities in Chinese territory and the Soviet Union called the UN Security Council's attention to China's complaint about the incident. The US proposed that a commission of India and Sweden determine what the US should pay in compensation but the Soviets vetoed the US proposal.\n",
      "Ground truth index: 18562\n",
      "Question representation shape: (1, 768)\n",
      "Top-5 best matching paragraphs have indexes [18550  3261 12275 18598 11688]\n",
      "Top-5 best matching paragraphs:\n",
      "0)\tOne facet of the changing attitude toward Korea and whether to get involved was Japan. Especially after the fall of China to the Communists, U.S. East Asian experts saw Japan as the critical counterweight to the Soviet Union and China in the region. While there was no United States policy that dealt with South Korea directly as a national interest, its proximity to Japan increased the importance of South Korea. Said Kim: \"The recognition that the security of Japan required a non-hostile Korea led directly to President Truman's decision to intervene... The essential point... is that the American response to the North Korean attack stemmed from considerations of US policy toward Japan.\"\n",
      "1)\tAnother situation can occur when one party wishes to create an obligation under international law, but the other party does not. This factor has been at work with respect to discussions between North Korea and the United States over security guarantees and nuclear proliferation.\n",
      "2)\tFrom as early as 1935 Japanese military strategists had concluded the Dutch East Indies were, because of their oil reserves, of considerable importance to Japan. By 1940 they had expanded this to include Indo-China, Malaya, and the Philippines within their concept of the Greater East Asia Co-Prosperity Sphere. Japanese troop build ups in Hainan, Taiwan, and Haiphong were noted, Japanese Army officers were openly talking about an inevitable war, and Admiral Sankichi Takahashi was reported as saying a showdown with the United States was necessary.\n",
      "3)\tAfter a new wave of UN sanctions, on 11 March 2013, North Korea claimed that it had invalidated the 1953 armistice. On 13 March 2013, North Korea confirmed it ended the 1953 Armistice and declared North Korea \"is not restrained by the North-South declaration on non-aggression\". On 30 March 2013, North Korea stated that it had entered a \"state of war\" with South Korea and declared that \"The long-standing situation of the Korean peninsula being neither at peace nor at war is finally over\". Speaking on 4 April 2013, the U.S. Secretary of Defense, Chuck Hagel, informed the press that Pyongyang had \"formally informed\" the Pentagon that it had \"ratified\" the potential usage of a nuclear weapon against South Korea, Japan and the United States of America, including Guam and Hawaii. Hagel also stated that the United States would deploy the Terminal High Altitude Area Defense anti-ballistic missile system to Guam, because of a credible and realistic nuclear threat from North Korea.\n",
      "4)\tDuring the Cold War, the Asian power of Japan and the European powers of the United Kingdom, France, and West Germany rebuilt their economies. France and the United Kingdom maintained technologically advanced armed forces with power projection capabilities and maintain large defence budgets to this day. Yet, as the Cold War continued, authorities began to question if France and the United Kingdom could retain their long-held statuses as great powers. China, with the world's largest population, has slowly risen to great power status, with large growth in economic and military power in the post-war period. After 1949, the Republic of China began to lose its recognition as the sole legitimate government of China by the other great powers, in favour of the People's Republic of China. Subsequently, in 1971, it lost its permanent seat at the UN Security Council to the People's Republic of China.\n"
     ]
    }
   ],
   "source": [
    "sample_question = random.choice(val_questions)\n",
    "question_text = sample_question['qas']['question']\n",
    "tokenized_question = tokenizer_distilbert(question_text, return_tensors=\"tf\")\n",
    "paragraph_index = get_paragraph_global_id_from_question(sample_question, paragraphs)\n",
    "print(f\"Question: {question_text}\")\n",
    "print(f\"Ground truth paragraph: {get_paragraph_from_question(sample_question, full_dict)['context']}\")\n",
    "print(f\"Ground truth index: {paragraph_index}\")\n",
    "sample_q_repr = model.enc.model_q(tokenized_question)[:,0,:]\n",
    "print(f\"Question representation shape: {sample_q_repr.shape}\")\n",
    "scores = tf.tensordot(sample_q_repr, paragraphs_encoding[:,0,:].T, axes=1)\n",
    "best_par_index = tf.argsort(scores, axis=1, direction='DESCENDING')[0, :5].numpy()\n",
    "print(f\"Top-5 best matching paragraphs have indexes {best_par_index}\")\n",
    "print(f\"Top-5 best matching paragraphs:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i})\\t{paragraphs[best_par_index[i]]['context']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOPMaSyb5CfJ"
   },
   "source": [
    "Usually, the correct paragraphs show up in the Top-5 list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWM3pdRAwBlz"
   },
   "source": [
    "### Quantitative test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJXrFcjnwFaA"
   },
   "source": [
    "We can measure the top-1/top-5 accuracy as we've done for the tf-idf baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MShv2E92wXJt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_top1 = 0\n",
    "count_top5 = 0\n",
    "count_total = len(val_questions)\n",
    "\n",
    "if RUN_ACCURACY_ANALYSIS:\n",
    "    def top_5_for_question(tok_q):\n",
    "        # Input: a tokenized question\n",
    "        # Pass the question through the question encoder\n",
    "        sample_q_repr = model.enc.model_q(tok_q)[:,0,:].numpy()\n",
    "        # Compute scores with the paragraphs\n",
    "        scores = np.dot(sample_q_repr, paragraphs_encoding[:,0,:].T)\n",
    "        # Obtain the best scores\n",
    "        top5_indices = np.argsort(scores, axis=1)[0, -5:]\n",
    "        top5_scores = np.sort(scores, axis=1)[0, -5:]\n",
    "        top5_para = [paragraphs[i] for i in top5_indices]\n",
    "        return top5_para, top5_scores, top5_indices     \n",
    "\n",
    "    for i,q in enumerate(tqdm(val_questions)):\n",
    "        top5_para, top5_scores, top5_indices = top_5_for_question(\n",
    "            tokenizer_distilbert(\n",
    "                q['qas']['question'], max_length = MAX_SEQ_LEN, \n",
    "                return_tensors='tf', truncation = True, padding = 'max_length'\n",
    "            )\n",
    "        )\n",
    "        top5_context_ids = [top5_para[i]['global_id'] for i in range(len(top5_para))]\n",
    "        gt_context_id = get_paragraph_global_id_from_question(q, paragraphs)\n",
    "        if gt_context_id == top5_context_ids[0]:\n",
    "            count_top1 += 1\n",
    "        if gt_context_id in top5_context_ids:\n",
    "            count_top5 += 1\n",
    "\n",
    "    top1_score = count_top1 / count_total * 100\n",
    "    top5_score = count_top5 / count_total * 100\n",
    "\n",
    "    print(f\"\\nTop 1 score: {top1_score:.2f}%,\\nTop 5 score: {top5_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 1 score: 16.54%\n",
    "\n",
    "Top 5 score: 38.62%\n",
    "\n",
    "It looks pretty bad, but these are paragraphs that were not seen in training so the model still has some generalisation capabilities. Also, the selected paragraphs are usually correlated to the question and furthermore we will analyse the first 24 paragraphs, not just the first 5, so the model still has a chance to be good at selecting the appropriate paragraph and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwctHyoXCrP5"
   },
   "source": [
    "To have a fair comparison with Tf-Idf, the same model was trained seeing all paragraphs from both training and validation and reached:\n",
    "\n",
    "Top 1 score: 86.49%\n",
    "\n",
    "Top 5 score: 96.58%\n",
    "\n",
    "This is a clear improvement with respect to the Tf-Idf baseline (72.01%/87.82%), but it's also quite slower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPG2DHje99it"
   },
   "source": [
    "## Question Answering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGzumB9nAVYd"
   },
   "source": [
    "We pre-tokenize the paragraphs so that we have easy access to them inside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDK42osrAlsX",
    "outputId": "e38eaf7c-d1d9-4a2c-d383-6e25e063eaaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 18896/18896 [00:10<00:00, 1879.18it/s]\n"
     ]
    }
   ],
   "source": [
    "pretokenized_paragraphs = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': [],\n",
    "    'offset_mapping': []\n",
    "}\n",
    "\n",
    "for i in tqdm(range(len(paragraphs))):\n",
    "    token_p = dict(tokenizer_distilbert(\n",
    "        paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
    "        return_tensors='tf', truncation = True, \n",
    "        padding = 'max_length', return_offsets_mapping = True\n",
    "    ))\n",
    "    pretokenized_paragraphs['input_ids'].append(token_p['input_ids'])\n",
    "    pretokenized_paragraphs['attention_mask'].append(token_p['attention_mask'])\n",
    "    pretokenized_paragraphs['offset_mapping'].append(token_p['offset_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOcyq9GtN7i-"
   },
   "source": [
    "Then, we define a function to create the model. The model should accept the top paragraphs encodings collected using the DPR and return their start, end and selection probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "R_v3r3DcBeKz"
   },
   "outputs": [],
   "source": [
    "class BestScoringCollector(keras.layers.Layer):\n",
    "    '''\n",
    "    Custom layer to collect the start and end probabilities from the best scoring\n",
    "    paragraph\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BestScoringCollector, self).__init__(trainable=False, **kwargs)\n",
    "\n",
    "    def call(self, probs_s, probs_e, probs_sel):\n",
    "        # Selection of best scoring paragraphs\n",
    "        best_scoring_paragraphs = tf.squeeze(tf.argmax(probs_sel, axis=1, output_type=tf.int32))\n",
    "        # Selection of related start-end probabilities\n",
    "        probs_s = tf.squeeze(tf.gather(probs_s, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
    "        probs_e = tf.squeeze(tf.gather(probs_e, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
    "        return probs_s, probs_e\n",
    "\n",
    "def create_QA_model(m):\n",
    "    # Receives in input the top paragraph full and search encodings collected using the DPR.\n",
    "    paragraphs_full_encodings = keras.Input(shape=(m, MAX_SEQ_LEN, BERT_DIMENSIONALITY), \n",
    "        dtype='float32', name=\"topm_full_encodings\")\n",
    "    paragraphs_search_encodings = keras.Input(shape=(m, BERT_DIMENSIONALITY), \n",
    "        dtype='float32', name=\"topm_search_encodings\")\n",
    "\n",
    "    # Compute probabilities for the start token\n",
    "    out_S = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"start_token_logits\")(paragraphs_full_encodings)\n",
    "    out_S = keras.layers.Reshape((m, MAX_SEQ_LEN))(out_S)\n",
    "    out_S = keras.layers.Softmax(name=\"start_probs\", axis=1, dtype='float32')(out_S)\n",
    "\n",
    "    # The same is done for the end tokens.\n",
    "    out_E = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"end_token_logits\")(paragraphs_full_encodings)\n",
    "    out_E = keras.layers.Reshape((m, MAX_SEQ_LEN))(out_E)\n",
    "    out_E = keras.layers.Softmax(name=\"end_probs\", axis=1, dtype='float32')(out_E)\n",
    "\n",
    "    # Also, we compute paragraph selection probabilities\n",
    "    out_SEL = keras.layers.Dense(1, name=\"selection_logits\")(paragraphs_search_encodings)\n",
    "    out_SEL = keras.layers.Flatten('channels_first')(out_SEL)\n",
    "    out_SEL = keras.layers.Softmax(name=\"selection_probs\", dtype='float32')(out_SEL)\n",
    "\n",
    "    out_S, out_E = BestScoringCollector(name='best_scoring_collector')(out_S, out_E, out_SEL)\n",
    "\n",
    "    # We return the keras model\n",
    "    model = keras.Model(\n",
    "        inputs=[paragraphs_full_encodings, paragraphs_search_encodings],\n",
    "        outputs = [out_S, out_E, out_SEL]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JR-Jr45AObut"
   },
   "source": [
    "We analyze the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Cvu8NLQGtSbJ"
   },
   "outputs": [],
   "source": [
    "M = 24              # Number of paragraphs to be collected by the DPR (24 in the paper)\n",
    "\n",
    "model_qa = create_QA_model(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkdbnPt_v-G8",
    "outputId": "e34d586c-5a2e-482d-b48a-d4b7b06c513f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " topm_full_encodings (InputLaye  [(None, 24, 400, 76  0          []                               \n",
      " r)                             8)]                                                               \n",
      "                                                                                                  \n",
      " topm_search_encodings (InputLa  [(None, 24, 768)]   0           []                               \n",
      " yer)                                                                                             \n",
      "                                                                                                  \n",
      " start_token_logits (TimeDistri  (None, 24, 400, 1)  769         ['topm_full_encodings[0][0]']    \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " end_token_logits (TimeDistribu  (None, 24, 400, 1)  769         ['topm_full_encodings[0][0]']    \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " selection_logits (Dense)       (None, 24, 1)        769         ['topm_search_encodings[0][0]']  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 24, 400)      0           ['start_token_logits[0][0]']     \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 24, 400)      0           ['end_token_logits[0][0]']       \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 24)           0           ['selection_logits[0][0]']       \n",
      "                                                                                                  \n",
      " start_probs (Softmax)          (None, 24, 400)      0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " end_probs (Softmax)            (None, 24, 400)      0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " selection_probs (Softmax)      (None, 24)           0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " best_scoring_collector (BestSc  (None, None)        0           ['start_probs[0][0]',            \n",
      " oringCollector)                                                  'end_probs[0][0]',              \n",
      "                                                                  'selection_probs[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,307\n",
      "Trainable params: 2,307\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_qa.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Etf4vYO5OB"
   },
   "source": [
    "The outputs are of the expected shape. We also use an utility to see the model visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "q8KH_VJSL8VN",
    "outputId": "bed812e2-58be-4f4b-ac9e-4ca1257e1764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model_qa, \"multi_input_and_output_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VukXJ1tECXl9"
   },
   "source": [
    "To make things easier and allow using the `memmap` API from NumPy without loading the whole matrix of pre-encoded paragraphs in memory, we remodel the training and validation datasets so to directly produce batches that can be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "w5mgm-pbCnLJ"
   },
   "outputs": [],
   "source": [
    "class DensePassageRetriever():\n",
    "    '''\n",
    "    This object retrieves the best available passages given a question\n",
    "    and a set of paragraphs (encodings). \n",
    "    It assumes that the deep question encoder (model_q) has already been\n",
    "    trained.\n",
    "    '''\n",
    "    def __init__(self, model_q, paragraphs_encodings, m=100):\n",
    "        self.model_q = model_q\n",
    "        self.paragraphs_encodings = paragraphs_encodings\n",
    "        self.m = m\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        '''\n",
    "        Assumes that inputs is a dictionary containing (at least):\n",
    "        - `input_ids`: the input ids of the questions obtained from the tokenizer\n",
    "        - `attention_mask`: the attention mask of the questions obtained from the tokenizer\n",
    "\n",
    "        Returns the (batch_size x num_parag) matrix of scores and the (batch_size x m) top_m indexes (according\n",
    "        to parameter `m` which defaults to 100)\n",
    "        '''\n",
    "        # 1) Obtain search encoding of question\n",
    "        qs = {\n",
    "            'input_ids': inputs['questions']['input_ids'],\n",
    "            'attention_mask': inputs['questions']['attention_mask']\n",
    "        }\n",
    "        q_repr = self.model_q(qs)[:,0,:].numpy()                      # batch_size x encoding_dim\n",
    "        # 2) Selection of paragraphs using search encoding\n",
    "        scores = np.dot(q_repr, self.paragraphs_encodings[:,0,:].T)   # batch_size x num_parag\n",
    "        topm_indices = np.argsort(scores, axis=1)[:, -self.m:]        # batch_size x m\n",
    "        return scores, topm_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 18896), (4, 100))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = DensePassageRetriever(model.enc.model_q, paragraphs_encoding)\n",
    "scores, top_m_indices = d(next(dataset_train.take(1).as_numpy_iterator()))\n",
    "scores.shape, top_m_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dense Passage retriever returns indices of paragraphs, which we can collect using this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, indices):\n",
    "    # Collect paragraphs. Inputs is the batch of paragraph indexes to gather\n",
    "    paragraphs = {\n",
    "        'input_ids': tf.squeeze(tf.gather(pretokenized_paragraphs['input_ids'], indices), axis=2),\n",
    "        'attention_mask': tf.squeeze(tf.gather(pretokenized_paragraphs['attention_mask'], indices), axis=2),\n",
    "        'offset_mapping': tf.squeeze(tf.gather(pretokenized_paragraphs['offset_mapping'], indices), axis=2),\n",
    "        'indexes': indices\n",
    "    }\n",
    "    # Collect paragraph representations and their search encodings\n",
    "    paragraphs_full_encodings = np.take(paragraphs_encodings, indices, axis=0)\n",
    "    paragraphs_search_encodings = paragraphs_full_encodings[:,:,0,:]\n",
    "    return paragraphs, paragraphs_full_encodings, paragraphs_search_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, full_enc, search_enc = \\\n",
    "    collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encoding, top_m_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can create a dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 65064/65064 [00:10<00:00, 6277.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 65064/65064 [00:32<00:00, 2019.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 65064/65064 [00:01<00:00, 43289.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset on disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 54/65064 [13:57<492:30:07, 27.27s/it]"
     ]
    }
   ],
   "source": [
    "def create_qa_dataset(questions, fn, model_q, paragraphs_encodings, pretokenized_paragraphs):\n",
    "    # Instantiate DPR\n",
    "    dpr = DensePassageRetriever(model_q, paragraphs_encodings)\n",
    "    # Read dataset from cloud disk\n",
    "    filename = f'{fn}_v3_{BERT_DIMENSIONALITY}.proto'\n",
    "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
    "    gcs_filename = f'gs://volpepe-nlp-project-squad-datasets/{fn_type}.proto'\n",
    "    \n",
    "    if OVERWRITE_DATASETS_QA:\n",
    "        print(\"Pre-tokenizing data...\")\n",
    "        tok_questions, tok_paragraphs = pre_tokenize_data(questions, full_dict, tokenizer_distilbert)\n",
    "        assert len(tok_questions) == len(tok_paragraphs), \"Error while pre-tokenizing dataset\"\n",
    "        print(\"Preprocessing answers...\")\n",
    "        answer_tokens = [find_start_end_token_one_hot_encoded(\n",
    "            questions[i]['qas']['answers'], tok_paragraphs[i]['offset_mapping'])\n",
    "        for i in tqdm(range(len(questions)))]\n",
    "        print(\"Saving dataset on disk...\")\n",
    "        with tf.io.TFRecordWriter(filename) as file_writer:\n",
    "            for i in tqdm(range(len(tok_questions))):\n",
    "                question_ids = tok_questions[i][\"input_ids\"]\n",
    "                question_attention_mask = tok_questions[i][\"attention_mask\"]\n",
    "                # Create the DPR inputs\n",
    "                dpr_inputs = {\n",
    "                    'questions': { \n",
    "                        'input_ids': np.array(question_ids),\n",
    "                        'attention_mask': np.array(question_attention_mask)\n",
    "                    }\n",
    "                }\n",
    "                # Retrieve scores and top indices by calling the DPR\n",
    "                scores, top_m_indices = dpr(dpr_inputs)\n",
    "                # Obtain the paragraphs and their encodings by calling the collect function\n",
    "                ps, full_enc, search_enc = \\\n",
    "                    collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, top_m_indices)\n",
    "                \n",
    "                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    \"question__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=question_ids)),\n",
    "                    \"question__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=question_attention_mask)),\n",
    "                    \"answer__out_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=answer_tokens[i][\"out_S\"])),\n",
    "                    \"answer__out_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=answer_tokens[i][\"out_E\"])),\n",
    "                    \"paragraph__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=[get_paragraph_global_id_from_question(questions[i], paragraphs)])),\n",
    "                    \"paragraph__full_enc\": tf.train.Feature(bytes_list=tf.train.BytesList(\n",
    "                        value=[tf.io.serialize_tensor(np.asarray(full_enc)).numpy()])),\n",
    "                    \"paragraph__tokens_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=[x[0] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
    "                    \"paragraph__tokens_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                            value=[x[1] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
    "                    \"dpr__top_m_indices\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
    "                        value=list(tf.squeeze(top_m_indices)))),\n",
    "                    })).SerializeToString()\n",
    "                file_writer.write(record_bytes)\n",
    "        print(\"Upload the dataset on Google Cloud and re-run the function\")\n",
    "        return None\n",
    "    \n",
    "    def decode_qa_fn(record_bytes):\n",
    "        # Read example from bytes dataset\n",
    "        example = tf.io.parse_single_example(\n",
    "            # Data\n",
    "            record_bytes,\n",
    "            # Schema\n",
    "            {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
    "            \"paragraph__full_enc\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "            \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
    "            \"dpr__top_100_indices\": tf.io.FixedLenFeature(shape=(100, ), dtype=tf.int64)\n",
    "           })\n",
    "        \n",
    "        # Yield the final dictionary structured however we need\n",
    "        return {\n",
    "            'questions': {\n",
    "                'input_ids': example['question__input_ids'],\n",
    "                'attention_mask': example['question__attention_mask']\n",
    "            },\n",
    "            'answers': {\n",
    "                'out_s': example['answer__out_s'],\n",
    "                'out_e': example['answer__out_e']\n",
    "            },\n",
    "            'paragraphs': {\n",
    "                'index': example['paragraph__index'],\n",
    "                'full_enc': tf.io.parse_tensor(example['paragraph__full_enc'], out_type=tf.float32),\n",
    "                'search_enc': tf.io.parse_tensor(example['paragraph__search_enc'], out_type=tf.float32),\n",
    "                'tokens_s': example['paragraph__tokens_s'],\n",
    "                'tokens_e': example['paragraph__tokens_e']\n",
    "            },\n",
    "            'dpr': {\n",
    "                'top_100_indices': example['dpr__top_100_indices']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
    "    # Return it as processed dataset\n",
    "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_qa_fn)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "dataset_train_qa = create_qa_dataset(train_questions, os.path.join(datasets_dir, 'train_qa'), \n",
    "                                    model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs)\n",
    "dataset_val_qa = create_qa_dataset(val_questions, os.path.join(datasets_dir, 'val_qa'), \n",
    "                                  model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec={'questions': {'input_ids': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'index': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, 'answers': {'out_s': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'out_e': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None)}, 'paragraphs': {'input_ids': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'tokens_s': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'tokens_e': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'index': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_datasets(, model_q, paragraphs_encodings, pretokenized_paragraphs):\n",
    "\n",
    "    def qa_dataset_generator(dataset):\n",
    "        # Instantiate DPR\n",
    "        dpr = DensePassageRetriever(model_q, paragraphs_encodings)\n",
    "        # Instantiate dataset iterator\n",
    "        dataset = dataset.as_numpy_iterator()\n",
    "        # Iterator over regular dataset\n",
    "        for el in dataset:\n",
    "            # Retrieve scores and top indices by calling the DPR\n",
    "            scores, top_m_indices = dpr(el)\n",
    "            # Obtain the paragraphs and their encodings by calling the collect function\n",
    "            paragraphs, full_enc, search_enc = \\\n",
    "                collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, top_m_indices)\n",
    "            # Yield the collected elements\n",
    "            gt_indices = el['paragraphs']['index']\n",
    "            gt_start = el['answers']['out_s']\n",
    "            gt_end = el['answers']['out_e']\n",
    "            yield {\n",
    "                'gt_indices': gt_indices, \n",
    "                'gt_start': gt_start,\n",
    "                'gt_end': gt_end,\n",
    "                'top_m_indices': top_m_indices,\n",
    "                'full_enc': full_enc, \n",
    "                'search_enc': search_enc\n",
    "            }\n",
    "\n",
    "    signature = {\n",
    "        'gt_indices': tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.int32), \n",
    "        'gt_start': tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_LEN), dtype=tf.int32),\n",
    "        'gt_end': tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_LEN), dtype=tf.int32),\n",
    "        'top_m_indices': tf.TensorSpec(shape=(BATCH_SIZE, 100, ), dtype=tf.int32),\n",
    "        'full_enc': tf.TensorSpec(shape=(BATCH_SIZE, 100, MAX_SEQ_LEN, BERT_DIMENSIONALITY), dtype=tf.float32), \n",
    "        'search_enc': tf.TensorSpec(shape=(BATCH_SIZE, 100, BERT_DIMENSIONALITY), dtype=tf.float32)\n",
    "    }\n",
    "\n",
    "    qa_dataset_train = tf.data.Dataset.from_generator(partial(\n",
    "            qa_dataset_generator, dataset_train), \n",
    "        output_signature=signature)\n",
    "    qa_dataset_val = tf.data.Dataset.from_generator(partial(\n",
    "            qa_dataset_generator, dataset_val), \n",
    "        output_signature=signature)\n",
    "    \n",
    "    qa_dataset_train.apply(tf.data.experimental.assert_cardinality(len(dataset_train)))\n",
    "    qa_dataset_val.apply(tf.data.experimental.assert_cardinality(len(dataset_val)))\n",
    "\n",
    "    # No need to batch the dataset, it's already batched.\n",
    "    qa_dataset_train = qa_dataset_train.cache()\n",
    "    qa_dataset_train = qa_dataset_train.shuffle(10000)\n",
    "    qa_dataset_train = qa_dataset_train.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    qa_dataset_val = qa_dataset_val.cache()\n",
    "    qa_dataset_val = qa_dataset_val.shuffle(10000)\n",
    "    qa_dataset_val = qa_dataset_val.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return qa_dataset_train, qa_dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset_train, qa_dataset_val = create_qa_datasets(\n",
    "    dataset_train, dataset_val, model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(qa_dataset_train.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNhAi3lsPUOZ"
   },
   "source": [
    "### Training\n",
    "\n",
    "To train the model we need to:\n",
    "\n",
    "- Compile it defining the losses and optimizer.\n",
    "- Create a ground truth batch that we use for comparing the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leUruObriIep"
   },
   "source": [
    "We define custom training and testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "3iWkkQK8zhv-"
   },
   "outputs": [],
   "source": [
    "# Metrics (create under the strategy scope if using TPU)\n",
    "if using_TPU:\n",
    "    with strategy.scope():\n",
    "        loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
    "        end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
    "        sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
    "else:\n",
    "    loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "    start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
    "    end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
    "    sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
    "\n",
    "class QuestionAnsweringModel(keras.Model):\n",
    "    def __init__(self, m=M):\n",
    "        super(QuestionAnsweringModel, self).__init__()\n",
    "        self.model_qa = create_QA_model(m)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs['top_m_indices'], self.model_qa({\n",
    "            'topm_full_encodings': inputs['full_enc'],\n",
    "            'topm_search_encodings': inputs['search_enc']\n",
    "        })\n",
    "\n",
    "    @tf.function\n",
    "    def tf_shuffle_on_columns(self, value):\n",
    "        '''\n",
    "        Utility function that shuffles a tensor randomly on each of its rows.\n",
    "        '''\n",
    "        # Create a tensor of random numbers, argsort it and use them as indices to gather\n",
    "        # values from the original tensor\n",
    "        return tf.gather(value, tf.argsort(tf.random.uniform(tf.shape(value))), batch_dims=1)\n",
    "\n",
    "    @tf.function\n",
    "    def obtain_training_info(self, indexes, topm_indexes):\n",
    "        '''\n",
    "        Obtains a batch of data and the ground truth mask to be used while training the model\n",
    "        '''\n",
    "        # Collect ground truth indexes\n",
    "        gt_paragraphs = tf.expand_dims(tf.cast(indexes, tf.int32), -1)\n",
    "        # A training sample is formed by the positive and m-1 negative examples\n",
    "        # obtained from the top-100 for each of the questions in the batch.\n",
    "        # We create a data batch by sampling m-1 examples from the masked 100 paragraphs\n",
    "        negative_masks = tf.math.not_equal(topm_indexes, gt_paragraphs)\n",
    "        # To keep the graph working with the correct sizes, we create a tensor of negatives \n",
    "        # by random shuffling the large tensor of topm and taking the first train_m elements.\n",
    "        # The positive examples are replaced by randomly sampling from the same tensor.\n",
    "        # It could happen that the positive example is replaced by itself, or that a \n",
    "        # negative sample appears twice in the batch, but it's a non-deterministic\n",
    "        # process.\n",
    "        negatives = self.tf_shuffle_on_columns(tf.where(\n",
    "            negative_masks, topm_indexes, self.tf_shuffle_on_columns(topm_indexes))\n",
    "        )[:,:M-1]\n",
    "        # We concatenate the positive paragraph index to the selected negatives and shuffle\n",
    "        # so that the positive is not always the last element\n",
    "        data_batch = self.tf_shuffle_on_columns(\n",
    "            tf.concat([negatives, gt_paragraphs], axis=1))\n",
    "        # When we have a data batch, we create the ground truth mask, which represents the position\n",
    "        # of the positive sample in the data batch in a one-hot encoded fashion.\n",
    "        gt_mask = tf.cast(data_batch == gt_paragraphs, tf.int32)\n",
    "        return data_batch, gt_mask\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Obtain the training batch and the ground truth mask\n",
    "        data_batch, gt_mask = self.obtain_training_info(\n",
    "            data['gt_indices'], data['top_m_indices'])\n",
    "        # Open the gradient tape, obtain predictions and compute the loss\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, out_S, out_E, out_SEL = self(data_batch, training=True)\n",
    "            loss_start = self.compiled_loss(data['gt_start'], out_S)\n",
    "            loss_end = self.compiled_loss(data['gt_end'], out_E)\n",
    "            loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
    "            loss_value = sum([loss_start, loss_end, loss_sel])\n",
    "        # Compute the gradients and apply them on the variables\n",
    "        grads = tape.gradient(loss_value, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        # Update the metrics\n",
    "        loss_tracker.update_state(loss_value)\n",
    "        start_acc_metric.update_state(data['gt_start'], out_S)\n",
    "        end_acc_metric.update_state(data['gt_end'], out_E)\n",
    "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
    "        return {\"loss\": loss_tracker.result(), \n",
    "                \"start_accuracy\": start_acc_metric.result(),\n",
    "                \"end_accuracy\": end_acc_metric.result(),\n",
    "                \"sel_accuracy\": sel_acc_metric.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Obtain the training batch and the ground truth mask to compute metrics\n",
    "        data_batch, gt_mask = self.obtain_training_info(\n",
    "            data['gt_indices'], data['top_m_indices'])\n",
    "        # Compute predictions\n",
    "        _, out_S, out_E, out_SEL = self(data_batch, training=False)\n",
    "        # Compute the loss to update its metric\n",
    "        loss_start = self.compiled_loss(data['gt_start'], out_S)\n",
    "        loss_end = self.compiled_loss(data['gt_end'], out_E)\n",
    "        loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
    "        loss_value = sum([loss_start, loss_end, loss_sel])\n",
    "        loss_tracker.update_state(loss_value)\n",
    "        # Updates the metrics\n",
    "        start_acc_metric.update_state(data['gt_start'], out_S)\n",
    "        end_acc_metric.update_state(data['gt_end'], out_E)\n",
    "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        return {\"loss\": loss_tracker.result(), \n",
    "                \"start_accuracy\": start_acc_metric.result(),\n",
    "                \"end_accuracy\": end_acc_metric.result(),\n",
    "                \"sel_accuracy\": sel_acc_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        return [loss_tracker, start_acc_metric, end_acc_metric, sel_acc_metric]\n",
    "\n",
    "def create_trainable_QA_model(model_q):\n",
    "    print(\"Creating Question Answering model...\")\n",
    "    model = QuestionAnsweringModel(model_q)\n",
    "\n",
    "    print(\"Compiling...\")\n",
    "    # Compile the model (metrics are defined into the model)\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=3e-6),\n",
    "        loss = keras.losses.CategoricalCrossentropy()\n",
    "    )\n",
    "\n",
    "    print(\"Testing on some data...\")\n",
    "    # Pass one batch of test data to build the model\n",
    "    inputs = next(qa_dataset_train.take(1).as_numpy_iterator())\n",
    "    top_m_indices, probs_sel, probs_s, probs_e = model(inputs)\n",
    "    \n",
    "    print(\"Output shapes:\")\n",
    "    print(f\"\\tparagraphs: {top_m_indices.shape}\")\n",
    "    print(f\"\\tprobs_sel: {probs_sel.shape}\")\n",
    "    print(f\"\\tprobs_s: {probs_s.shape}\")\n",
    "    print(f\"\\tprobs_e: {probs_e.shape}\")\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5RLqioFsIon",
    "outputId": "dd938727-727a-4ef2-f2c3-6babf79550db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Question Answering model...\n",
      "Compiling...\n",
      "Testing on some data...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Exception encountered when calling layer \"dense_passage_retriever_2\" (type DensePassageRetriever).\n\nFailed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n\nCall arguments received:\n  • inputs={'questions': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}, 'answers': {'out_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'out_e': 'tf.Tensor(shape=(4, 400), dtype=int64)'}, 'paragraphs': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_e': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/787245389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \"training_qa_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmodel_QA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_trainable_QA_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretokenized_paragraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraphs_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/3641427625.py\u001b[0m in \u001b[0;36mcreate_trainable_QA_model\u001b[1;34m(model_q, pretokenized_paragraphs, paragraphs_encodings)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;31m# Pass one batch of test data to build the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopm_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpr_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_scoring_paragraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_sel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopm_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/613652970.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mq_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m                                                     \u001b[1;31m# batch_size x encoding_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# 2) Selection of paragraphs using search encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_repr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m# batch_size x num_parag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtopm_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DESCENDING'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m        \u001b[1;31m# batch_size x m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopm_indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Exception encountered when calling layer \"dense_passage_retriever_2\" (type DensePassageRetriever).\n\nFailed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n\nCall arguments received:\n  • inputs={'questions': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}, 'answers': {'out_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'out_e': 'tf.Tensor(shape=(4, 400), dtype=int64)'}, 'paragraphs': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_e': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}}"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "if using_TPU:\n",
    "    # TPU requires to create the model within the scope of the distributed strategy\n",
    "    # we're using.\n",
    "    with strategy.scope():\n",
    "        model_QA = create_trainable_QA_model(model.enc.model_q, pretokenized_paragraphs, paragraphs_encoding)\n",
    "\n",
    "    # Workaraound for saving locally when using cloud TPUs\n",
    "    local_device_option = tf.train.CheckpointOptions(\n",
    "        experimental_io_device=\"/job:localhost\")\n",
    "else:\n",
    "    # GPUs and local systems don't need the above specifications. We simply\n",
    "    # create a pattern for the filename and let the callbacks deal with it.\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model_qa_cp-{epoch:04d}.ckpt\")\n",
    "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
    "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
    "        \"training_qa_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    \n",
    "    model_QA = create_trainable_QA_model(model.enc.model_q, pretokenized_paragraphs, paragraphs_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffh1nGX0I9FT"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmsAi9v63eia"
   },
   "outputs": [],
   "source": [
    "QA_BATCH_SIZE = 4 if not using_TPU else 32\n",
    "\n",
    "dataset_train = create_dataset_from_records(train_questions, paragraphs, train_dict, tokenizer_distilbert, \n",
    "                                            os.path.join(datasets_dir, 'train'), batch_size=QA_BATCH_SIZE)\n",
    "dataset_val = create_dataset_from_records(val_questions, paragraphs, val_dict, tokenizer_distilbert,\n",
    "                                            os.path.join(datasets_dir, 'val'), training=False, batch_size=QA_BATCH_SIZE)\n",
    "\n",
    "if DO_QA_TRAINING:\n",
    "    # Early stopping can be used by both hardware\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        patience = PATIENCE,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    callbacks = [es_callback]\n",
    "    if not using_TPU:\n",
    "        # ModelCheckpoint callback is only available when not using TPU\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = checkpoint_path,\n",
    "            verbose=1,\n",
    "            save_weights_only = True,\n",
    "            save_best_only = False\n",
    "        )\n",
    "        # Same for tensorboard callback\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1\n",
    "        )\n",
    "        # Save the first weights using the pattern from before\n",
    "        model_QA.save_weights(checkpoint_path.format(epoch=0))\n",
    "        # These callback imply saving stuff on local disk, which cannot be \n",
    "        # done automatically using TPUs.\n",
    "        # Therefore, they are only active when using GPUs and local systems\n",
    "        callbacks.extend([cp_callback, tensorboard_callback])\n",
    "    else:\n",
    "        # Save first weights in a h5 file (it's the most stable way)\n",
    "        model_QA.save_weights(os.path.join(\n",
    "            checkpoint_dir, 'training_qa_tpu_0.h5'), overwrite=True)        \n",
    "\n",
    "    # We fit the model\n",
    "    history = model_QA.fit(\n",
    "        dataset_train, \n",
    "        y=None,\n",
    "        validation_data=dataset_val,\n",
    "        epochs=EPOCHS, \n",
    "        callbacks=callbacks,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        initial_epoch=0,\n",
    "        verbose=1 # Show progress bar\n",
    "    )\n",
    "\n",
    "    if using_TPU:\n",
    "        # Save last weights\n",
    "        model_qa.save_weights(os.path.join(\n",
    "            checkpoint_dir, 'training_qa_tpu_last.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I79iOHjGbE-"
   },
   "source": [
    "### Obtaining an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWUaL844hyf6"
   },
   "outputs": [],
   "source": [
    "def start_end_token_from_probabilities(\n",
    "    pstartv: np.array, \n",
    "    pendv: np.array, \n",
    "    dim:int=512) -> List[List[int]]:\n",
    "    '''\n",
    "    Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
    "    '''\n",
    "    idxs = []\n",
    "    for i in range(pstartv.shape[0]):\n",
    "        # For each element in the batch, transform the vectors into matrices\n",
    "        # by repeating them dim times:\n",
    "        # - Vectors of starting probabilities are stacked on the columns\n",
    "        pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
    "        # - Vectors of ending probabilities are repeated on the rows\n",
    "        pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
    "        # Once we have the two matrices, we sum them (element-wise operation)\n",
    "        # to obtain the scores of each combination\n",
    "        sums = pstart + pend\n",
    "        # We only care about the scores in the upper triangular part of the matrix\n",
    "        # (where the ending index is greater than the starting index)\n",
    "        # therefore we zero out the diagonal and the lower triangular area\n",
    "        sums = np.triu(sums, k=1)\n",
    "        # The most probable set of tokens is the one with highest score in the\n",
    "        # remaining matrix. Through argmax we obtain its position.\n",
    "        val = np.argmax(sums)\n",
    "        # Since the starting probabilities are repeated on the columns, each element\n",
    "        # is identified by the row. Ending probabilities are instead repeated on rows,\n",
    "        # so each element is identified by the column.\n",
    "        row = val // dim\n",
    "        col = val - dim*row\n",
    "        idxs.append([row,col])\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CUVf45Cj0oI"
   },
   "outputs": [],
   "source": [
    "answers_start_end = start_end_token_from_probabilities(probs_s, probs_e)\n",
    "print(answers_start_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVfgDsb_GHIE"
   },
   "source": [
    "Finally, we can obtain the answers to the questions we have given the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcpI3LVz2vKY"
   },
   "outputs": [],
   "source": [
    "best_indices = tf.squeeze(tf.gather(paragraphs['indexes'], tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
    "best_offsets = tf.squeeze(tf.gather(pretokenized_val_paragraphs['offset_mapping'], best_indices))\n",
    "best_offsets.shape, best_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuWtARzUITEV"
   },
   "outputs": [],
   "source": [
    "char_start_end = [(best_offsets[i][answers_start_end[i][0]][0].numpy(),\n",
    "                   best_offsets[i][answers_start_end[i][1]][1].numpy())\n",
    "                 for i in range(BATCH_SIZE)]\n",
    "char_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktvdpz52fxOc"
   },
   "outputs": [],
   "source": [
    "# Correction for answers arriving to the end of the sequence\n",
    "for i in range(BATCH_SIZE):\n",
    "    c = char_start_end[i]\n",
    "    if c[1] >= c[0]:\n",
    "        char_start_end[i] = (c[0], c[1])\n",
    "    else:\n",
    "        char_start_end[i] = (c[0], 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6edjxYTw4k-t"
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(best_indices.numpy()):\n",
    "    print(val_paragraphs[p]['context'][char_start_end[i][0]:char_start_end[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_B5U-_tKR6v"
   },
   "source": [
    "Of course, answers are extremely bad because we need to train the Dense layers selecting the start and end tokens, as well as the paragraph selector."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dense_passage_retriever.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3f1aedb1eb2c65e580c7e6628d6a6f161dbf7d87a60144a28bd6d91db1c87885"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "077aa58e5bda4982861cdce735c72b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d7fc2c9cf9749e287a5e07e59af7a4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "126e6520864a437581b7d97ab3a4e0af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "195a140c70e745778ace224541ce4141": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dffd56db474486cb4c107bfd70c50d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22633e9d60084edd8fcca380116ddf93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d994ebe2d654c1eb2e89a461f035821",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c633582d8924d92a0601bb379508f62",
      "value": 231508
     }
    },
    "25fa6fa95bd94b8fac2d6fb3391d9a74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2cfcd23bb87745fd90c77c37590134a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "390594b7577a486c9bf520cd5b2ab193": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "397a6ec7a4704a0e9122ac3c0271fb28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c633582d8924d92a0601bb379508f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9954ce2c424f32a4b81bbca876dfc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "422efbc57dd74cc08a438caac84bd601": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8604cc168684198b265a57b1cc8b61a",
      "placeholder": "​",
      "style": "IPY_MODEL_873c56217f3e426991f3313d9b5849c4",
      "value": " 483/483 [00:00&lt;00:00, 5.08kB/s]"
     }
    },
    "42e0ea86a4f74d508c48e1f7960966b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43d39781d07a498aa1e3898a2f028526": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45e76c1b09a146fa9dca419c046f00b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ba10b7dca1640c4b263bbc8907da41e",
      "placeholder": "​",
      "style": "IPY_MODEL_1dffd56db474486cb4c107bfd70c50d2",
      "value": "Downloading: 100%"
     }
    },
    "4cbe1783e2e84d4fa7c2962e9f0c8b5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "501efdef1b2841d38b1c53de69186ed9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54f45a351dee44089b44f9bc8ee5570d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f2526e897de4f31918bae07945fbf0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f58b3b46ca043358bba575ec1a54493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cbe1783e2e84d4fa7c2962e9f0c8b5a",
      "placeholder": "​",
      "style": "IPY_MODEL_077aa58e5bda4982861cdce735c72b0d",
      "value": " 347M/347M [00:16&lt;00:00, 25.3MB/s]"
     }
    },
    "65816d8027d14104920f6eed6a730f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_db7b192708a24e84bc5d1c3fa85d1153",
       "IPY_MODEL_db5782d258ea46d39776f7a6fbb5203b",
       "IPY_MODEL_5f58b3b46ca043358bba575ec1a54493"
      ],
      "layout": "IPY_MODEL_90040941dbdc4bb7952a8741abd583e4"
     }
    },
    "661505b92910476da651a07b44fab528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ba10b7dca1640c4b263bbc8907da41e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d2dd941140143729bdd6fd878e2c376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0db667cb45f470cbf2525cfea9eb1ce",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54f45a351dee44089b44f9bc8ee5570d",
      "value": 28
     }
    },
    "7da2b839270845c3b3fb0928edcfc149": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c7d7090757264106b93615b3fd1574c9",
       "IPY_MODEL_6d2dd941140143729bdd6fd878e2c376",
       "IPY_MODEL_d4f1e3dda90c43a19d9b769668080e86"
      ],
      "layout": "IPY_MODEL_397a6ec7a4704a0e9122ac3c0271fb28"
     }
    },
    "7f647104453e4250869175070609be5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e137603ce4d14dc9b90be144b7b475a6",
       "IPY_MODEL_d2c5370180ad4485a2252a2b85704de9",
       "IPY_MODEL_422efbc57dd74cc08a438caac84bd601"
      ],
      "layout": "IPY_MODEL_3c9954ce2c424f32a4b81bbca876dfc8"
     }
    },
    "83219e13854747f5894fe9d605223019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_126e6520864a437581b7d97ab3a4e0af",
      "placeholder": "​",
      "style": "IPY_MODEL_cc411be30cab4a9c9c9e02b8edb44e84",
      "value": "Downloading: 100%"
     }
    },
    "873c56217f3e426991f3313d9b5849c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "89c34fcb072f4809903711866f9dca4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8d84a2b731d0475ea3119245622ec1c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d994ebe2d654c1eb2e89a461f035821": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90040941dbdc4bb7952a8741abd583e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94a8bd23a58344e3a42ede24f10374ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94aeffc8dea14ec5955b4ca24b65a7d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a9380a1cf8a44ee85282fe02351731f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f1adc5fc1ea4b7d943638ee6e3815ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a6fbdc84a77440c589ad6f9352364a39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a9380a1cf8a44ee85282fe02351731f",
      "placeholder": "​",
      "style": "IPY_MODEL_8d84a2b731d0475ea3119245622ec1c2",
      "value": " 226k/226k [00:00&lt;00:00, 1.39MB/s]"
     }
    },
    "a8604cc168684198b265a57b1cc8b61a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a96d71574bf740a79a3f88010f08471b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9ad60c7e5274d41a5a25f81732a0695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad6595455134492d82701f96e5dd7a9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_195a140c70e745778ace224541ce4141",
      "placeholder": "​",
      "style": "IPY_MODEL_a96d71574bf740a79a3f88010f08471b",
      "value": " 455k/455k [00:00&lt;00:00, 2.41MB/s]"
     }
    },
    "af23f0e25a734b729b0ee583fac6d61f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c31b0fd6b4264587afc364a901049efa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7d7090757264106b93615b3fd1574c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42e0ea86a4f74d508c48e1f7960966b7",
      "placeholder": "​",
      "style": "IPY_MODEL_a9ad60c7e5274d41a5a25f81732a0695",
      "value": "Downloading: 100%"
     }
    },
    "c8d3f72f7b314b2ba135f171e276de8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_45e76c1b09a146fa9dca419c046f00b0",
       "IPY_MODEL_f9e80c6147b74b9e8834aeb622935321",
       "IPY_MODEL_ad6595455134492d82701f96e5dd7a9c"
      ],
      "layout": "IPY_MODEL_94a8bd23a58344e3a42ede24f10374ae"
     }
    },
    "ca33ac54e7df4f92a06c2c72200aff4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83219e13854747f5894fe9d605223019",
       "IPY_MODEL_22633e9d60084edd8fcca380116ddf93",
       "IPY_MODEL_a6fbdc84a77440c589ad6f9352364a39"
      ],
      "layout": "IPY_MODEL_25fa6fa95bd94b8fac2d6fb3391d9a74"
     }
    },
    "cc411be30cab4a9c9c9e02b8edb44e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2c5370180ad4485a2252a2b85704de9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c31b0fd6b4264587afc364a901049efa",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f1adc5fc1ea4b7d943638ee6e3815ef",
      "value": 483
     }
    },
    "d4f1e3dda90c43a19d9b769668080e86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d7fc2c9cf9749e287a5e07e59af7a4f",
      "placeholder": "​",
      "style": "IPY_MODEL_2cfcd23bb87745fd90c77c37590134a1",
      "value": " 28.0/28.0 [00:00&lt;00:00, 442B/s]"
     }
    },
    "db5782d258ea46d39776f7a6fbb5203b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94aeffc8dea14ec5955b4ca24b65a7d7",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_89c34fcb072f4809903711866f9dca4c",
      "value": 363423424
     }
    },
    "db7b192708a24e84bc5d1c3fa85d1153": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_501efdef1b2841d38b1c53de69186ed9",
      "placeholder": "​",
      "style": "IPY_MODEL_43d39781d07a498aa1e3898a2f028526",
      "value": "Downloading: 100%"
     }
    },
    "e137603ce4d14dc9b90be144b7b475a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af23f0e25a734b729b0ee583fac6d61f",
      "placeholder": "​",
      "style": "IPY_MODEL_5f2526e897de4f31918bae07945fbf0b",
      "value": "Downloading: 100%"
     }
    },
    "f0db667cb45f470cbf2525cfea9eb1ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9e80c6147b74b9e8834aeb622935321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_390594b7577a486c9bf520cd5b2ab193",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_661505b92910476da651a07b44fab528",
      "value": 466062
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
