{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oZkaIoI1esj"
      },
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
        "    # Google cloud credentials\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/MyDrive/Uni/Magistrale/NLP/Project/nlp-project-338723-0510aa0a4912.json\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXDnbCt1ess"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tIdZRSl1esu"
      },
      "source": [
        "In this notebook, we will try to implement the architecture detailed in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/pdf/2004.04906.pdf). \n",
        "\n",
        "The idea is that we have a corpus of documents $C = {p_1, p_2, \\dots, p_M}$ where each passage $p_i$ can be viewed as a sequence of tokens $w_1^{(i)}, w_2^{(i)}, \\dots, w_{|p_i|}^{(i)}$ and given a question $q$ we want to find the sequence of tokens $w_s^{(i)}, w_{s+1}^{(i)}, \\dots, w_{e}^{(i)}$ from one of the passage $i$ that can answer the question.\n",
        "\n",
        "In order to find the passage $i$ we need an efficient **Retriever** (i.e. a function $R: (q, C) \\rightarrow C_F$ where $C_F$ is a very small set of $k$ documents that have a high correlation with the query.)\n",
        "\n",
        "In the Tf-Idf example, the retriever was simply a function that returned the top score obtained by computing the vector cosine similarity between the query and all other documents. The problem with this approach is that it is not very efficient. Tf-Idf is a **sparse** document/query representation, thus computing a multitude of dot products between these very long vectors can be expensive.\n",
        "\n",
        "The paper cited above proposes a **dense** representation instead. It uses a Dense Encoder $E_P$ which maps all paragraphs to $d$-dimensional vectors. These vectors are stored in a database so that they can be efficiently retrieved. \n",
        "\n",
        "At run-time, another Dense Encoder $E_Q$ is used: it maps the input question to a vector with the same dimensionality $d$. Then, a similarity score is computed between the two representations:\n",
        "\n",
        "$sim(p,q) = E_Q(q)^\\intercal E_P(p)$\n",
        "\n",
        "In the paper, $E_Q$ and $E_P$ are two independent BERT transformers and the $d$-dimensional vector is the **output at the $\\texttt{[CLS]}$ token** (so, $d = 768$).\n",
        "- This leaves open the possibility to use a larger dimensionality (eg. concatenating the output at multiple blocks like we did for the QA task).\n",
        "\n",
        "<!-- The $d$-dimensional representations of the $M$ passages are indexed using [FAISS](https://github.com/facebookresearch/faiss), an efficient, open-source library for similarity search and clustering of dense vectors developed at Facebook AI.  -->\n",
        "At run-time, we simply compute $v_q = E_Q(q)$ and retrieve the top $k$ passages with embeddings closest to $v_q$.\n",
        "\n",
        "In this case, training the network means solving a **metric learning** problem: the two BERT networks need to learn an **effective vector space** such that relevant pairs of questions and passages are close, while irrelevant pairs are placed further away. In this problem we usually build a **training instance $D$** as ${(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-)}^m_{i=1}$, where question $q$ is paired with a relevant (positive) passage $p_i^+$ and $n$ irrelevant (negative) passages. Then, the loss function is the negative log-likelihood of the positive passage:\n",
        "\n",
        "$L(q_i, p_i^+, p_{i,1}^-, p_{i,2}^-, \\dots, p_{i,n}^-) = -\\log\\frac{e^{sim(q_i, p_i^+)}}{e^{sim(q_i, p_i^+)} + \\sum_{j=1}^n e^{sim(q_i, p_{i,j}^-)}}$\n",
        "\n",
        "It's easy to find the positive paragraph, but choosing the negatives is quite important. In particular, the paper proposes different ways for sampling the negatives:\n",
        "- Random: a negative is any random passage in the corpus\n",
        "- TF-IDF (The paper uses a variant, BM25): the negatives are the top passages (not containing the answer) returned by a TF-IDF search\n",
        "- Gold: the negatives are positives for other questions in the mini-batch. For the researchers, this is the best negative-mining option, because it's the most efficient and also it makes a batch a complete unit of learning (we learn the relationship that each question in the batch has with the other paragraphs).\n",
        "\n",
        "The Gold method allows the **in-batch negatives** technique: assuming to have a batch size of $B$, then we collect two $B \\times d$ matrices (one for questions, one for their positive paragraphs). Then, we compute $S = QP^\\intercal$ which is a $B \\times B$ matrix of **similarity scored** between each question and paragraph. This matrix can directly be used for training: any ($q_i, p_j$) pair where $i = j$ is considered to be a positive example, while it's negative otherwise. In total there will be $B$ training instances per batch, each with $B-1$ negative passages. \n",
        "\n",
        "Furthermore, at least one of the $B-1$ negative passages should be hard enough, so that the model learns to discriminate well. We pick the first non-positive paragraph returned by a Tf-Idf vectorizer and replace a random negative in the batch with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s1cbEuy1es4"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm45AN-fDEiv"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8CBcRCvCCpzg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import List, Union, Dict, Tuple\n",
        "from transformers import BertTokenizerFast, DistilBertTokenizerFast, \\\n",
        "                         TFBertModel, TFDistilBertModel\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import utils\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_SEQ_LEN = 512\n",
        "BERT_DIMENSIONALITY = 768\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Ih1ZqSDCmW"
      },
      "source": [
        "### TPU check\n",
        "The training could be made faster if we use the cloud GPUs offered by Google on Google Colab. Since TPUs require manual intialization and other oddities, we check multiple times throughout the notebook what kind of hardware we are running the code on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyt5e89MCm4X",
        "outputId": "ffc5f0f3-0c75-48e8-995a-3b561f702018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPUs are not available, setting flag 'using_TPU' to False.\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "if using_TPU:\n",
        "    try: \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
        "        using_TPU = False\n",
        "        print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "BATCH_SIZE = 8 if not using_TPU else 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDjUKQxol1lS",
        "outputId": "c143989d-8a9b-4119-f042-2047ed078bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "    datasets_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
        "else:\n",
        "    # Create the folder where we'll save the weights of the model\n",
        "    checkpoint_dir = os.path.join(\"data\", \"training_dpr\")\n",
        "    datasets_dir = os.path.join(\"data\", \"training_dpr\", \"dataset\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(datasets_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVHmkEeC2kuf"
      },
      "source": [
        "## Allowed operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QSfXDW2g2sAr"
      },
      "outputs": [],
      "source": [
        "ANALYSE_TOKENIZATIONS = True    # Tokenises all paragraphs and showcases a simple analysis\n",
        "                                # for reducing the length of the sequence. Runs in less \n",
        "                                # than a few minutes on any architecture.\n",
        "\n",
        "OVERWRITE_DATASETS = False      # Best to be run on GPU, takes about half an hour.\n",
        "                                # It saves a new copy of the pre-tokenized\n",
        "                                # question-paragraph pair on Google Drive.\n",
        "                                # The copy should then be uploaded to GCloud into\n",
        "                                # the bucket allocated for the project to be used by\n",
        "                                # TPUs.\n",
        "\n",
        "DO_TRAINING = True              # Best to be run on TPU, takes about 2 hours.\n",
        "                                # It trains the model and saves a copy of the \n",
        "                                # last weights on Google Drive. When run on GPU\n",
        "                                # It also saves checkpoints and Tensorboard data,\n",
        "                                # but it takes over 4 hours to do a single epoch,\n",
        "                                # so it's impossible to run it on Colab.\n",
        "\n",
        "OVERRIDE_REPRESENTATIONS = True  # Best to be run on GPU, it takes about 30 minutes.\n",
        "                                 # It produces representations of all paragraphs\n",
        "                                 # using the trained model_p as a large NumPy array\n",
        "                                 # that is stored on Google Drive at the end.\n",
        "\n",
        "RUN_ACCURACY_ANALYSIS = True    # Best to be run on GPU, takes about an hour\n",
        "                                # It analyzes the paragraph retrieval capability\n",
        "                                # of the model with top-1 and top-5 accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAv3n12tDIAo"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8piWcupDOPR"
      },
      "source": [
        "We define all the paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "On3I2PUgCwNF"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLC6xwfKDSAN"
      },
      "source": [
        "We collect the training, validation and testing questions and paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1zdDCsLZDMkm",
        "outputId": "5b8f0c41-0ae9-4933-b5a0-22ca5d559e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training questions: 65064\n",
            "Number of training paragraphs: 13975\n",
            "\n",
            "Number of val questions: 22535\n",
            "Number of val paragraphs: 4921\n",
            "\n",
            "Number of test questions: 10570\n",
            "Number of test paragraphs: 2067\n"
          ]
        }
      ],
      "source": [
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]\n",
        "\n",
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)\n",
        "\n",
        "print(f\"Number of training questions: {len(train_questions)}\")\n",
        "print(f\"Number of training paragraphs: {len(train_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of val questions: {len(val_questions)}\")\n",
        "print(f\"Number of val paragraphs: {len(val_paragraphs)}\")\n",
        "print()\n",
        "print(f\"Number of test questions: {len(test_questions)}\")\n",
        "print(f\"Number of test paragraphs: {len(test_paragraphs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvGAqUBvDbIg"
      },
      "source": [
        "We create the two different DistilBert models for encoding and test them on a random question/paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XI6YUBZDit8",
        "outputId": "faeca830-a42b-49b0-eccb-abcaf6413c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model_q, model_p = TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                   TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFP6H-ok1jyN"
      },
      "source": [
        "What is the ideal maximum length of a sequence of tokens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RFfkIhW-1i8P",
        "outputId": "f1ed6c29-c8aa-4136-f8a9-6c5449befb83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 331/13975 [00:00<00:12, 1107.93it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 13975/13975 [00:15<00:00, 878.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total n. of paragraphs: 13975\n",
            "Only 184 are truncated when using 350 max tokens (1.3166368515205724%)\n",
            "Only 156 are truncated when using 360 max tokens (1.1162790697674418%)\n",
            "Only 132 are truncated when using 370 max tokens (0.9445438282647585%)\n",
            "Only 119 are truncated when using 380 max tokens (0.851520572450805%)\n",
            "Only 100 are truncated when using 390 max tokens (0.7155635062611807%)\n",
            "Only 84 are truncated when using 400 max tokens (0.6010733452593918%)\n",
            "Only 68 are truncated when using 410 max tokens (0.48658318425760283%)\n",
            "Only 58 are truncated when using 420 max tokens (0.4150268336314848%)\n",
            "Only 46 are truncated when using 430 max tokens (0.3291592128801431%)\n",
            "Only 39 are truncated when using 440 max tokens (0.27906976744186046%)\n",
            "Only 35 are truncated when using 450 max tokens (0.25044722719141327%)\n",
            "Only 30 are truncated when using 460 max tokens (0.2146690518783542%)\n",
            "Only 28 are truncated when using 470 max tokens (0.2003577817531306%)\n",
            "Only 27 are truncated when using 480 max tokens (0.19320214669051877%)\n",
            "Only 22 are truncated when using 490 max tokens (0.15742397137745975%)\n"
          ]
        }
      ],
      "source": [
        "if ANALYSE_TOKENIZATIONS:\n",
        "    lens = []\n",
        "    for paragraph in tqdm(train_paragraphs):\n",
        "        emb = tokenizer_distilbert(paragraph['context'], return_tensors='np')\n",
        "        tokens = emb['attention_mask'].shape[1]\n",
        "        lens.append(tokens)\n",
        "    print(f\"Total n. of paragraphs: {len(train_paragraphs)}\")\n",
        "    for i in range(350, 500, 10):\n",
        "        print(f\"Only {np.sum(np.asarray(lens) > i)} are truncated when using {i} max tokens ({np.sum(np.asarray(lens) > i) / len(train_paragraphs)*100}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t80p88uc5493"
      },
      "source": [
        "To reduce memory consumption, we can:\n",
        "- Truncate sequences at something like 380 tokens (less than 1% of the total training paragraphs would be cut with this decision)\n",
        "- Reduce the dimensionality of Bert/DistilBert's output to `BERT_DIMENSIONALITY` (instead of the default of 768)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 512\n",
        "BERT_DIMENSIONALITY = 768"
      ],
      "metadata": {
        "id": "yQmglauV5p5V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we decide to reduce the dimensionality, we can add a `Dense` layer on top of the encoder. This layer reduces the dimension of the output to whatever we need."
      ],
      "metadata": {
        "id": "ZF8bdGkg5uAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8GSRnaQpJOqB"
      },
      "outputs": [],
      "source": [
        "class ReducedDistilBertModel(keras.Model):\n",
        "    def __init__(self, distilbert_model):\n",
        "        super(ReducedDistilBertModel, self).__init__()\n",
        "        self.distilbert_model = distilbert_model\n",
        "        self.reduction_layer = keras.layers.Dense(BERT_DIMENSIONALITY, \n",
        "                                                  activation='gelu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_state = self.distilbert_model(inputs).last_hidden_state\n",
        "        # We introduce a dense layer that simply reduces the dimensionality of distilbert\n",
        "        return self.reduction_layer(hidden_state)\n",
        "\n",
        "model_q = ReducedDistilBertModel(model_q)\n",
        "model_p = ReducedDistilBertModel(model_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this model on a test question:"
      ],
      "metadata": {
        "id": "hrnVC5w26ZrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcCbC-Vi1es5",
        "outputId": "50882c6c-e6d0-4e4b-de43-dd81e80a327e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on a simple question. \n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Representation dimensionality: (768,)\n"
          ]
        }
      ],
      "source": [
        "test_question = train_questions[0]['qas']['question']\n",
        "print(f\"Testing on a simple question. \\nQuestion: {test_question}\")\n",
        "inputs_test = tokenizer_distilbert(test_question, return_tensors=\"tf\")\n",
        "outputs = model_q(inputs_test)\n",
        "\n",
        "# As a representation of the token we use the last hidden state at the [CLS] token (the first one)\n",
        "test_q_repr = outputs[0,0,:]\n",
        "print(f\"Representation dimensionality: {test_q_repr.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also create the Tf-Idf vectorizers for the training and validation sets."
      ],
      "metadata": {
        "id": "yrIu_o4AskgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')\n",
        "val_vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True, max_df=0.8, norm='l2')"
      ],
      "metadata": {
        "id": "oxY534ShszhQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform all paragraphs into a vectorized representation."
      ],
      "metadata": {
        "id": "mNtqOSlWtTBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs_vect = train_vectorizer.fit_transform([train_paragraphs[i]['context'] for i in range(len(train_paragraphs))])\n",
        "val_docs_vect = val_vectorizer.fit_transform([val_paragraphs[i]['context'] for i in range(len(val_paragraphs))])"
      ],
      "metadata": {
        "id": "TfphwqPKs45x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each question in these sets, obtain its highest scoring negative."
      ],
      "metadata": {
        "id": "RuyfXIt5tbs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_negative_paragraph_for_question(question, vectorizer, docs_vet, paragraphs):\n",
        "    # Get the groundtruth paragraph for the question\n",
        "    gt_paragraph, _ = question['context_id']\n",
        "    # Transform the question into a vector\n",
        "    vect_question = vectorizer.transform([question['qas']['question']])\n",
        "    # Compute similarities with the input matrix of representations\n",
        "    similarities = np.asarray(np.dot(docs_vet, vect_question.T).todense()).flatten()\n",
        "    # Obtain the best scoring indices and remove the groundtruth index\n",
        "    best_scoring_paragraph_indices = np.argsort(-similarities)\n",
        "    best_scoring_paragraph_indices = best_scoring_paragraph_indices[\n",
        "        best_scoring_paragraph_indices != gt_paragraph]\n",
        "    best_negative = best_scoring_paragraph_indices[0]\n",
        "    # Return the corresponding paragraph\n",
        "    return paragraphs[best_negative]"
      ],
      "metadata": {
        "id": "cEsPdTbNtfzK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_best_negative_paragraph_for_question(\n",
        "    train_questions[0], train_vectorizer, train_docs_vect, train_paragraphs)"
      ],
      "metadata": {
        "id": "iYLBAXEgvj_p",
        "outputId": "38d7c3fb-5c76-41cc-90e6-f235db4dbf11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': ' In Methodism, Mary is honored as the Mother of God. Methodists do not have any additional teachings on the Virgin Mary except from what is mentioned in Scripture and the ecumenical Creeds. As such, Methodists believe that Mary was conceived in her womb through the Holy Ghost and accept the doctrine of the Virgin Birth, although they, along with Orthodox Christians and other Protestant Christians, reject the doctrine of the Immaculate Conception. John Wesley, the principal founder of the Methodist movement within the Church of England, believed that Mary \"continued a pure and unspotted virgin\", thus upholding the doctrine of the perpetual virginity of Mary. Contemporary Methodism does hold that Mary was a virgin before, during, and immediately after the birth of Christ. In addition, some Methodists also hold the doctrine of the Assumption of Mary as a pious opinion.',\n",
              " 'context_id': 114}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is indeed a hard negative, because it talks about Mary and Christian religion just like the grondtruth paragraph:"
      ],
      "metadata": {
        "id": "IOQngL0QvyPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_paragraphs[0]"
      ],
      "metadata": {
        "id": "kRCc0lsrv7x0",
        "outputId": "421e01ad-a19b-4242-869e-7f82d4b7b342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
              " 'context_id': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCYkx8rf1es_"
      },
      "source": [
        "# Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we need to train our models. To do that, we need to create a dataset that feeds batches of questions and positive and negative paragraphs to our model. The model is then used to compute the representations of the questions/paragraphs and through the representations we can compute the similarities and correct the learnt distributions from the encoder models."
      ],
      "metadata": {
        "id": "KENlnOwNCxN_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynvz3Gw1etA"
      },
      "source": [
        "For the dataset, we use the `tf.data.Dataset` API. In particular, we first create the dataset in a `.proto` file which contains all the information in a byte format. The file is then uploaded on Google Cloud, where it can easily be accessed all future times. This is the only working way to have a large dataset accessible from the TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaBWVf5k1etB",
        "outputId": "df216da1-8e01-4412-b836-a0845c3a0a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/train_768.proto).\n",
            "Loading val_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/val_768.proto).\n"
          ]
        }
      ],
      "source": [
        "def get_paragraph_from_question(qas, dataset):\n",
        "    # Returns the ground truth paragraph for a question.\n",
        "    i,j = qas['context_id']\n",
        "    return dataset[i]['paragraphs'][j]\n",
        "\n",
        "def pre_tokenize_data(questions, dataset, tokenizer):\n",
        "    # Pre-tokenizes questions and paragraphs\n",
        "    tokenized_questions = [\n",
        "        dict(tokenizer(questions[i]['qas']['question'], \n",
        "            max_length = MAX_SEQ_LEN, truncation = True, \n",
        "            padding = 'max_length'))\n",
        "        for i in tqdm(range(len(questions)))]\n",
        "    tokenized_paragraphs = [\n",
        "        dict(tokenizer(get_paragraph_from_question(\n",
        "                    questions[i], dataset\n",
        "                )['context'], max_length = MAX_SEQ_LEN, \n",
        "            truncation = True, padding = 'max_length',\n",
        "            return_offsets_mapping = True))             # For the paragraphs we need the offset mappings too\n",
        "        for i in tqdm(range(len(questions)))]\n",
        "    return tokenized_questions, tokenized_paragraphs\n",
        "\n",
        "def find_start_end_token_one_hot_encoded(\n",
        "    answers: Dict, \n",
        "    offsets: List[Tuple[int]]) -> Dict:\n",
        "    '''\n",
        "    This function returns the starting and ending token of the answer, \n",
        "    already one hot encoded and ready for binary crossentropy.\n",
        "    Inputs:\n",
        "        - answers: `List[Dict]` --> for each question, a list of answers.\n",
        "            Each answer contains:\n",
        "            - `answer_start`: the index of the starting character\n",
        "            - `text`: the text of the answer, that we exploit through the \n",
        "                number of chars that it contains\n",
        "        - offsets: `List[Tuple[int]]` --> the tokenizer from HuggingFace \n",
        "            transforms the paragraph into a sequence of tokens. \n",
        "            Offsets keeps track of the character start and end indexes for each token.\n",
        "   \n",
        "    Output:\n",
        "        - result: `Dict` --> each key contains only one array, the one-hot \n",
        "            encoded version of, respectively, the start and end token of \n",
        "            the answer in the sentence (question+context)\n",
        "    '''\n",
        "    result = {\n",
        "        \"out_S\": np.zeros(len(offsets), dtype=np.int32),\n",
        "        \"out_E\": np.zeros(len(offsets), dtype=np.int32)\n",
        "    } \n",
        "    for answer in answers:\n",
        "        starting_char = answer['answer_start']\n",
        "        answer_len = len(answer['text'])\n",
        "        # We skip the first token, [CLS], that has (0,0) as a tuple\n",
        "        for i in range(1, len(offsets)):\n",
        "            # Check if starting char is within the indexes\n",
        "            if (starting_char >= offsets[i][0]) and \\\n",
        "                (starting_char <= offsets[i][1]):\n",
        "                result[\"out_S\"][i] += 1\n",
        "            # If the ending char (starting + length -1) is in the interval, \n",
        "            # same as above.\n",
        "            if (starting_char + answer_len - 1 >= offsets[i][0]) and \\\n",
        "                (starting_char + answer_len - 1 < offsets[i][1]):\n",
        "                result[\"out_E\"][i] += 1\n",
        "                break\n",
        "    return result\n",
        "\n",
        "def decode_fn(record_bytes):\n",
        "    # Reads one element from the dataset (as bytes) and decodes it in a tf.data Dataset element.\n",
        "    example = tf.io.parse_single_example(\n",
        "      # Data\n",
        "      record_bytes,\n",
        "      # Schema\n",
        "      {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"context__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64)})\n",
        "    return {\n",
        "      \"questions\": {'input_ids': example['question__input_ids'],\n",
        "                    'attention_mask': example['question__attention_mask'],\n",
        "                    'index': example['question__index']},\n",
        "      \"answers\":   {'out_s': example['answer__out_s'],\n",
        "                    'out_e': example['answer__out_e']},\n",
        "      \"paragraphs\":{'input_ids': example['paragraph__input_ids'],\n",
        "                    'attention_mask': example['paragraph__attention_mask'],\n",
        "                    'tokens_s': example['paragraph__tokens_s'],\n",
        "                    'tokens_e': example['paragraph__tokens_e']},\n",
        "      \"hard_paragraphs\": {'input_ids': example['hard_paragraph__input_ids'],\n",
        "                          'attention_mask': example['hard_paragraph__attention_mask']},\n",
        "      \"context_ids\": (example['context__index'], example['paragraph__index'])\n",
        "    }\n",
        "\n",
        "def create_dataset_from_records(questions, paragraphs, dataset, tokenizer, \n",
        "                                fn, vectorizer, docs_vect, batch_size=BATCH_SIZE):\n",
        "    # Import functions to update cloud bucket. Key for using Google cloud services is on drive\n",
        "    from google.cloud import storage\n",
        "    import time\n",
        "\n",
        "    # Prepare strings\n",
        "    filename = f'{fn}_{BERT_DIMENSIONALITY}.proto'\n",
        "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
        "    dst_name = fn_type + '.proto'\n",
        "    bucket_name = 'volpepe-nlp-project-squad-datasets'\n",
        "    gcs_filename = f'gs://{bucket_name}/{dst_name}'\n",
        "    # If we want to write a new dataset or overwrite one with the same name:\n",
        "    if OVERWRITE_DATASETS:\n",
        "        print(\"Pre-tokenizing data...\")\n",
        "        tok_questions, tok_paragraphs = pre_tokenize_data(questions, dataset, tokenizer)\n",
        "        assert len(tok_questions) == len(tok_paragraphs), \"Error while pre-tokenizing dataset\"\n",
        "        print(\"Preprocessing answers...\")\n",
        "        answer_tokens = [find_start_end_token_one_hot_encoded(\n",
        "            questions[i]['qas']['answers'], tok_paragraphs[i]['offset_mapping'])\n",
        "        for i in tqdm(range(len(questions)))]\n",
        "        print(\"Saving dataset on disk...\")\n",
        "        with tf.io.TFRecordWriter(filename) as file_writer:\n",
        "            for i in tqdm(range(len(tok_questions))):\n",
        "                hard_paragraph = get_best_negative_paragraph_for_question(questions[i], vectorizer, docs_vect, paragraphs)\n",
        "                tok_hard_paragraph = dict(tokenizer(hard_paragraph['context'], \n",
        "                                                    max_length = MAX_SEQ_LEN, \n",
        "                                                    truncation = True, \n",
        "                                                    padding = 'max_length',\n",
        "                                                    return_offsets_mapping = False))\n",
        "                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                    \"question__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=tok_questions[i][\"input_ids\"])),\n",
        "                    \"question__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=tok_questions[i][\"attention_mask\"])),\n",
        "                    \"question__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[i])),\n",
        "                    \"answer__out_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_S\"])),\n",
        "                    \"answer__out_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_E\"])),\n",
        "                    \"paragraph__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_paragraphs[i][\"input_ids\"])),\n",
        "                    \"paragraph__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_paragraphs[i][\"attention_mask\"])),\n",
        "                    \"hard_paragraph__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_hard_paragraph['input_ids'])),\n",
        "                    \"hard_paragraph__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=tok_hard_paragraph['attention_mask'])),\n",
        "                    \"paragraph__tokens_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[0] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
        "                    \"paragraph__tokens_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[1] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
        "                    \"context__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[questions[i]['context_id'][0]])),\n",
        "                    \"paragraph__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[questions[i]['context_id'][1]]))\n",
        "                    })).SerializeToString()\n",
        "                file_writer.write(record_bytes)\n",
        "        # Upload the dataset on cloud\n",
        "        print(f\"Uploading {filename} on {gcs_filename} (bucket {bucket_name}) with name {dst_name}\")\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(dst_name)\n",
        "        blob.upload_from_filename(filename)\n",
        "        time.sleep(5)   # Wait 5 seconds\n",
        "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
        "    # Return it as processed dataset\n",
        "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_fn)\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "### CREATE DATASETS ###\n",
        "# Valid for both TPU and GPU\n",
        "dataset_train = create_dataset_from_records(train_questions, train_paragraphs, train_paragraphs_and_questions, \n",
        "                                            tokenizer_distilbert, os.path.join(datasets_dir, 'train'),\n",
        "                                            train_vectorizer, train_docs_vect)\n",
        "dataset_val = create_dataset_from_records(val_questions, val_paragraphs, val_paragraphs_and_questions, \n",
        "                                          tokenizer_distilbert, os.path.join(datasets_dir, 'val'),\n",
        "                                          val_vectorizer, val_docs_vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAnwbiFU1etF"
      },
      "source": [
        "# Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52lwz19a1etG"
      },
      "source": [
        "First of all, we need a layer that takes as input the dictionary containing the tokenized questions and answers and returns their compact representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjPUMwKk1etG",
        "outputId": "1600d05c-4c78-4542-b674-05a27ceacb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape when in training mode: (8, 768), (8, 768), (8, 768)\n",
            "Output shape when in testing mode: (8, 768)\n",
            "Output shape when dealing with a single question: (1, 768)\n"
          ]
        }
      ],
      "source": [
        "class DenseEncoder(layers.Layer):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.model_q = model_q  # Dense encoder for questions\n",
        "        self.model_p = model_p  # Dense encoder for paragraphs\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "        # Encode the questions in the batch\n",
        "        # Take the first token as representation of each question\n",
        "        q_repr = self.model_q({\n",
        "            'input_ids': inputs['questions']['input_ids'],\n",
        "            'attention_mask': inputs['questions']['attention_mask']\n",
        "        })[:,0,:]\n",
        "        # If we are training, we also return the representation of the paragraphs\n",
        "        # and of the hard paragraph\n",
        "        if training:\n",
        "            # Encode the batch of paragraphs\n",
        "            p_repr = self.model_p({\n",
        "                'input_ids': inputs['paragraphs']['input_ids'],\n",
        "                'attention_mask': inputs['paragraphs']['attention_mask']\n",
        "            })[:,0,:]\n",
        "            # We also encode the batch of hard paragraphs separately. \n",
        "            p_hard_repr = self.model_p({\n",
        "                'input_ids': inputs['hard_paragraphs']['input_ids'],\n",
        "                'attention_mask': inputs['hard_paragraphs']['attention_mask']\n",
        "            })[:,0,:]\n",
        "            return q_repr, p_repr, p_hard_repr\n",
        "        else:\n",
        "            return q_repr\n",
        "\n",
        "# Small test for the layer\n",
        "class TestDenseEncoderModel(keras.Model):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.enc(inputs, training=training)\n",
        "\n",
        "test_model = TestDenseEncoderModel(model_q, model_p)\n",
        "q_repr, p_repr, p_hard_repr = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=True)\n",
        "print(f\"Output shape when in training mode: {q_repr.shape}, {p_repr.shape}, {p_hard_repr.shape}\")\n",
        "q_repr_2 = test_model(next(dataset_train.take(1).as_numpy_iterator()), training=False)\n",
        "print(f\"Output shape when in testing mode: {q_repr_2.shape}\")\n",
        "q = tokenizer_distilbert(\n",
        "    train_questions[0]['qas']['question'], max_length = MAX_SEQ_LEN, \n",
        "    truncation = True, padding = 'max_length', return_tensors=\"tf\")\n",
        "q_repr_3 = test_model({'questions': q})\n",
        "print(f\"Output shape when dealing with a single question: {q_repr_3.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4r1MfHP1etI"
      },
      "source": [
        "Once we have the representations, we should compute the similarities, thus obtaining a a full mini-batch of positive-negative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdG5bkWV2FU5",
        "outputId": "52e92111-c8e8-4a30-c220-b1604ff17e25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Create the similarity matrix\n",
        "S = tf.tensordot(q_repr, tf.transpose(p_repr), axes=1)\n",
        "S.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEPy1ug1__n0"
      },
      "source": [
        "This similarity matrix has the following meaning:\n",
        "- Rows represent questions.\n",
        "- Each element in the row contains the similarity that the respective question has with the paragraphs in the batch (paragraphs are on the columns and one of them is the positive one, the others are negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To this matrix, we want to append the score that the paragraphs have with the hard paragraphs. To do that we:\n",
        "- One by one, we compute the score that each question representations has with the corresponding hard paragraph.\n",
        "- We append these scores to the original $8\\times8$ matrix, obtaining a $8\\times9$ matrix of similarities. "
      ],
      "metadata": {
        "id": "gRi022EH6cuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hard_scores = tf.gather(\n",
        "    # Get the elements on the diagonal of the 8x8 matrix of scores between questions and \n",
        "    # hard paragraphs\n",
        "    tf.tensordot(q_repr, tf.transpose(p_hard_repr), axes=1), \n",
        "    tf.expand_dims(tf.range(BATCH_SIZE), axis=1), batch_dims=1)\n",
        "hard_scores.shape"
      ],
      "metadata": {
        "id": "FDRxz8ImIUs2",
        "outputId": "fcf019c2-38ee-48b7-e60a-fb4852ab8823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We concatenate this vector as an additional column to S."
      ],
      "metadata": {
        "id": "_7oZI3ngF0HX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complete_S = tf.concat([S, hard_scores], axis=1)\n",
        "complete_S.shape"
      ],
      "metadata": {
        "id": "yZ9nP9r3F3-A",
        "outputId": "3f4839c4-54bf-4698-cc19-1068c58d33b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: the groundtruth value are still the ones from 1 to 8, so this does not damage the loss computation."
      ],
      "metadata": {
        "id": "7lHTo-9EGhwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the paper, they refer to the loss as a *minimization of the negative log-likelihood of the positive passage*: what it really means is that we need to transform similarities to probabilities and use a categorical cross-entropy loss, where labels are the row index (which is also the column index in that row for the positive passage)"
      ],
      "metadata": {
        "id": "zBwtW8D76aJH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_aJezqWCKsw",
        "outputId": "b56c1d0a-9890-44e9-a351-2fc3784a5c93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5730038"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True\n",
        ")\n",
        "loss(y_true=tf.range(BATCH_SIZE), y_pred=complete_S).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnidWQLZDwsf"
      },
      "source": [
        "The loss seems to be quite high for this batch. We can study it with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "Vdgc2-gQD1PP",
        "outputId": "753f9c5d-f614-4b24-cbf0-8b44a03b98c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f47b60624d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "S_arr = complete_S.numpy()\n",
        "ConfusionMatrixDisplay.from_predictions(y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQwdzlTfEsH4"
      },
      "source": [
        "Indeed, ideally the predictions should be on the diagonal. This means that the \"default\" space for this metric learning problem is not that good. We are ready to learn a new representation distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOLMK5mIMPc"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "nme0koNHE76t"
      },
      "outputs": [],
      "source": [
        "class DeepQPEncoder(keras.Model):\n",
        "\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            # For training we return the similarity matrix\n",
        "            repr_q, repr_p, repr_hard_p = self.enc(inputs, training=training)\n",
        "            S = tf.tensordot(repr_q, tf.transpose(repr_p), axes=1)\n",
        "            # We append the hard scores\n",
        "            hard_scores = tf.gather(\n",
        "                # Get the elements on the diagonal of the 8x8 matrix of \n",
        "                # scores between questions and hard paragraphs\n",
        "                tf.tensordot(repr_q, tf.transpose(repr_hard_p), axes=1), \n",
        "                tf.expand_dims(tf.range(BATCH_SIZE), axis=1), batch_dims=1\n",
        "            )\n",
        "            S = tf.concat([S, hard_scores], axis=1)\n",
        "            return S\n",
        "        else:\n",
        "            # In other cases, we return the representation of the question(s)\n",
        "            repr_q = self.enc(inputs, training=training)            \n",
        "            return repr_q\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        # y = [0, ..., batch_size-1]\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Obtain similarities\n",
        "            S = self(x, training=True)\n",
        "            # Obtain loss value\n",
        "            loss = self.compiled_loss(y, S)\n",
        "        # Construct gradients and apply them through the optimizer\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Update and return metrics (specifically the one for the loss value).\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x = data\n",
        "        # y = [0, ..., batch_size-1]\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        S = self(x, training=True) # We are not really training, but we have to obtain S\n",
        "        self.compiled_loss(y, S)\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "def create_model(sample, freeze_layers_up_to=5):\n",
        "    print(\"Creating BERT models...\")\n",
        "    model_q, model_p =  TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                        TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Freeze layers \n",
        "    for i in range(freeze_layers_up_to): # layers 0 to variable are frozen, successive layers learn\n",
        "        model_q.distilbert.transformer.layer[i].trainable = False\n",
        "        model_p.distilbert.transformer.layer[i].trainable = False\n",
        "\n",
        "    model_q, model_p = ReducedDistilBertModel(model_q), ReducedDistilBertModel(model_p)\n",
        "\n",
        "    print(\"Creating Deep Encoder...\")\n",
        "    model = DeepQPEncoder(model_q, model_p)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model and loss\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of data to build the model\n",
        "    model(sample)\n",
        "\n",
        "    # Return the model\n",
        "    print(\"Model created!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA_dwB6MIHaq"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfdxw7F4IWVi"
      },
      "source": [
        "Define utility variables and saving paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "s838Rj9GIUA9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 100\n",
        "PATIENCE = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOrJEqCQIZ3c"
      },
      "source": [
        "Before training, we check if we're using a TPU, in order to create the model within the scope of the strategy.\n",
        "\n",
        "Then, we train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O5zDcaTEtxV",
        "outputId": "323a0de5-c7d7-429e-c59d-9cc0bc53371b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating BERT models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Deep Encoder...\n",
            "Compiling...\n",
            "Testing on some data...\n",
            "Model created!\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "# GPUs and local systems don't need the above specifications. We simply\n",
        "# create a pattern for the filename and let the callbacks deal with it.\n",
        "model_name = f'dpr_{BERT_DIMENSIONALITY}_hard'\n",
        "checkpoint_path = os.path.join(checkpoint_dir, model_name + \".ckpt\")\n",
        "# Workaraound for saving locally when using cloud TPUs\n",
        "local_device_option = tf.train.CheckpointOptions(\n",
        "    experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)\n",
        "else:\n",
        "    # On TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        f\"training_dpr_{BERT_DIMENSIONALITY}_hard\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    model = create_model(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN6WxrL-IRjj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if DO_TRAINING:\n",
        "    if not using_TPU:\n",
        "        # Tensorboard callback is not available on TPU\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        )\n",
        "    \n",
        "    # ModelCheckpoint callback is available both on TPU and GPU \n",
        "    # with the options parameter\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath = checkpoint_path,\n",
        "        verbose=1,\n",
        "        save_weights_only = True,\n",
        "        save_best_only = True,\n",
        "        options=local_device_option\n",
        "    )\n",
        "\n",
        "    # Early stopping can be used by both hardware\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    callbacks = [es_callback, cp_callback]\n",
        "    if not using_TPU:\n",
        "        # These callback imply saving stuff on local disk, which cannot be \n",
        "        # done automatically using TPUs.\n",
        "        # Therefore, they are only active when using GPUs and local systems\n",
        "        callbacks.extend([tensorboard_callback])\n",
        "\n",
        "    # We fit the model\n",
        "    history = model.fit(\n",
        "        dataset_train, \n",
        "        y=None,\n",
        "        validation_data=dataset_val,\n",
        "        epochs=EPOCHS, \n",
        "        callbacks=callbacks,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=0,\n",
        "        verbose=1 # Show progress bar\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCgA-4RbAZCh"
      },
      "source": [
        "The model was trained in 7 (+7 due to patience) epochs on a cloud TPU. It took less than an hour and reached a validation accuracy of 93.52%. We can make a simple test with the same data of before to see if the embedding space is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkTmbBS0CmZ4"
      },
      "outputs": [],
      "source": [
        "if DO_TRAINING:\n",
        "    S = model(next(dataset_val.take(1).as_numpy_iterator()), training=True)\n",
        "    S_arr = S.numpy()\n",
        "    ConfusionMatrixDisplay.from_predictions(\n",
        "        y_true=np.arange(BATCH_SIZE), y_pred=np.argmax(S_arr, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training history:"
      ],
      "metadata": {
        "id": "84i9cMRsUCaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.axvspan(len(history.history['loss'])-7, len(history.history['loss'])-1, color='red', alpha=0.3)"
      ],
      "metadata": {
        "id": "Ux56cvk6DI6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5InDU76v93rS"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbp2lLe9iX-u"
      },
      "source": [
        "First of all, we load the weights of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry1uahz-bh4_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Load the obtained weights\n",
        "model.load_weights(checkpoint_path, options=local_device_option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6vabTXiR0Dc"
      },
      "source": [
        "We load the encodings in memory but using the memmap mode because the files are larger than the available RAM. Basically, we are only loading a map of the file on disk in memory and for complex reads we follow this map and read from the disk.\n",
        "\n",
        "This makes it possible to access the array, even though it will be slower than having the representations on RAM. But in this way we can also enlarge the dimensionality if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPpSZ9nSZWMz"
      },
      "outputs": [],
      "source": [
        "representations_dir = os.path.join(datasets_dir, 'representations')\n",
        "os.makedirs(representations_dir, exist_ok=True)\n",
        "\n",
        "if OVERRIDE_REPRESENTATIONS:\n",
        "    test_paragraphs_encodings = np.stack([\n",
        "            model.enc.model_p(tokenizer_distilbert(\n",
        "                test_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "                return_tensors='tf', truncation = True, padding = 'max_length'\n",
        "            ))[0,:,:].numpy().astype(np.float32) \n",
        "        for i in tqdm(range(len(test_paragraphs)))\n",
        "    ])\n",
        "    np.save(os.path.join(representations_dir, 'test_paragraphs_encodings'), test_paragraphs_encodings)\n",
        "    del test_paragraphs_encodings\n",
        "\n",
        "test_paragraphs_encodings = np.load(os.path.join(representations_dir, 'test_paragraphs_encodings.npy'), allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_paragraphs_encodings.shape, test_paragraphs_encodings.dtype"
      ],
      "metadata": {
        "id": "cLL1bwtOurbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to map paragraphs to their position in the encoding matrix."
      ],
      "metadata": {
        "id": "acFbh9kH4ChH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_paragraph_encoding_index(question, dataset):\n",
        "    art_id, par_id = question['context_id']\n",
        "    idx = sum([len(dataset[i]['paragraphs']) for i in range(art_id)]) + par_id\n",
        "    return idx"
      ],
      "metadata": {
        "id": "P32oYqfP4GFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReN18CaggRW5"
      },
      "source": [
        "### Qualitative test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ4-VRXZgXlb"
      },
      "source": [
        "We select some random questions from the test set and see whether the selected paragraph is the correct one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1NISpx7gs4I"
      },
      "outputs": [],
      "source": [
        "sample_question = random.choice(test_questions)\n",
        "question_text = sample_question['qas']['question']\n",
        "tokenized_question = tokenizer_distilbert(question_text, return_tensors=\"tf\")\n",
        "paragraph_index = get_paragraph_encoding_index(sample_question, test_paragraphs_and_questions)\n",
        "print(f\"Question: {question_text}\")\n",
        "print(f\"Ground truth paragraph: {get_paragraph_from_question(sample_question, test_paragraphs_and_questions)['context']}\")\n",
        "print(f\"Ground truth index: {paragraph_index}\")\n",
        "sample_q_repr = model.enc.model_q(tokenized_question)[:,0,:]\n",
        "print(f\"Question representation shape: {sample_q_repr.shape}\")\n",
        "scores = tf.tensordot(sample_q_repr, test_paragraphs_encodings[:,0,:].T, axes=1)\n",
        "best_par_index = tf.argsort(scores, axis=1, direction='DESCENDING')[0, :5].numpy()\n",
        "print(f\"Top-5 best matching paragraphs have indexes {best_par_index}\")\n",
        "print(f\"Top-5 best matching paragraphs:\")\n",
        "for i in range(5):\n",
        "    print(f\"{i})\\t{test_paragraphs[best_par_index[i]]['context']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOPMaSyb5CfJ"
      },
      "source": [
        "Usually, the correct paragraphs show up in the Top-5 list, or very similar paragraphs appear instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWM3pdRAwBlz"
      },
      "source": [
        "### Quantitative test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJXrFcjnwFaA"
      },
      "source": [
        "We can measure the top-1/top-5 accuracy as we've done for the tf-idf baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MShv2E92wXJt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "count_top1 = 0\n",
        "count_top5 = 0\n",
        "count_total = len(test_questions)\n",
        "\n",
        "if RUN_ACCURACY_ANALYSIS:\n",
        "    def top_5_for_question(tok_q, paragraphs_encodings):\n",
        "        # Input: a tokenized question\n",
        "        # Pass the question through the question encoder\n",
        "        sample_q_repr = model.enc.model_q(tok_q)[:,0,:].numpy()\n",
        "        # Compute scores with the paragraphs\n",
        "        scores = np.dot(sample_q_repr, paragraphs_encodings[:,0,:].T)\n",
        "        # Obtain the best scores and indices\n",
        "        top5_indices = np.argsort(scores, axis=1)[0, -5:]\n",
        "        top5_scores = np.sort(scores, axis=1)[0, -5:]\n",
        "        return top5_indices, top5_scores     \n",
        "\n",
        "    for i,q in enumerate(tqdm(test_questions)):\n",
        "        gt_context_id = get_paragraph_encoding_index(q, test_paragraphs_and_questions)\n",
        "        top5_indices, top5_scores = top_5_for_question(\n",
        "            tokenizer_distilbert(\n",
        "                q['qas']['question'], max_length = MAX_SEQ_LEN, \n",
        "                return_tensors='tf', truncation = True, padding = 'max_length'\n",
        "            ), test_paragraphs_encodings\n",
        "        )\n",
        "        if gt_context_id == top5_indices[0]:\n",
        "            count_top1 += 1\n",
        "        if gt_context_id in top5_indices:\n",
        "            count_top5 += 1\n",
        "\n",
        "    top1_score = count_top1 / count_total * 100\n",
        "    top5_score = count_top5 / count_total * 100\n",
        "\n",
        "    print(f\"\\nTop 1 test score: {top1_score:.2f}%,\\nTop 5 test score: {top5_score:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMe1p_OQR0De"
      },
      "source": [
        "- Top 1 score: 4.64%\n",
        "- Top 5 score: 52.31%\n",
        "\n",
        "To have a fair comparison with Tf-Idf, the same model was trained seeing all test paragraphs and reached:\n",
        "\n",
        "- Top 1 score: 89.04%\n",
        "- Top 5 score: 97.69%\n",
        "\n",
        "It looks pretty bad, but these are paragraphs that were not seen in training so the model still has some generalisation capabilities. Also, the selected paragraphs are usually correlated to the question. Finally, the training procedure is bad, because we are only comparing 64 scores at a time, instead of all scores like we do in evaluation: it means that if a negative paragraph that is very similar to the ground truth one is not in the batch, then the job of the network might be too easy."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dense_passage_retriever.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3f1aedb1eb2c65e580c7e6628d6a6f161dbf7d87a60144a28bd6d91db1c87885"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}