{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "username = 'MarcelloCeresini'\n",
        "repository = 'QuestionAnswering'\n",
        "\n",
        "# COLAB ONLY CELLS\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    !pip3 install transformers\n",
        "    !git clone https://www.github.com/{username}/{repository}.git\n",
        "    #from google.colab import drive\n",
        "    #drive.mount('/content/drive/')\n",
        "    %cd /content/QuestionAnswering/src\n",
        "    using_TPU = True    # If we are running this notebook on Colab, use a TPU\n",
        "    # Google cloud credentials\n",
        "    %env GOOGLE_APPLICATION_CREDENTIALS=/content/drive/MyDrive/Uni/Magistrale/NLP/Project/nlp-project-338723-0510aa0a4912.json\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    using_TPU = False   # If you're not on Colab you probably won't have access to a TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm45AN-fDEiv"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Volpe\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from typing import List, Union, Dict, Tuple\n",
        "from transformers import BertTokenizerFast, DistilBertTokenizerFast, \\\n",
        "                         TFBertModel, TFDistilBertModel\n",
        "import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import utils\n",
        "import faiss\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "MAX_SEQ_LEN = 512\n",
        "BERT_DIMENSIONALITY = 768\n",
        "BATCH_SIZE = 8 if not using_TPU else 128\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "ROOT_PATH = os.path.dirname(os.getcwd())\n",
        "TRAINING_FILE = os.path.join(ROOT_PATH, 'data', 'training_set.json')\n",
        "VALIDATION_FILE = os.path.join(ROOT_PATH, 'data', 'validation_set.json')\n",
        "TEST_FILE = os.path.join(ROOT_PATH, 'data', 'dev_set.json')\n",
        "\n",
        "if using_TPU:\n",
        "    try: \n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.TPUStrategy(resolver)\n",
        "    except:\n",
        "        print(\"TPUs are not available, setting flag 'using_TPU' to False.\")\n",
        "        using_TPU = False\n",
        "        print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "else:\n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question Answering with the DPR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DtqzKMW6scF"
      },
      "source": [
        "We have trained the two Bert (`bert_p`, the paragraphs encoder and `bert_q`, the questions encoder) models to produce embeddings that are as similar as possible for matching question-paragraph pairs. \n",
        "\n",
        "Thanks to our training, when we use `bert_q` to encode our question, we will now be sure that questions and paragraphs will both be encoded in the same space and have a with the matching paragraph's encoding.\n",
        "\n",
        "We can now use `bert_p` to encode all of our paragraphs a-priori using the same method we have used before (taking the 768-d encoding at the `[CLS]` token). \n",
        "\n",
        "Then, we can define our final Question Answering model in this way:\n",
        "- It receives only a question's embedding as input.\n",
        "- It uses `bert_q` to create a representation of the question in the learnt 768-d space, that is in common with the paragraph representations.\n",
        "- We compute similarity scores between the representation of the question and all representations of paragraphs. Based on these scores, we select the top-k ($k=100$) paragraphs.\n",
        "- For each of the $k$ paragraphs, we must compute the probability of the paragraph being selected $P_{selected}(i)$, as well as the usual $P_{start, i}(s), P_{end, i}(t)$ for each of the $s$-th and $t$-th words of the $i$-th paragraph. To do that, we need the full encoding of the paragraph (the $512 \\times 768$ output of Bert), which will be denoted as $P_i$ in contrast to $\\hat{P}_i$ which is the 768-d encoding at the `[CLS]` token. We obtain the full encoding by passing the $k$ paragraphs through `bert_p`, which is set to non-trainable (otherwise the encoding of the `[CLS]` token would constantly change). \n",
        "- All probabilities are computed through dense layers:\n",
        "\\begin{gather}\n",
        "P_{start,i}(s) = softmax(P_i w_{start})_s\n",
        "\\\\\n",
        "P_{end,i}(t) = softmax(P_i w_{end})_t\n",
        "\\\\\n",
        "P_{selected}(i) = softmax(\\hat{P}_i^\\intercal w_{selected})_i\n",
        "\\\\\n",
        "\\end{gather}\n",
        "where $w_{start}$, $w_{end}$ and $w_{selected}$ are learnt vectors, while $\\hat{P}_i = [P_{1}^{[CLS]}, \\dots, P_k^{[CLS]}]$.\n",
        "- As final answer, we select the highest scoring start-end legal span from the highest-scoring paragraph.\n",
        "\n",
        "During training: For each question, we create a batch by sampling $m$ ($m=24$ in the paper) from the top-100 passages returned by the retrieval system (DPR, so by computing similarities with the pre-computed representations). The training objective is to maximize the marginal log-likelihood of all the correct answer spans in the positive passage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. In the paper, a batch size of 16 was used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load all data that was prepared into the `dense_passage_retriever` notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "    checkpoint_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/weights/training_dpr/'\n",
        "    datasets_dir = '/content/drive/MyDrive/Uni/Magistrale/NLP/Project/datasets/dpr/'\n",
        "else:\n",
        "    # Create the folder where we'll save the weights of the model\n",
        "    checkpoint_dir = os.path.join(\"checkpoints\", \"training_dpr\")\n",
        "    datasets_dir = os.path.join(\"checkpoints\", \"training_dpr\", \"dataset\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(datasets_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_paragraphs_and_questions = utils.read_question_set(TRAINING_FILE)['data']\n",
        "val_paragraphs_and_questions = utils.read_question_set(VALIDATION_FILE)['data']\n",
        "test_paragraphs_and_questions = utils.read_question_set(TEST_FILE)['data']\n",
        "\n",
        "# Remove the validation set from the train set\n",
        "train_paragraphs_and_questions = [article for article in train_paragraphs_and_questions \\\n",
        "                                  if article not in val_paragraphs_and_questions]\n",
        "\n",
        "def get_questions_and_paragraphs(dataset):\n",
        "    questions = [{\n",
        "            'qas': qas,\n",
        "            'context_id': (i,j)    # We also track the question's original context and paragraph indices so to have a ground truth\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for j, para in enumerate(dataset[i]['paragraphs'])\n",
        "        for qas in para['qas']\n",
        "    ]\n",
        "\n",
        "    paragraphs = [{\n",
        "            'context': para['context'],\n",
        "            'context_id': i\n",
        "        }\n",
        "        for i in range(len(dataset))\n",
        "        for para in dataset[i]['paragraphs']\n",
        "    ]\n",
        "\n",
        "    return questions, paragraphs\n",
        "\n",
        "train_questions, train_paragraphs = get_questions_and_paragraphs(train_paragraphs_and_questions)\n",
        "val_questions, val_paragraphs = get_questions_and_paragraphs(val_paragraphs_and_questions)\n",
        "test_questions, test_paragraphs = get_questions_and_paragraphs(test_paragraphs_and_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading train_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/train_768.proto).\n",
            "Loading val_768 dataset from GCS (gs://volpepe-nlp-project-squad-datasets/val_768.proto).\n"
          ]
        }
      ],
      "source": [
        "def decode_fn(record_bytes):\n",
        "    # Reads one element from the dataset (as bytes) and decodes it in a tf.data Dataset element.\n",
        "    example = tf.io.parse_single_example(\n",
        "      # Data\n",
        "      record_bytes,\n",
        "      # Schema\n",
        "      {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"question__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"hard_paragraph__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "       \"context__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "       \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64)})\n",
        "    return {\n",
        "      \"questions\": {'input_ids': example['question__input_ids'],\n",
        "                    'attention_mask': example['question__attention_mask'],\n",
        "                    'index': example['question__index']},\n",
        "      \"answers\":   {'out_s': example['answer__out_s'],\n",
        "                    'out_e': example['answer__out_e']},\n",
        "      \"paragraphs\":{'input_ids': example['paragraph__input_ids'],\n",
        "                    'attention_mask': example['paragraph__attention_mask'],\n",
        "                    'tokens_s': example['paragraph__tokens_s'],\n",
        "                    'tokens_e': example['paragraph__tokens_e']},\n",
        "      \"hard_paragraphs\": {'input_ids': example['hard_paragraph__input_ids'],\n",
        "                          'attention_mask': example['hard_paragraph__attention_mask']},\n",
        "      \"context_ids\": (example['context__index'], example['paragraph__index'])\n",
        "    }\n",
        "\n",
        "def load_tf_dataset_from_cloud(questions, fn, batch_size=BATCH_SIZE):\n",
        "    # Prepare strings\n",
        "    filename = f'{fn}_{BERT_DIMENSIONALITY}.proto'\n",
        "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
        "    dst_name = fn_type + '.proto'\n",
        "    bucket_name = 'volpepe-nlp-project-squad-datasets'\n",
        "    gcs_filename = f'gs://{bucket_name}/{dst_name}'\n",
        "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
        "    # Return it as processed dataset\n",
        "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_fn)\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "dataset_train = load_tf_dataset_from_cloud(train_questions, os.path.join(datasets_dir, 'train'))\n",
        "dataset_val = load_tf_dataset_from_cloud(val_questions, os.path.join(datasets_dir, 'val'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_distilbert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "\n",
        "class ReducedDistilBertModel(keras.Model):\n",
        "    def __init__(self, distilbert_model):\n",
        "        super(ReducedDistilBertModel, self).__init__()\n",
        "        self.distilbert_model = distilbert_model\n",
        "        self.reduction_layer = keras.layers.Dense(BERT_DIMENSIONALITY, \n",
        "                                                  activation='gelu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_state = self.distilbert_model(inputs).last_hidden_state\n",
        "        # We introduce a dense layer that simply reduces the dimensionality of the model's output.\n",
        "        # It's not used if the output dimensionality is the same of the Bert model\n",
        "        return self.reduction_layer(hidden_state) if BERT_DIMENSIONALITY != 768 else hidden_state\n",
        "\n",
        "\n",
        "class DenseEncoder(layers.Layer):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.model_q = model_q  # Dense encoder for questions\n",
        "        self.model_p = model_p  # Dense encoder for paragraphs\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "        # Encode the questions in the batch\n",
        "        # Take the first token as representation of each question\n",
        "        q_repr = self.model_q({\n",
        "            'input_ids': inputs['questions']['input_ids'],\n",
        "            'attention_mask': inputs['questions']['attention_mask']\n",
        "        })[:,0,:]\n",
        "        # If we are training, we also return the representation of the paragraphs\n",
        "        # and of the hard paragraph\n",
        "        if training:\n",
        "            # Encode the batch of paragraphs\n",
        "            p_repr = self.model_p({\n",
        "                'input_ids': inputs['paragraphs']['input_ids'],\n",
        "                'attention_mask': inputs['paragraphs']['attention_mask']\n",
        "            })[:,0,:]\n",
        "            # We also encode the batch of hard paragraphs separately. \n",
        "            p_hard_repr = self.model_p({\n",
        "                'input_ids': inputs['hard_paragraphs']['input_ids'],\n",
        "                'attention_mask': inputs['hard_paragraphs']['attention_mask']\n",
        "            })[:,0,:]\n",
        "            return q_repr, p_repr, p_hard_repr\n",
        "        else:\n",
        "            return q_repr\n",
        "\n",
        "\n",
        "class DeepQPEncoder(keras.Model):\n",
        "    def __init__(self, model_q, model_p):\n",
        "        super().__init__()\n",
        "        self.enc = DenseEncoder(model_q, model_p)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            # For training we return the similarity matrix\n",
        "            repr_q, repr_p, repr_hard_p = self.enc(inputs, training=training)\n",
        "            S = tf.tensordot(repr_q, tf.transpose(repr_p), axes=1)\n",
        "            # We append the hard scores\n",
        "            hard_scores = tf.gather(\n",
        "                # Get the elements on the diagonal of the 8x8 matrix of \n",
        "                # scores between questions and hard paragraphs\n",
        "                tf.tensordot(repr_q, tf.transpose(repr_hard_p), axes=1), \n",
        "                    tf.expand_dims(\n",
        "                        tf.range(tf.shape(inputs['questions']['input_ids'])[0]), \n",
        "                        axis=1), \n",
        "                    batch_dims=1\n",
        "            )\n",
        "            S = tf.concat([S, hard_scores], axis=1)\n",
        "            return S\n",
        "        else:\n",
        "            # In other cases, we return the representation of the question(s)\n",
        "            repr_q = self.enc(inputs, training=training)            \n",
        "            return repr_q\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        # y = [0, ..., batch_size-1]\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Obtain similarities\n",
        "            S = self(x, training=True)\n",
        "            # Obtain loss value\n",
        "            loss = self.compiled_loss(y, S)\n",
        "        # Construct gradients and apply them through the optimizer\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Update and return metrics (specifically the one for the loss value).\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x = data\n",
        "        # y = [0, ..., batch_size-1]\n",
        "        y = tf.range(tf.shape(x['questions']['input_ids'])[0])\n",
        "        S = self(x, training=True) # We are not really training, but we have to obtain S\n",
        "        self.compiled_loss(y, S)\n",
        "        self.compiled_metrics.update_state(y, S)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "def create_dpr(sample, freeze_layers_up_to=5):\n",
        "    print(\"Creating BERT models...\")\n",
        "    model_q, model_p =  TFDistilBertModel.from_pretrained('distilbert-base-uncased'), \\\n",
        "                        TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    # Freeze layers \n",
        "    for i in range(freeze_layers_up_to): # layers 0 to variable are frozen, successive layers learn\n",
        "        model_q.distilbert.transformer.layer[i].trainable = False\n",
        "        model_p.distilbert.transformer.layer[i].trainable = False\n",
        "\n",
        "    model_q, model_p = ReducedDistilBertModel(model_q), ReducedDistilBertModel(model_p)\n",
        "\n",
        "    print(\"Creating Deep Encoder...\")\n",
        "    model = DeepQPEncoder(model_q, model_p)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model and loss\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of data to build the model\n",
        "    model(sample)\n",
        "\n",
        "    # Return the model\n",
        "    print(\"Model created!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating BERT models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
            "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Deep Encoder...\n",
            "Compiling...\n",
            "Testing on some data...\n",
            "Model created!\n"
          ]
        }
      ],
      "source": [
        "dpr_model_name = f'dpr_{BERT_DIMENSIONALITY}_hard'\n",
        "dpr_checkpoint_path = os.path.join(checkpoint_dir, dpr_model_name + \".ckpt\")\n",
        "local_device_option = tf.train.CheckpointOptions(\n",
        "    experimental_io_device=\"/job:localhost\")\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        dpr_model = create_dpr(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)\n",
        "else:\n",
        "    # On TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        f\"training_dpr_{BERT_DIMENSIONALITY}_hard\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    dpr_model = create_dpr(sample=next(dataset_train.take(1).as_numpy_iterator()),\n",
        "                             freeze_layers_up_to=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19b78d10700>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the obtained weights\n",
        "dpr_model.load_weights(dpr_checkpoint_path, options=local_device_option)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPG2DHje99it"
      },
      "source": [
        "## Question Answering model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGzumB9nAVYd"
      },
      "source": [
        "We pre-tokenize the paragraphs so that we have easy access to them inside the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDK42osrAlsX",
        "outputId": "e38eaf7c-d1d9-4a2c-d383-6e25e063eaaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13975/13975 [00:09<00:00, 1514.58it/s]\n",
            "100%|██████████| 4921/4921 [00:02<00:00, 1691.06it/s]\n"
          ]
        }
      ],
      "source": [
        "pretokenized_paragraphs = {\n",
        "    'train': {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'offset_mapping': []\n",
        "    },\n",
        "    'val': {\n",
        "        'input_ids': [],\n",
        "        'attention_mask': [],\n",
        "        'offset_mapping': []\n",
        "    }\n",
        "}\n",
        "\n",
        "for i in tqdm(range(len(train_paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        train_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['train']['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['train']['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['train']['offset_mapping'].append(token_p['offset_mapping'])\n",
        "\n",
        "for i in tqdm(range(len(val_paragraphs))):\n",
        "    token_p = dict(tokenizer_distilbert(\n",
        "        train_paragraphs[i]['context'], max_length = MAX_SEQ_LEN, \n",
        "        return_tensors='tf', truncation = True, \n",
        "        padding = 'max_length', return_offsets_mapping = True\n",
        "    ))\n",
        "    pretokenized_paragraphs['val']['input_ids'].append(token_p['input_ids'])\n",
        "    pretokenized_paragraphs['val']['attention_mask'].append(token_p['attention_mask'])\n",
        "    pretokenized_paragraphs['val']['offset_mapping'].append(token_p['offset_mapping'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOcyq9GtN7i-"
      },
      "source": [
        "Then, we define a function to create the model. The model should accept the top paragraphs encodings collected using the DPR and return their start, end and selection probabilities.\n",
        "\n",
        "Note that at training time, this is the only part that learns. We assume that the rest is fixed. We use FAISS to index the best paragraph for each encoded question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R_v3r3DcBeKz"
      },
      "outputs": [],
      "source": [
        "class BestScoringCollector(keras.layers.Layer):\n",
        "    '''\n",
        "    Custom layer to collect the start and end probabilities from the best scoring\n",
        "    paragraph\n",
        "    '''\n",
        "    def __init__(self, **kwargs):\n",
        "        super(BestScoringCollector, self).__init__(trainable=False, **kwargs)\n",
        "\n",
        "    def call(self, probs_s, probs_e, probs_sel):\n",
        "        # Selection of best scoring paragraphs\n",
        "        best_scoring_paragraphs = tf.squeeze(tf.argmax(probs_sel, axis=1, output_type=tf.int32))\n",
        "        # Selection of related start-end probabilities\n",
        "        probs_s = tf.squeeze(tf.gather(probs_s, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        probs_e = tf.squeeze(tf.gather(probs_e, indices=tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "        return probs_s, probs_e\n",
        "\n",
        "def create_QA_model(m):\n",
        "    # Receives in input the top paragraph full and search encodings collected using the DPR.\n",
        "    paragraphs_full_encodings = keras.Input(shape=(m, MAX_SEQ_LEN, BERT_DIMENSIONALITY), \n",
        "        dtype='float32', name=\"topm_full_encodings\")\n",
        "    paragraphs_search_encodings = keras.Input(shape=(m, BERT_DIMENSIONALITY), \n",
        "        dtype='float32', name=\"topm_search_encodings\")\n",
        "\n",
        "    # Compute probabilities for the start token\n",
        "    out_S = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"start_token_logits\")(paragraphs_full_encodings)\n",
        "    out_S = keras.layers.Reshape((m, MAX_SEQ_LEN))(out_S)\n",
        "    out_S = keras.layers.Softmax(name=\"start_probs\", axis=1, dtype='float32')(out_S)\n",
        "\n",
        "    # The same is done for the end tokens.\n",
        "    out_E = keras.layers.TimeDistributed(keras.layers.Dense(1), name=\"end_token_logits\")(paragraphs_full_encodings)\n",
        "    out_E = keras.layers.Reshape((m, MAX_SEQ_LEN))(out_E)\n",
        "    out_E = keras.layers.Softmax(name=\"end_probs\", axis=1, dtype='float32')(out_E)\n",
        "\n",
        "    # Also, we compute paragraph selection probabilities\n",
        "    out_SEL = keras.layers.Dense(1, name=\"selection_logits\")(paragraphs_search_encodings)\n",
        "    out_SEL = keras.layers.Flatten('channels_first')(out_SEL)\n",
        "    out_SEL = keras.layers.Softmax(name=\"selection_probs\", dtype='float32')(out_SEL)\n",
        "\n",
        "    out_S, out_E = BestScoringCollector(name='best_scoring_collector')(out_S, out_E, out_SEL)\n",
        "\n",
        "    # We return the keras model\n",
        "    model = keras.Model(\n",
        "        inputs=[paragraphs_full_encodings, paragraphs_search_encodings],\n",
        "        outputs = [out_S, out_E, out_SEL]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-Jr45AObut"
      },
      "source": [
        "We analyze the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Cvu8NLQGtSbJ"
      },
      "outputs": [],
      "source": [
        "M = 24              # Number of paragraphs to be collected by the DPR (24 in the paper)\n",
        "\n",
        "model_qa = create_QA_model(M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkdbnPt_v-G8",
        "outputId": "e34d586c-5a2e-482d-b48a-d4b7b06c513f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " topm_full_encodings (InputLaye  [(None, 24, 512, 76  0          []                               \n",
            " r)                             8)]                                                               \n",
            "                                                                                                  \n",
            " topm_search_encodings (InputLa  [(None, 24, 768)]   0           []                               \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " start_token_logits (TimeDistri  (None, 24, 512, 1)  769         ['topm_full_encodings[0][0]']    \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " end_token_logits (TimeDistribu  (None, 24, 512, 1)  769         ['topm_full_encodings[0][0]']    \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            " selection_logits (Dense)       (None, 24, 1)        769         ['topm_search_encodings[0][0]']  \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 24, 512)      0           ['start_token_logits[0][0]']     \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 24, 512)      0           ['end_token_logits[0][0]']       \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 24)           0           ['selection_logits[0][0]']       \n",
            "                                                                                                  \n",
            " start_probs (Softmax)          (None, 24, 512)      0           ['reshape[0][0]']                \n",
            "                                                                                                  \n",
            " end_probs (Softmax)            (None, 24, 512)      0           ['reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " selection_probs (Softmax)      (None, 24)           0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " best_scoring_collector (BestSc  (None, None)        0           ['start_probs[0][0]',            \n",
            " oringCollector)                                                  'end_probs[0][0]',              \n",
            "                                                                  'selection_probs[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,307\n",
            "Trainable params: 2,307\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_qa.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Etf4vYO5OB"
      },
      "source": [
        "The outputs are of the expected shape. We also use an utility to see the model visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "q8KH_VJSL8VN",
        "outputId": "bed812e2-58be-4f4b-ac9e-4ca1257e1764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
          ]
        }
      ],
      "source": [
        "keras.utils.plot_model(model_qa, \"multi_input_and_output_model.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrzBIEVvR0Dg"
      },
      "source": [
        "### Model input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The input to the model are pre-encoded paragraph encodings. The encoding happens with the pre-trained `model_q` that we previously trained in the `dense_passage_retriever` notebook. We don't use a neural model anymore to retrieve the best scoring paragraphs: instead we have created a \"pre-trained\" FAISS index on the encodings of `model_p` for all paragraphs. In this way we can efficiently collect the best paragraphs given one or a sequence of queries (`model_q` encodings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_index = faiss.read_index(os.path.join(ROOT_PATH, 'data', 'indices', 'train_index'))\n",
        "val_index   = faiss.read_index(os.path.join(ROOT_PATH, 'data', 'indices', 'val_index'))\n",
        "test_index  = faiss.read_index(os.path.join(ROOT_PATH, 'data', 'indices', 'test_index'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We try out our method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_paragraph_from_question(question, paragraphs_set):\n",
        "    ctx, par = question['context_id']\n",
        "    return paragraphs_set[ctx]['paragraphs'][par]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 5,  1,  5,  2, 27,  6, 25,  4], dtype=int64),\n",
              " array([ 6, 82, 19, 22,  5, 18, 26,  5], dtype=int64))"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_data = next(dataset_val.take(1).as_numpy_iterator())\n",
        "input_data['context_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 768)"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = input_data['questions']\n",
        "qs_representations = dpr_model.enc.model_q({\n",
        "    'input_ids': qs['input_ids'], \n",
        "    'attention_mask': qs['attention_mask']\n",
        "})[:,0,:].numpy()\n",
        "qs_representations.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "dAUXxy7lR0Dg",
        "outputId": "7933b6a7-d26d-449b-8c03-f7206ed5604f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 100), (8, 100))"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "distances, best_indices = val_index.search(qs_representations, 100)\n",
        "distances.shape, best_indices.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The search is quite fast! Let's see if it's also accurate..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1038, 4082, 1925,  307,   59, 4078, 1780, 4664, 2178,  300],\n",
              "       [  64, 4681,   60, 1038,   59, 4664,  496,   61, 3346, 4366],\n",
              "       [2653, 1318,  424, 2744, 1780,  689,  131,  304,  294, 2763],\n",
              "       [1038,  689, 4664,   59, 4082, 4681, 3641, 2653, 1417,  982],\n",
              "       [1051, 2653, 2273, 3107, 1038, 2588, 2286, 4681, 2261, 1211],\n",
              "       [2653, 4681,  982, 1780,  689, 1809, 2744, 1794, 3346, 2650],\n",
              "       [1038,  982,  889,  896,  989,  689, 1318, 1192, 2744,  992],\n",
              "       [4681,   64, 1038,  294,   59,   60, 4082, 4678, 3346, 4789]],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_indices[:,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_best_indices_to_paragraph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(NOPE IT'S ALL RANDOM FOR NOW WTF) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The rest of it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O1KsF7MR0Dg"
      },
      "source": [
        "The Dense Passage retriever returns indices of paragraphs, which we can collect using this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVuywyVKR0Dh"
      },
      "outputs": [],
      "source": [
        "def collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, indices):\n",
        "    # Collect paragraphs. Inputs is the batch of paragraph indexes to gather\n",
        "    paragraphs = {\n",
        "        'input_ids': tf.squeeze(tf.gather(pretokenized_paragraphs['input_ids'], indices), axis=2),\n",
        "        'attention_mask': tf.squeeze(tf.gather(pretokenized_paragraphs['attention_mask'], indices), axis=2),\n",
        "        'offset_mapping': tf.squeeze(tf.gather(pretokenized_paragraphs['offset_mapping'], indices), axis=2),\n",
        "        'indexes': indices\n",
        "    }\n",
        "    # Collect paragraph representations and their search encodings\n",
        "    paragraphs_full_encodings = np.take(paragraphs_encodings, indices, axis=0)\n",
        "    paragraphs_search_encodings = paragraphs_full_encodings[:,:,0,:]\n",
        "    return paragraphs, paragraphs_full_encodings, paragraphs_search_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwtBPPqvR0Dh"
      },
      "outputs": [],
      "source": [
        "ps, full_enc, search_enc = \\\n",
        "    collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encoding, top_m_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niPmBhTmR0Dh"
      },
      "source": [
        "So we can create a dataset like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ibpoyrCR0Dh",
        "outputId": "a93f610c-1416-4351-a970-f893c65dceaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-tokenizing data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████| 65064/65064 [00:10<00:00, 6277.52it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████| 65064/65064 [00:32<00:00, 2019.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing answers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████| 65064/65064 [00:01<00:00, 43289.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving dataset on disk...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                           | 54/65064 [13:57<492:30:07, 27.27s/it]"
          ]
        }
      ],
      "source": [
        "def create_qa_dataset(questions, fn, model_q, paragraphs_encodings, pretokenized_paragraphs):\n",
        "    # Instantiate DPR\n",
        "    dpr = DensePassageRetriever(model_q, paragraphs_encodings)\n",
        "    # Read dataset from cloud disk\n",
        "    filename = f'{fn}_v3_{BERT_DIMENSIONALITY}.proto'\n",
        "    fn_type = filename.split(os.sep)[-1].replace('.proto','')\n",
        "    gcs_filename = f'gs://volpepe-nlp-project-squad-datasets/{fn_type}.proto'\n",
        "    \n",
        "    if OVERWRITE_DATASETS_QA:\n",
        "        print(\"Pre-tokenizing data...\")\n",
        "        tok_questions, tok_paragraphs = pre_tokenize_data(questions, full_dict, tokenizer_distilbert)\n",
        "        assert len(tok_questions) == len(tok_paragraphs), \"Error while pre-tokenizing dataset\"\n",
        "        print(\"Preprocessing answers...\")\n",
        "        answer_tokens = [find_start_end_token_one_hot_encoded(\n",
        "            questions[i]['qas']['answers'], tok_paragraphs[i]['offset_mapping'])\n",
        "        for i in tqdm(range(len(questions)))]\n",
        "        print(\"Saving dataset on disk...\")\n",
        "        with tf.io.TFRecordWriter(filename) as file_writer:\n",
        "            for i in tqdm(range(len(tok_questions))):\n",
        "                question_ids = tok_questions[i][\"input_ids\"]\n",
        "                question_attention_mask = tok_questions[i][\"attention_mask\"]\n",
        "                # Create the DPR inputs\n",
        "                dpr_inputs = {\n",
        "                    'questions': { \n",
        "                        'input_ids': np.array(question_ids),\n",
        "                        'attention_mask': np.array(question_attention_mask)\n",
        "                    }\n",
        "                }\n",
        "                # Retrieve scores and top indices by calling the DPR\n",
        "                scores, top_m_indices = dpr(dpr_inputs)\n",
        "                # Obtain the paragraphs and their encodings by calling the collect function\n",
        "                ps, full_enc, search_enc = \\\n",
        "                    collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, top_m_indices)\n",
        "                \n",
        "                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                    \"question__input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=question_ids)),\n",
        "                    \"question__attention_mask\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=question_attention_mask)),\n",
        "                    \"answer__out_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_S\"])),\n",
        "                    \"answer__out_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=answer_tokens[i][\"out_E\"])),\n",
        "                    \"paragraph__index\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=[get_paragraph_global_id_from_question(questions[i], paragraphs)])),\n",
        "                    \"paragraph__full_enc\": tf.train.Feature(bytes_list=tf.train.BytesList(\n",
        "                        value=[tf.io.serialize_tensor(np.asarray(full_enc)).numpy()])),\n",
        "                    \"paragraph__tokens_s\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[0] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
        "                    \"paragraph__tokens_e\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                            value=[x[1] for x in tok_paragraphs[i][\"offset_mapping\"]])),\n",
        "                    \"dpr__top_m_indices\": tf.train.Feature(int64_list=tf.train.Int64List(\n",
        "                        value=list(tf.squeeze(top_m_indices)))),\n",
        "                    })).SerializeToString()\n",
        "                file_writer.write(record_bytes)\n",
        "        print(\"Upload the dataset on Google Cloud and re-run the function\")\n",
        "        return None\n",
        "    \n",
        "    def decode_qa_fn(record_bytes):\n",
        "        # Read example from bytes dataset\n",
        "        example = tf.io.parse_single_example(\n",
        "            # Data\n",
        "            record_bytes,\n",
        "            # Schema\n",
        "            {\"question__input_ids\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"question__attention_mask\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"answer__out_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"answer__out_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"paragraph__index\": tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n",
        "            \"paragraph__full_enc\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
        "            \"paragraph__tokens_s\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"paragraph__tokens_e\": tf.io.FixedLenFeature(shape=(MAX_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"dpr__top_100_indices\": tf.io.FixedLenFeature(shape=(100, ), dtype=tf.int64)\n",
        "           })\n",
        "        \n",
        "        # Yield the final dictionary structured however we need\n",
        "        return {\n",
        "            'questions': {\n",
        "                'input_ids': example['question__input_ids'],\n",
        "                'attention_mask': example['question__attention_mask']\n",
        "            },\n",
        "            'answers': {\n",
        "                'out_s': example['answer__out_s'],\n",
        "                'out_e': example['answer__out_e']\n",
        "            },\n",
        "            'paragraphs': {\n",
        "                'index': example['paragraph__index'],\n",
        "                'full_enc': tf.io.parse_tensor(example['paragraph__full_enc'], out_type=tf.float32),\n",
        "                'search_enc': tf.io.parse_tensor(example['paragraph__search_enc'], out_type=tf.float32),\n",
        "                'tokens_s': example['paragraph__tokens_s'],\n",
        "                'tokens_e': example['paragraph__tokens_e']\n",
        "            },\n",
        "            'dpr': {\n",
        "                'top_100_indices': example['dpr__top_100_indices']\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    print(f\"Loading {fn_type} dataset from GCS ({gcs_filename}).\")\n",
        "    # Return it as processed dataset\n",
        "    dataset = tf.data.TFRecordDataset([gcs_filename]).map(decode_qa_fn)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.shuffle(10000)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    dataset = dataset.apply(tf.data.experimental.assert_cardinality(len(questions)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "dataset_train_qa = create_qa_dataset(train_questions, os.path.join(datasets_dir, 'train_qa'), \n",
        "                                    model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs)\n",
        "dataset_val_qa = create_qa_dataset(val_questions, os.path.join(datasets_dir, 'val_qa'), \n",
        "                                  model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suAR_76XR0Dh",
        "outputId": "806bd0a0-62aa-46fc-e791-611782a66d56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec={'questions': {'input_ids': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'index': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}, 'answers': {'out_s': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'out_e': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None)}, 'paragraphs': {'input_ids': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'tokens_s': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'tokens_e': TensorSpec(shape=(None, 400), dtype=tf.int64, name=None), 'index': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}}>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_train_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uha4NPnpR0Di"
      },
      "outputs": [],
      "source": [
        "def create_qa_datasets(, model_q, paragraphs_encodings, pretokenized_paragraphs):\n",
        "\n",
        "    def qa_dataset_generator(dataset):\n",
        "        # Instantiate DPR\n",
        "        dpr = DensePassageRetriever(model_q, paragraphs_encodings)\n",
        "        # Instantiate dataset iterator\n",
        "        dataset = dataset.as_numpy_iterator()\n",
        "        # Iterator over regular dataset\n",
        "        for el in dataset:\n",
        "            # Retrieve scores and top indices by calling the DPR\n",
        "            scores, top_m_indices = dpr(el)\n",
        "            # Obtain the paragraphs and their encodings by calling the collect function\n",
        "            paragraphs, full_enc, search_enc = \\\n",
        "                collect_paragraphs_from_dpr(pretokenized_paragraphs, paragraphs_encodings, top_m_indices)\n",
        "            # Yield the collected elements\n",
        "            gt_indices = el['paragraphs']['index']\n",
        "            gt_start = el['answers']['out_s']\n",
        "            gt_end = el['answers']['out_e']\n",
        "            yield {\n",
        "                'gt_indices': gt_indices, \n",
        "                'gt_start': gt_start,\n",
        "                'gt_end': gt_end,\n",
        "                'top_m_indices': top_m_indices,\n",
        "                'full_enc': full_enc, \n",
        "                'search_enc': search_enc\n",
        "            }\n",
        "\n",
        "    signature = {\n",
        "        'gt_indices': tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.int32), \n",
        "        'gt_start': tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_LEN), dtype=tf.int32),\n",
        "        'gt_end': tf.TensorSpec(shape=(BATCH_SIZE, MAX_SEQ_LEN), dtype=tf.int32),\n",
        "        'top_m_indices': tf.TensorSpec(shape=(BATCH_SIZE, 100, ), dtype=tf.int32),\n",
        "        'full_enc': tf.TensorSpec(shape=(BATCH_SIZE, 100, MAX_SEQ_LEN, BERT_DIMENSIONALITY), dtype=tf.float32), \n",
        "        'search_enc': tf.TensorSpec(shape=(BATCH_SIZE, 100, BERT_DIMENSIONALITY), dtype=tf.float32)\n",
        "    }\n",
        "\n",
        "    qa_dataset_train = tf.data.Dataset.from_generator(partial(\n",
        "            qa_dataset_generator, dataset_train), \n",
        "        output_signature=signature)\n",
        "    qa_dataset_val = tf.data.Dataset.from_generator(partial(\n",
        "            qa_dataset_generator, dataset_val), \n",
        "        output_signature=signature)\n",
        "    \n",
        "    qa_dataset_train.apply(tf.data.experimental.assert_cardinality(len(dataset_train)))\n",
        "    qa_dataset_val.apply(tf.data.experimental.assert_cardinality(len(dataset_val)))\n",
        "\n",
        "    # No need to batch the dataset, it's already batched.\n",
        "    qa_dataset_train = qa_dataset_train.cache()\n",
        "    qa_dataset_train = qa_dataset_train.shuffle(10000)\n",
        "    qa_dataset_train = qa_dataset_train.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    qa_dataset_val = qa_dataset_val.cache()\n",
        "    qa_dataset_val = qa_dataset_val.shuffle(10000)\n",
        "    qa_dataset_val = qa_dataset_val.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return qa_dataset_train, qa_dataset_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvPEufKAR0Di"
      },
      "outputs": [],
      "source": [
        "qa_dataset_train, qa_dataset_val = create_qa_datasets(\n",
        "    dataset_train, dataset_val, model.enc.model_q, paragraphs_encoding, pretokenized_paragraphs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31v6I4RsR0Di"
      },
      "outputs": [],
      "source": [
        "next(qa_dataset_train.take(1).as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNhAi3lsPUOZ"
      },
      "source": [
        "### Training\n",
        "\n",
        "To train the model we need to:\n",
        "\n",
        "- Compile it defining the losses and optimizer.\n",
        "- Create a ground truth batch that we use for comparing the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leUruObriIep"
      },
      "source": [
        "We define custom training and testing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iWkkQK8zhv-"
      },
      "outputs": [],
      "source": [
        "# Metrics (create under the strategy scope if using TPU)\n",
        "if using_TPU:\n",
        "    with strategy.scope():\n",
        "        loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
        "        end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
        "        sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
        "else:\n",
        "    loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "    start_acc_metric = keras.metrics.CategoricalAccuracy(name=\"start_token_accuracy\")\n",
        "    end_acc_metric = keras.metrics.CategoricalAccuracy(name=\"end_token_accuracy\")\n",
        "    sel_acc_metric = keras.metrics.CategoricalAccuracy(name=\"par_selection_accuracy\")\n",
        "\n",
        "class QuestionAnsweringModel(keras.Model):\n",
        "    def __init__(self, m=M):\n",
        "        super(QuestionAnsweringModel, self).__init__()\n",
        "        self.model_qa = create_QA_model(m)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return inputs['top_m_indices'], self.model_qa({\n",
        "            'topm_full_encodings': inputs['full_enc'],\n",
        "            'topm_search_encodings': inputs['search_enc']\n",
        "        })\n",
        "\n",
        "    @tf.function\n",
        "    def tf_shuffle_on_columns(self, value):\n",
        "        '''\n",
        "        Utility function that shuffles a tensor randomly on each of its rows.\n",
        "        '''\n",
        "        # Create a tensor of random numbers, argsort it and use them as indices to gather\n",
        "        # values from the original tensor\n",
        "        return tf.gather(value, tf.argsort(tf.random.uniform(tf.shape(value))), batch_dims=1)\n",
        "\n",
        "    @tf.function\n",
        "    def obtain_training_info(self, indexes, topm_indexes):\n",
        "        '''\n",
        "        Obtains a batch of data and the ground truth mask to be used while training the model\n",
        "        '''\n",
        "        # Collect ground truth indexes\n",
        "        gt_paragraphs = tf.expand_dims(tf.cast(indexes, tf.int32), -1)\n",
        "        # A training sample is formed by the positive and m-1 negative examples\n",
        "        # obtained from the top-100 for each of the questions in the batch.\n",
        "        # We create a data batch by sampling m-1 examples from the masked 100 paragraphs\n",
        "        negative_masks = tf.math.not_equal(topm_indexes, gt_paragraphs)\n",
        "        # To keep the graph working with the correct sizes, we create a tensor of negatives \n",
        "        # by random shuffling the large tensor of topm and taking the first train_m elements.\n",
        "        # The positive examples are replaced by randomly sampling from the same tensor.\n",
        "        # It could happen that the positive example is replaced by itself, or that a \n",
        "        # negative sample appears twice in the batch, but it's a non-deterministic\n",
        "        # process.\n",
        "        negatives = self.tf_shuffle_on_columns(tf.where(\n",
        "            negative_masks, topm_indexes, self.tf_shuffle_on_columns(topm_indexes))\n",
        "        )[:,:M-1]\n",
        "        # We concatenate the positive paragraph index to the selected negatives and shuffle\n",
        "        # so that the positive is not always the last element\n",
        "        data_batch = self.tf_shuffle_on_columns(\n",
        "            tf.concat([negatives, gt_paragraphs], axis=1))\n",
        "        # When we have a data batch, we create the ground truth mask, which represents the position\n",
        "        # of the positive sample in the data batch in a one-hot encoded fashion.\n",
        "        gt_mask = tf.cast(data_batch == gt_paragraphs, tf.int32)\n",
        "        return data_batch, gt_mask\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Obtain the training batch and the ground truth mask\n",
        "        data_batch, gt_mask = self.obtain_training_info(\n",
        "            data['gt_indices'], data['top_m_indices'])\n",
        "        # Open the gradient tape, obtain predictions and compute the loss\n",
        "        with tf.GradientTape() as tape:\n",
        "            _, out_S, out_E, out_SEL = self(data_batch, training=True)\n",
        "            loss_start = self.compiled_loss(data['gt_start'], out_S)\n",
        "            loss_end = self.compiled_loss(data['gt_end'], out_E)\n",
        "            loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "            loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        # Compute the gradients and apply them on the variables\n",
        "        grads = tape.gradient(loss_value, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        # Update the metrics\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        start_acc_metric.update_state(data['gt_start'], out_S)\n",
        "        end_acc_metric.update_state(data['gt_end'], out_E)\n",
        "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        return {\"loss\": loss_tracker.result(), \n",
        "                \"start_accuracy\": start_acc_metric.result(),\n",
        "                \"end_accuracy\": end_acc_metric.result(),\n",
        "                \"sel_accuracy\": sel_acc_metric.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Obtain the training batch and the ground truth mask to compute metrics\n",
        "        data_batch, gt_mask = self.obtain_training_info(\n",
        "            data['gt_indices'], data['top_m_indices'])\n",
        "        # Compute predictions\n",
        "        _, out_S, out_E, out_SEL = self(data_batch, training=False)\n",
        "        # Compute the loss to update its metric\n",
        "        loss_start = self.compiled_loss(data['gt_start'], out_S)\n",
        "        loss_end = self.compiled_loss(data['gt_end'], out_E)\n",
        "        loss_sel = self.compiled_loss(gt_mask, out_SEL)\n",
        "        loss_value = sum([loss_start, loss_end, loss_sel])\n",
        "        loss_tracker.update_state(loss_value)\n",
        "        # Updates the metrics\n",
        "        start_acc_metric.update_state(data['gt_start'], out_S)\n",
        "        end_acc_metric.update_state(data['gt_end'], out_E)\n",
        "        sel_acc_metric.update_state(gt_mask, out_SEL)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        return {\"loss\": loss_tracker.result(), \n",
        "                \"start_accuracy\": start_acc_metric.result(),\n",
        "                \"end_accuracy\": end_acc_metric.result(),\n",
        "                \"sel_accuracy\": sel_acc_metric.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        return [loss_tracker, start_acc_metric, end_acc_metric, sel_acc_metric]\n",
        "\n",
        "def create_trainable_QA_model(model_q):\n",
        "    print(\"Creating Question Answering model...\")\n",
        "    model = QuestionAnsweringModel(model_q)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    # Compile the model (metrics are defined into the model)\n",
        "    model.compile(\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=3e-6),\n",
        "        loss = keras.losses.CategoricalCrossentropy()\n",
        "    )\n",
        "\n",
        "    print(\"Testing on some data...\")\n",
        "    # Pass one batch of test data to build the model\n",
        "    inputs = next(qa_dataset_train.take(1).as_numpy_iterator())\n",
        "    top_m_indices, probs_sel, probs_s, probs_e = model(inputs)\n",
        "    \n",
        "    print(\"Output shapes:\")\n",
        "    print(f\"\\tparagraphs: {top_m_indices.shape}\")\n",
        "    print(f\"\\tprobs_sel: {probs_sel.shape}\")\n",
        "    print(f\"\\tprobs_s: {probs_s.shape}\")\n",
        "    print(f\"\\tprobs_e: {probs_e.shape}\")\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RLqioFsIon",
        "outputId": "dd938727-727a-4ef2-f2c3-6babf79550db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Question Answering model...\n",
            "Compiling...\n",
            "Testing on some data...\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "Exception encountered when calling layer \"dense_passage_retriever_2\" (type DensePassageRetriever).\n\nFailed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n\nCall arguments received:\n  • inputs={'questions': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}, 'answers': {'out_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'out_e': 'tf.Tensor(shape=(4, 400), dtype=int64)'}, 'paragraphs': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_e': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/787245389.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \"training_qa_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmodel_QA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_trainable_QA_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretokenized_paragraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparagraphs_encoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/3641427625.py\u001b[0m in \u001b[0;36mcreate_trainable_QA_model\u001b[1;34m(model_q, pretokenized_paragraphs, paragraphs_encodings)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;31m# Pass one batch of test data to build the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopm_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpr_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[0mparagraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_scoring_paragraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_sel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobs_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopm_indexes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Volpe\\anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22196/613652970.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mq_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m                                                     \u001b[1;31m# batch_size x encoding_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# 2) Selection of paragraphs using search encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_repr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m# batch_size x num_parag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mtopm_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'DESCENDING'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m]\u001b[0m        \u001b[1;31m# batch_size x m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopm_indexes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInternalError\u001b[0m: Exception encountered when calling layer \"dense_passage_retriever_2\" (type DensePassageRetriever).\n\nFailed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n\nCall arguments received:\n  • inputs={'questions': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}, 'answers': {'out_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'out_e': 'tf.Tensor(shape=(4, 400), dtype=int64)'}, 'paragraphs': {'input_ids': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'attention_mask': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_s': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'tokens_e': 'tf.Tensor(shape=(4, 400), dtype=int64)', 'index': 'tf.Tensor(shape=(4,), dtype=int64)'}}"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "if using_TPU:\n",
        "    # TPU requires to create the model within the scope of the distributed strategy\n",
        "    # we're using.\n",
        "    with strategy.scope():\n",
        "        model_QA = create_trainable_QA_model(model.enc.model_q, pretokenized_paragraphs, paragraphs_encoding)\n",
        "\n",
        "    # Workaraound for saving locally when using cloud TPUs\n",
        "    local_device_option = tf.train.CheckpointOptions(\n",
        "        experimental_io_device=\"/job:localhost\")\n",
        "else:\n",
        "    # GPUs and local systems don't need the above specifications. We simply\n",
        "    # create a pattern for the filename and let the callbacks deal with it.\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"model_qa_cp-{epoch:04d}.ckpt\")\n",
        "    # Also, on TPU we cannot use tensorboard, but on GPU we can\n",
        "    log_dir = os.path.join(ROOT_PATH, \"data\", \"logs\", \n",
        "        \"training_qa_dpr\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    \n",
        "    model_QA = create_trainable_QA_model(model.enc.model_q, pretokenized_paragraphs, paragraphs_encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffh1nGX0I9FT"
      },
      "source": [
        "Train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmsAi9v63eia"
      },
      "outputs": [],
      "source": [
        "QA_BATCH_SIZE = 4 if not using_TPU else 32\n",
        "\n",
        "dataset_train = create_dataset_from_records(train_questions, paragraphs, train_dict, tokenizer_distilbert, \n",
        "                                            os.path.join(datasets_dir, 'train'), batch_size=QA_BATCH_SIZE)\n",
        "dataset_val = create_dataset_from_records(val_questions, paragraphs, val_dict, tokenizer_distilbert,\n",
        "                                            os.path.join(datasets_dir, 'val'), training=False, batch_size=QA_BATCH_SIZE)\n",
        "\n",
        "if DO_QA_TRAINING:\n",
        "    # Early stopping can be used by both hardware\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        patience = PATIENCE,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    callbacks = [es_callback]\n",
        "    if not using_TPU:\n",
        "        # ModelCheckpoint callback is only available when not using TPU\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath = checkpoint_path,\n",
        "            verbose=1,\n",
        "            save_weights_only = True,\n",
        "            save_best_only = False\n",
        "        )\n",
        "        # Same for tensorboard callback\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir,\n",
        "            histogram_freq=1\n",
        "        )\n",
        "        # Save the first weights using the pattern from before\n",
        "        model_QA.save_weights(checkpoint_path.format(epoch=0))\n",
        "        # These callback imply saving stuff on local disk, which cannot be \n",
        "        # done automatically using TPUs.\n",
        "        # Therefore, they are only active when using GPUs and local systems\n",
        "        callbacks.extend([cp_callback, tensorboard_callback])\n",
        "    else:\n",
        "        # Save first weights in a h5 file (it's the most stable way)\n",
        "        model_QA.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_qa_tpu_0.h5'), overwrite=True)        \n",
        "\n",
        "    # We fit the model\n",
        "    history = model_QA.fit(\n",
        "        dataset_train, \n",
        "        y=None,\n",
        "        validation_data=dataset_val,\n",
        "        epochs=EPOCHS, \n",
        "        callbacks=callbacks,\n",
        "        shuffle=True,\n",
        "        use_multiprocessing=True,\n",
        "        initial_epoch=0,\n",
        "        verbose=1 # Show progress bar\n",
        "    )\n",
        "\n",
        "    if using_TPU:\n",
        "        # Save last weights\n",
        "        model_qa.save_weights(os.path.join(\n",
        "            checkpoint_dir, 'training_qa_tpu_last.h5'), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I79iOHjGbE-"
      },
      "source": [
        "### Obtaining an answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWUaL844hyf6"
      },
      "outputs": [],
      "source": [
        "def start_end_token_from_probabilities(\n",
        "    pstartv: np.array, \n",
        "    pendv: np.array, \n",
        "    dim:int=512) -> List[List[int]]:\n",
        "    '''\n",
        "    Returns a List of [StartToken, EndToken] elements computed from the batch outputs.\n",
        "    '''\n",
        "    idxs = []\n",
        "    for i in range(pstartv.shape[0]):\n",
        "        # For each element in the batch, transform the vectors into matrices\n",
        "        # by repeating them dim times:\n",
        "        # - Vectors of starting probabilities are stacked on the columns\n",
        "        pstart = np.stack([pstartv[i,:]]*dim, axis=1)\n",
        "        # - Vectors of ending probabilities are repeated on the rows\n",
        "        pend = np.stack([pendv[i,:]]*dim, axis=0)\n",
        "        # Once we have the two matrices, we sum them (element-wise operation)\n",
        "        # to obtain the scores of each combination\n",
        "        sums = pstart + pend\n",
        "        # We only care about the scores in the upper triangular part of the matrix\n",
        "        # (where the ending index is greater than the starting index)\n",
        "        # therefore we zero out the diagonal and the lower triangular area\n",
        "        sums = np.triu(sums, k=1)\n",
        "        # The most probable set of tokens is the one with highest score in the\n",
        "        # remaining matrix. Through argmax we obtain its position.\n",
        "        val = np.argmax(sums)\n",
        "        # Since the starting probabilities are repeated on the columns, each element\n",
        "        # is identified by the row. Ending probabilities are instead repeated on rows,\n",
        "        # so each element is identified by the column.\n",
        "        row = val // dim\n",
        "        col = val - dim*row\n",
        "        idxs.append([row,col])\n",
        "    return idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CUVf45Cj0oI"
      },
      "outputs": [],
      "source": [
        "answers_start_end = start_end_token_from_probabilities(probs_s, probs_e)\n",
        "print(answers_start_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVfgDsb_GHIE"
      },
      "source": [
        "Finally, we can obtain the answers to the questions we have given the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcpI3LVz2vKY"
      },
      "outputs": [],
      "source": [
        "best_indices = tf.squeeze(tf.gather(paragraphs['indexes'], tf.expand_dims(best_scoring_paragraphs, -1), batch_dims=1))\n",
        "best_offsets = tf.squeeze(tf.gather(pretokenized_val_paragraphs['offset_mapping'], best_indices))\n",
        "best_offsets.shape, best_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuWtARzUITEV"
      },
      "outputs": [],
      "source": [
        "char_start_end = [(best_offsets[i][answers_start_end[i][0]][0].numpy(),\n",
        "                   best_offsets[i][answers_start_end[i][1]][1].numpy())\n",
        "                 for i in range(BATCH_SIZE)]\n",
        "char_start_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktvdpz52fxOc"
      },
      "outputs": [],
      "source": [
        "# Correction for answers arriving to the end of the sequence\n",
        "for i in range(BATCH_SIZE):\n",
        "    c = char_start_end[i]\n",
        "    if c[1] >= c[0]:\n",
        "        char_start_end[i] = (c[0], c[1])\n",
        "    else:\n",
        "        char_start_end[i] = (c[0], 1000000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6edjxYTw4k-t"
      },
      "outputs": [],
      "source": [
        "for i, p in enumerate(best_indices.numpy()):\n",
        "    print(val_paragraphs[p]['context'][char_start_end[i][0]:char_start_end[i][1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_B5U-_tKR6v"
      },
      "source": [
        "Of course, answers are extremely bad because we need to train the Dense layers selecting the start and end tokens, as well as the paragraph selector."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "QA_DPR.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "68bcab510961ee730b63c1a3ea5fc4f15a05e7cdf00d3442986724f8b958dcdf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
